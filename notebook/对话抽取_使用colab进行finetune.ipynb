{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/Zero-Haruhi/blob/main/notebook/%E5%AF%B9%E8%AF%9D%E6%8A%BD%E5%8F%96_%E4%BD%BF%E7%94%A8colab%E8%BF%9B%E8%A1%8Cfinetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zElyM_0tGpZH",
        "outputId": "cc40ef09-f6db-492b-a822-d069cdc1c0fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.11.17)\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: read).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade huggingface_hub\n",
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "需要授权HF"
      ],
      "metadata": {
        "id": "ROcHXpfyZdU9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjFuGKxUFvbY",
        "outputId": "45512c3e-a12b-4516-d528-c15f3546f310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 6750, done.\u001b[K\n",
            "remote: Counting objects: 100% (769/769), done.\u001b[K\n",
            "remote: Compressing objects: 100% (316/316), done.\u001b[K\n",
            "remote: Total 6750 (delta 505), reused 664 (delta 451), pack-reused 5981\u001b[K\n",
            "Receiving objects: 100% (6750/6750), 204.48 MiB | 22.54 MiB/s, done.\n",
            "Resolving deltas: 100% (4851/4851), done.\n",
            "Updating files: 100% (140/140), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAcYKc9qGwcY",
        "outputId": "d7b1293e-afdb-4a0e-9108-625953f5a101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.7/79.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%cd LLaMA-Factory\n",
        "!pip install -q -r requirements.txt\n",
        "!pip install -q einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9OSdjnmbHCSz"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIhsKveEIT7C",
        "outputId": "fde645df-dda3-405e-eebb-021219ef8652"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/content/LLaMA-Factory/dataset': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls /content/LLaMA-Factory/dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJnK1Za2J-0A"
      },
      "source": [
        "请为我实现一段python代码，对于/content/LLaMA-Factory/data/dataset_info.json 备份到dataset_info_back.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exqKxSYMLKMf",
        "outputId": "87270087-7300-4462-e152-bc55a89532f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backup of /content/LLaMA-Factory/data/dataset_info.json to /content/LLaMA-Factory/data/dataset_info_back.json succeeded!\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "original_file = '/content/LLaMA-Factory/data/dataset_info.json'\n",
        "backup_file = '/content/LLaMA-Factory/data/dataset_info_back.json'\n",
        "\n",
        "# Make a copy of the original file\n",
        "shutil.copy(original_file, backup_file)\n",
        "\n",
        "# Verify copy succeeded\n",
        "if os.path.exists(backup_file):\n",
        "    print(f'Backup of {original_file} to {backup_file} succeeded!')\n",
        "else:\n",
        "    print(f'Backup of {original_file} to {backup_file} failed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48s-ojJQLOsZ"
      },
      "source": [
        "为我实现一段python代码，复制/content/LLaMA-Factory/data/dataset_info.json中的信息\n",
        "\n",
        "写入到 /content/LLaMA-Factory/data/dataset_info.json\n",
        "\n",
        "并且在末尾添加\n",
        "\n",
        "\n",
        "```json\n",
        "\"haruhi_dialogue_extract\": {\n",
        "  \"file_name\": \"dialogue_merged.json\",\n",
        "  \"columns\": {\n",
        "    \"messages\": \"conversations\",\n",
        "    \"system\": \"system\",\n",
        "    \"tools\": \"tools\"\n",
        "  },\n",
        "  \"tags\": {\n",
        "    \"role_tag\": \"from\",\n",
        "    \"content_tag\": \"value\"\n",
        "  }\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ik4Bvq1LNpE",
        "outputId": "11ee9aff-6ba8-4383-dddc-80d84070d7ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data copied and new info appended to /content/LLaMA-Factory/data/dataset_info.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "original_file = '/content/LLaMA-Factory/data/dataset_info.json'\n",
        "new_file = '/content/LLaMA-Factory/data/dataset_info.json'\n",
        "\n",
        "with open(original_file, 'r') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "data['haruhi_dialogue_extract'] = {\n",
        "  \"hf_hub_url\":\"silk-road/Haruhi-Dialogue-Speaker-Extract\",\n",
        "  \"formatting\":\"sharegpt\",\n",
        "  \"columns\": {\n",
        "    \"messages\": \"conversations\",\n",
        "    \"system\": \"system\",\n",
        "  },\n",
        "  \"tags\": {\n",
        "    \"role_tag\": \"from\",\n",
        "    \"content_tag\": \"value\",\n",
        "    \"user_tag\": \"human\",\n",
        "    \"assistant_tag\":\"gpt\",\n",
        "  }\n",
        "}\n",
        "\n",
        "with open(new_file, 'w') as f:\n",
        "  json.dump(data, f, indent=2)\n",
        "\n",
        "print('Data copied and new info appended to', new_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaruMxYdL6QU",
        "outputId": "7680e613-c3bb-4284-a9a5-335214f26711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/2.0 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/2.0 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m1.6/2.0 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers_stream_generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q tiktoken transformers_stream_generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nqZswBFMN6-C"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhZPqKBcGHau",
        "outputId": "e3c418d1-7818-4051-cb69-3477db59d2cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-25 02:17:55.852715: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-25 02:17:55.852768: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-25 02:17:55.854582: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-25 02:17:56.965141: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "01/25/2024 02:17:59 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
            "[INFO|training_args.py:1828] 2024-01-25 02:17:59,726 >> PyTorch: setting up devices\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1741: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
            "  warnings.warn(\n",
            "01/25/2024 02:17:59 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1\n",
            "  distributed training: True, compute dtype: torch.float16\n",
            "01/25/2024 02:17:59 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0002,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=qwen_1_8-finetuned/runs/Jan25_02-17-59_e1a270f13780,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=100,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=cosine,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=qwen_1_8-finetuned,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=qwen_1_8-finetuned,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=1000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "tokenizer_config.json: 100% 173/173 [00:00<00:00, 905kB/s]\n",
            "tokenization_qwen.py: 100% 9.62k/9.62k [00:00<00:00, 35.9MB/s]\n",
            "qwen.tiktoken: 100% 2.56M/2.56M [00:00<00:00, 9.74MB/s]\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-01-25 02:18:01,039 >> loading file qwen.tiktoken from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/qwen.tiktoken\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-01-25 02:18:01,039 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-01-25 02:18:01,039 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-01-25 02:18:01,039 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-01-25 02:18:01,040 >> loading file tokenizer.json from cache at None\n",
            "config.json: 100% 910/910 [00:00<00:00, 5.63MB/s]\n",
            "[INFO|configuration_utils.py:729] 2024-01-25 02:18:01,933 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/config.json\n",
            "configuration_qwen.py: 100% 2.35k/2.35k [00:00<00:00, 15.9MB/s]\n",
            "[INFO|configuration_utils.py:729] 2024-01-25 02:18:02,226 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/config.json\n",
            "[INFO|configuration_utils.py:792] 2024-01-25 02:18:02,227 >> Model config QWenConfig {\n",
            "  \"_name_or_path\": \"Qwen/Qwen-1_8B-Chat\",\n",
            "  \"architectures\": [\n",
            "    \"QWenLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_dropout_prob\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"Qwen/Qwen-1_8B-Chat--configuration_qwen.QWenConfig\",\n",
            "    \"AutoModelForCausalLM\": \"Qwen/Qwen-1_8B-Chat--modeling_qwen.QWenLMHeadModel\"\n",
            "  },\n",
            "  \"bf16\": false,\n",
            "  \"emb_dropout_prob\": 0.0,\n",
            "  \"fp16\": false,\n",
            "  \"fp32\": false,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"kv_channels\": 128,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"qwen\",\n",
            "  \"no_bias\": true,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"onnx_safe\": null,\n",
            "  \"rotary_emb_base\": 10000,\n",
            "  \"rotary_pct\": 1.0,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"seq_length\": 8192,\n",
            "  \"softmax_in_fp32\": false,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"QWenTokenizer\",\n",
            "  \"transformers_version\": \"4.37.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_cache_kernel\": false,\n",
            "  \"use_cache_quantization\": false,\n",
            "  \"use_dynamic_ntk\": true,\n",
            "  \"use_flash_attn\": \"auto\",\n",
            "  \"use_logn_attn\": true,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "modeling_qwen.py: 100% 55.6k/55.6k [00:00<00:00, 190MB/s]\n",
            "cpp_kernels.py: 100% 1.92k/1.92k [00:00<00:00, 14.4MB/s]\n",
            "qwen_generation_utils.py: 100% 14.6k/14.6k [00:00<00:00, 75.1MB/s]\n",
            "model.safetensors.index.json: 100% 14.7k/14.7k [00:00<00:00, 78.6MB/s]\n",
            "[INFO|modeling_utils.py:3478] 2024-01-25 02:18:03,264 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/model.safetensors.index.json\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/2.04G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 10.5M/2.04G [00:00<01:42, 19.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 21.0M/2.04G [00:00<01:11, 28.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 31.5M/2.04G [00:00<00:55, 36.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 41.9M/2.04G [00:01<00:45, 43.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 52.4M/2.04G [00:01<00:37, 52.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 62.9M/2.04G [00:01<00:31, 62.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 73.4M/2.04G [00:01<00:30, 65.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 83.9M/2.04G [00:01<00:26, 73.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 105M/2.04G [00:01<00:21, 90.5MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 126M/2.04G [00:01<00:18, 103MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 147M/2.04G [00:02<00:17, 105MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 168M/2.04G [00:02<00:16, 113MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 189M/2.04G [00:02<00:15, 117MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 210M/2.04G [00:02<00:15, 122MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 231M/2.04G [00:02<00:14, 126MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 252M/2.04G [00:02<00:14, 123MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 273M/2.04G [00:03<00:14, 124MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 294M/2.04G [00:03<00:13, 128MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 315M/2.04G [00:03<00:13, 128MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 336M/2.04G [00:03<00:13, 130MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 357M/2.04G [00:03<00:12, 132MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 377M/2.04G [00:03<00:13, 126MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 398M/2.04G [00:04<00:13, 125MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 419M/2.04G [00:04<00:12, 127MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 440M/2.04G [00:04<00:12, 127MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 461M/2.04G [00:04<00:12, 122MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 482M/2.04G [00:04<00:12, 121MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 503M/2.04G [00:04<00:12, 122MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 524M/2.04G [00:05<00:12, 125MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 545M/2.04G [00:05<00:11, 128MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 566M/2.04G [00:05<00:11, 129MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 587M/2.04G [00:05<00:11, 131MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 608M/2.04G [00:05<00:10, 133MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 629M/2.04G [00:05<00:10, 131MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 650M/2.04G [00:06<00:10, 130MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 671M/2.04G [00:06<00:10, 132MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 692M/2.04G [00:06<00:10, 125MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 713M/2.04G [00:06<00:10, 125MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 734M/2.04G [00:06<00:10, 124MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 755M/2.04G [00:08<00:46, 27.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 776M/2.04G [00:09<00:33, 37.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 797M/2.04G [00:09<00:25, 48.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 818M/2.04G [00:09<00:20, 58.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 839M/2.04G [00:09<00:16, 70.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 860M/2.04G [00:09<00:14, 79.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 881M/2.04G [00:09<00:12, 90.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 902M/2.04G [00:09<00:11, 99.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 923M/2.04G [00:10<00:11, 101MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 944M/2.04G [00:10<00:11, 92.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 965M/2.04G [00:10<00:13, 82.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 986M/2.04G [00:10<00:12, 86.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 996M/2.04G [00:11<00:12, 85.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 1.02G/2.04G [00:11<00:11, 86.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 1.03G/2.04G [00:11<00:12, 78.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 1.05G/2.04G [00:11<00:11, 88.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 1.07G/2.04G [00:11<00:09, 99.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 1.09G/2.04G [00:12<00:10, 91.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 1.10G/2.04G [00:12<00:10, 93.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 1.11G/2.04G [00:12<00:12, 77.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 1.12G/2.04G [00:12<00:12, 70.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 1.14G/2.04G [00:12<00:10, 87.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 1.16G/2.04G [00:12<00:08, 97.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 1.17G/2.04G [00:13<00:10, 79.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 1.20G/2.04G [00:13<00:09, 91.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 1.21G/2.04G [00:13<00:11, 71.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 1.23G/2.04G [00:13<00:09, 84.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 1.25G/2.04G [00:13<00:08, 97.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 1.27G/2.04G [00:14<00:08, 87.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 1.28G/2.04G [00:14<00:10, 70.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 1.30G/2.04G [00:14<00:09, 77.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 1.32G/2.04G [00:14<00:08, 88.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 1.33G/2.04G [00:15<00:07, 88.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 1.35G/2.04G [00:15<00:06, 102MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 1.37G/2.04G [00:15<00:06, 109MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 1.39G/2.04G [00:15<00:05, 117MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 1.42G/2.04G [00:15<00:05, 122MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 1.44G/2.04G [00:15<00:04, 121MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 1.46G/2.04G [00:15<00:04, 126MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 1.48G/2.04G [00:16<00:04, 129MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 1.50G/2.04G [00:17<00:16, 32.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 1.51G/2.04G [00:19<00:25, 21.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 1.53G/2.04G [00:19<00:17, 29.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 1.55G/2.04G [00:19<00:12, 40.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 1.57G/2.04G [00:19<00:08, 51.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 1.59G/2.04G [00:19<00:06, 64.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 1.61G/2.04G [00:19<00:05, 74.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 1.64G/2.04G [00:20<00:04, 83.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 1.66G/2.04G [00:20<00:04, 93.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 1.68G/2.04G [00:20<00:03, 102MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 1.70G/2.04G [00:20<00:03, 110MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 1.72G/2.04G [00:20<00:02, 117MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 1.74G/2.04G [00:20<00:02, 119MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 1.76G/2.04G [00:21<00:02, 121MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 1.78G/2.04G [00:21<00:02, 126MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 1.80G/2.04G [00:21<00:02, 113MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 1.82G/2.04G [00:21<00:01, 114MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 1.85G/2.04G [00:21<00:01, 120MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 1.87G/2.04G [00:21<00:01, 125MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 1.89G/2.04G [00:22<00:01, 123MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 1.91G/2.04G [00:22<00:01, 128MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 1.93G/2.04G [00:24<00:03, 30.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 1.95G/2.04G [00:24<00:02, 40.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 1.98G/2.04G [00:24<00:00, 58.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 2.00G/2.04G [00:24<00:00, 68.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 2.02G/2.04G [00:24<00:00, 72.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 2.04G/2.04G [00:25<00:00, 81.1MB/s]\n",
            "Downloading shards:  50% 1/2 [00:25<00:25, 25.49s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/1.63G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 10.5M/1.63G [00:00<00:30, 52.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 21.0M/1.63G [00:00<00:27, 58.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 31.5M/1.63G [00:00<00:25, 63.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 41.9M/1.63G [00:00<00:22, 72.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 52.4M/1.63G [00:00<00:21, 72.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 62.9M/1.63G [00:00<00:19, 81.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 83.9M/1.63G [00:01<00:15, 98.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 105M/1.63G [00:01<00:13, 111MB/s]  \u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 126M/1.63G [00:01<00:13, 113MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 147M/1.63G [00:01<00:12, 119MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 168M/1.63G [00:01<00:11, 124MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 189M/1.63G [00:01<00:12, 120MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 210M/1.63G [00:02<00:12, 116MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 231M/1.63G [00:02<00:12, 117MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 252M/1.63G [00:02<00:11, 119MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 273M/1.63G [00:02<00:11, 118MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 294M/1.63G [00:02<00:11, 119MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 315M/1.63G [00:02<00:10, 123MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 336M/1.63G [00:03<00:10, 120MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 357M/1.63G [00:03<00:11, 114MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 377M/1.63G [00:03<00:10, 121MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 398M/1.63G [00:03<00:10, 123MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 419M/1.63G [00:03<00:09, 124MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 440M/1.63G [00:03<00:09, 123MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 461M/1.63G [00:04<00:09, 127MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 482M/1.63G [00:04<00:08, 128MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 503M/1.63G [00:04<00:08, 130MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 524M/1.63G [00:04<00:08, 127MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 545M/1.63G [00:04<00:08, 127MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 566M/1.63G [00:04<00:08, 130MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 587M/1.63G [00:05<00:08, 125MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 608M/1.63G [00:05<00:08, 125MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 629M/1.63G [00:05<00:07, 129MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 650M/1.63G [00:05<00:08, 123MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 671M/1.63G [00:05<00:07, 123MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 692M/1.63G [00:05<00:07, 125MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 713M/1.63G [00:06<00:07, 127MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 734M/1.63G [00:06<00:07, 128MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 755M/1.63G [00:06<00:06, 129MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 776M/1.63G [00:06<00:06, 131MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 797M/1.63G [00:06<00:06, 128MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 818M/1.63G [00:06<00:06, 123MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 839M/1.63G [00:09<00:30, 26.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 860M/1.63G [00:09<00:22, 35.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 881M/1.63G [00:09<00:16, 46.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 902M/1.63G [00:09<00:13, 54.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 923M/1.63G [00:09<00:10, 65.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 944M/1.63G [00:10<00:09, 76.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 965M/1.63G [00:10<00:07, 85.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 986M/1.63G [00:10<00:06, 95.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 1.01G/1.63G [00:10<00:07, 88.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 1.03G/1.63G [00:10<00:07, 84.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 1.05G/1.63G [00:11<00:06, 87.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 1.06G/1.63G [00:11<00:06, 84.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 1.07G/1.63G [00:11<00:06, 87.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 1.09G/1.63G [00:11<00:05, 98.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 1.10G/1.63G [00:11<00:06, 86.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 1.12G/1.63G [00:11<00:05, 95.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 1.14G/1.63G [00:12<00:04, 103MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 1.16G/1.63G [00:12<00:05, 93.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 1.17G/1.63G [00:12<00:05, 89.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 1.18G/1.63G [00:12<00:05, 82.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 1.20G/1.63G [00:12<00:05, 80.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 1.21G/1.63G [00:12<00:05, 72.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 1.22G/1.63G [00:13<00:06, 67.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 1.23G/1.63G [00:13<00:06, 64.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 1.25G/1.63G [00:13<00:05, 74.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 1.26G/1.63G [00:13<00:05, 74.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 1.27G/1.63G [00:13<00:05, 67.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 1.29G/1.63G [00:14<00:04, 73.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 1.30G/1.63G [00:14<00:04, 74.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 1.31G/1.63G [00:14<00:04, 75.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 1.32G/1.63G [00:14<00:04, 67.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 1.33G/1.63G [00:14<00:04, 65.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 1.35G/1.63G [00:14<00:03, 83.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 1.37G/1.63G [00:15<00:02, 95.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 1.39G/1.63G [00:15<00:02, 103MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 1.42G/1.63G [00:15<00:02, 108MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 1.44G/1.63G [00:15<00:01, 99.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 1.45G/1.63G [00:15<00:01, 99.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 1.47G/1.63G [00:15<00:01, 107MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 1.49G/1.63G [00:16<00:01, 117MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 1.51G/1.63G [00:16<00:01, 112MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 1.53G/1.63G [00:16<00:00, 118MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 1.55G/1.63G [00:16<00:00, 123MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 1.57G/1.63G [00:16<00:00, 123MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 1.59G/1.63G [00:16<00:00, 125MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 1.61G/1.63G [00:17<00:00, 126MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 1.63G/1.63G [00:17<00:00, 94.5MB/s]\n",
            "Downloading shards: 100% 2/2 [00:42<00:00, 21.48s/it]\n",
            "[INFO|modeling_utils.py:1428] 2024-01-25 02:18:46,228 >> Instantiating QWenLMHeadModel model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:826] 2024-01-25 02:18:46,230 >> Generate config GenerationConfig {}\n",
            "\n",
            "Try importing flash-attention for faster inference...\n",
            "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
            "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
            "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
            "Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.24s/it]\n",
            "[INFO|modeling_utils.py:4352] 2024-01-25 02:18:51,252 >> All model checkpoint weights were used when initializing QWenLMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:4360] 2024-01-25 02:18:51,252 >> All the weights of QWenLMHeadModel were initialized from the model checkpoint at Qwen/Qwen-1_8B-Chat.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use QWenLMHeadModel for predictions without further training.\n",
            "generation_config.json: 100% 249/249 [00:00<00:00, 1.60MB/s]\n",
            "[INFO|configuration_utils.py:781] 2024-01-25 02:18:51,451 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/generation_config.json\n",
            "[INFO|configuration_utils.py:826] 2024-01-25 02:18:51,451 >> Generate config GenerationConfig {\n",
            "  \"chat_format\": \"chatml\",\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"max_new_tokens\": 512,\n",
            "  \"max_window_size\": 6144,\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"top_k\": 0,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "[WARNING|modeling_utils.py:2134] 2024-01-25 02:18:51,452 >> You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
            "01/25/2024 02:18:51 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.\n",
            "01/25/2024 02:18:51 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
            "01/25/2024 02:18:51 - INFO - llmtuner.model.loader - trainable params: 3145728 || all params: 1839974400 || trainable%: 0.1710\n",
            "01/25/2024 02:18:51 - INFO - llmtuner.data.template - Add eos token: <|endoftext|>\n",
            "01/25/2024 02:18:51 - INFO - llmtuner.data.template - Add pad token: <|endoftext|>\n",
            "01/25/2024 02:18:51 - INFO - llmtuner.data.template - Replace eos token: <|im_end|>\n",
            "https://huggingface.co/datasets/silk-road/Haruhi-Dialogue-Speaker-Extract/resolve/06f50be9c4343c7d83b28e9c65831c052bb658cb/README.md not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/f2b87bc87289d03a7104fee259db1bf93fa6c3bf493bdc8347d58c70f5ae208a.4dfe18434a5ab0f61e06906e4b54d047be4b04733b69259fe73162fb38668887.incomplete\n",
            "Downloading readme: 100% 27.0/27.0 [00:00<00:00, 189kB/s]\n",
            "storing https://huggingface.co/datasets/silk-road/Haruhi-Dialogue-Speaker-Extract/resolve/06f50be9c4343c7d83b28e9c65831c052bb658cb/README.md in cache at /root/.cache/huggingface/datasets/downloads/f2b87bc87289d03a7104fee259db1bf93fa6c3bf493bdc8347d58c70f5ae208a.4dfe18434a5ab0f61e06906e4b54d047be4b04733b69259fe73162fb38668887\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/f2b87bc87289d03a7104fee259db1bf93fa6c3bf493bdc8347d58c70f5ae208a.4dfe18434a5ab0f61e06906e4b54d047be4b04733b69259fe73162fb38668887\n",
            "No config specified, defaulting to the single config: haruhi-dialogue-speaker-extract/default\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset haruhi-dialogue-speaker-extract (/root/.cache/huggingface/datasets/silk-road___haruhi-dialogue-speaker-extract/default/0.0.0/06f50be9c4343c7d83b28e9c65831c052bb658cb)\n",
            "Downloading and preparing dataset haruhi-dialogue-speaker-extract/default to /root/.cache/huggingface/datasets/silk-road___haruhi-dialogue-speaker-extract/default/0.0.0/06f50be9c4343c7d83b28e9c65831c052bb658cb...\n",
            "Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "hf://datasets/silk-road/Haruhi-Dialogue-Speaker-Extract@06f50be9c4343c7d83b28e9c65831c052bb658cb/chinese_dialogue_clean.jsonl not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/e3ef5f2746e4019ead1fb3b8c7048c5cf216a2c8e40c21bd05a0151280647e68.incomplete\n",
            "Downloading data: 100% 225M/225M [00:18<00:00, 12.4MB/s]\n",
            "storing hf://datasets/silk-road/Haruhi-Dialogue-Speaker-Extract@06f50be9c4343c7d83b28e9c65831c052bb658cb/chinese_dialogue_clean.jsonl in cache at /root/.cache/huggingface/datasets/downloads/e3ef5f2746e4019ead1fb3b8c7048c5cf216a2c8e40c21bd05a0151280647e68\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/e3ef5f2746e4019ead1fb3b8c7048c5cf216a2c8e40c21bd05a0151280647e68\n",
            "hf://datasets/silk-road/Haruhi-Dialogue-Speaker-Extract@06f50be9c4343c7d83b28e9c65831c052bb658cb/dialogue_negative.jsonl not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/ec8ea87ff213440d23e5947c6b7ec3b8e6cf9def384cbda7f450a8d051a9e93f.incomplete\n",
            "Downloading data: 100% 6.05M/6.05M [00:00<00:00, 11.8MB/s]\n",
            "storing hf://datasets/silk-road/Haruhi-Dialogue-Speaker-Extract@06f50be9c4343c7d83b28e9c65831c052bb658cb/dialogue_negative.jsonl in cache at /root/.cache/huggingface/datasets/downloads/ec8ea87ff213440d23e5947c6b7ec3b8e6cf9def384cbda7f450a8d051a9e93f\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/ec8ea87ff213440d23e5947c6b7ec3b8e6cf9def384cbda7f450a8d051a9e93f\n",
            "hf://datasets/silk-road/Haruhi-Dialogue-Speaker-Extract@06f50be9c4343c7d83b28e9c65831c052bb658cb/english_dialogue_clean.jsonl not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/e6c4a8eeb88c4675455e12728cff024014da196db1743b3f70e14690e8d59174.incomplete\n",
            "Downloading data: 100% 125M/125M [00:13<00:00, 9.26MB/s]\n",
            "storing hf://datasets/silk-road/Haruhi-Dialogue-Speaker-Extract@06f50be9c4343c7d83b28e9c65831c052bb658cb/english_dialogue_clean.jsonl in cache at /root/.cache/huggingface/datasets/downloads/e6c4a8eeb88c4675455e12728cff024014da196db1743b3f70e14690e8d59174\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/e6c4a8eeb88c4675455e12728cff024014da196db1743b3f70e14690e8d59174\n",
            "Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "Generating train split\n",
            "Generating train split: 72927 examples [00:04, 17903.10 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "Dataset haruhi-dialogue-speaker-extract downloaded and prepared to /root/.cache/huggingface/datasets/silk-road___haruhi-dialogue-speaker-extract/default/0.0.0/06f50be9c4343c7d83b28e9c65831c052bb658cb. Subsequent calls will reuse this data.\n",
            "Converting format of dataset:   0% 0/72927 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/silk-road___haruhi-dialogue-speaker-extract/default/0.0.0/06f50be9c4343c7d83b28e9c65831c052bb658cb/cache-8f24396e266c91ea.arrow\n",
            "Converting format of dataset: 100% 72927/72927 [00:04<00:00, 16935.38 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/72927 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/silk-road___haruhi-dialogue-speaker-extract/default/0.0.0/06f50be9c4343c7d83b28e9c65831c052bb658cb/cache-93c71143cb471ec9.arrow\n",
            "Running tokenizer on dataset:  43% 31000/72927 [01:04<01:31, 458.62 examples/s]"
          ]
        }
      ],
      "source": [
        "! CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path  Qwen/Qwen-1_8B-Chat\\\n",
        "    --do_train True\\\n",
        "    --dataset haruhi_dialogue_extract \\\n",
        "    --template qwen \\\n",
        "    --finetuning_type lora \\\n",
        "    --lora_target c_attn \\\n",
        "    --lora_rank 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --output_dir qwen_1_8-finetuned \\\n",
        "    --overwrite_output_dir \\\n",
        "    --overwrite_cache \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --logging_steps 100 \\\n",
        "    --save_steps 1000 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --plot_loss \\\n",
        "    --fp16 True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9OaXCFQLze_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GTSnqTT4YcBh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIWh5UBLsUgqCxvcHVUWtt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}