{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/Zero-Haruhi/blob/main/notebook/%E5%AF%B9%E8%AF%9D%E6%8A%BD%E5%8F%96_%E4%BD%BF%E7%94%A8colab%E8%BF%9B%E8%A1%8Cfinetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zElyM_0tGpZH",
        "outputId": "accd57fa-0c9e-4490-dd19-33c49e261bcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.11.17)\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: read).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade huggingface_hub\n",
        "# !huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjFuGKxUFvbY",
        "outputId": "6a7f31e5-9972-4b60-e295-5e3cec59f7d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 6750, done.\u001b[K\n",
            "remote: Counting objects: 100% (769/769), done.\u001b[K\n",
            "remote: Compressing objects: 100% (316/316), done.\u001b[K\n",
            "remote: Total 6750 (delta 506), reused 661 (delta 451), pack-reused 5981\u001b[K\n",
            "Receiving objects: 100% (6750/6750), 204.48 MiB | 20.83 MiB/s, done.\n",
            "Resolving deltas: 100% (4857/4857), done.\n",
            "Updating files: 100% (140/140), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAcYKc9qGwcY",
        "outputId": "6782f84c-bfef-4a4c-dd78-73df2880ffb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/LLaMA-Factory\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.7/79.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%cd LLaMA-Factory\n",
        "!pip install -q -r requirements.txt\n",
        "!pip install -q einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OSdjnmbHCSz"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIhsKveEIT7C",
        "outputId": "d8a401ce-d594-4ce9-a380-2fd18c511569"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ls: cannot access '/content/LLaMA-Factory/dataset': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls /content/LLaMA-Factory/dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFANKTp0JWSZ",
        "outputId": "ac06bc64-d529-4c0c-d23a-050eb4443cc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfrKNsHhJbKC",
        "outputId": "197be821-47db-4b0b-e2b0-79126b9a0a08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/CardBuild/HaruhiZero/chinese_dialogue_clean.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/dialogue_extract.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/dialogue_negative.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/english_dialogue_clean.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/english_dialogue_extract.jsonl\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/CardBuild/HaruhiZero/*dialogue*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDYOUoHDK4x9"
      },
      "source": [
        "\n",
        "请为我实现一段python代码，把input_path下的几个file_names对应的jsonl文件(utf-8, ensure_ascii=False) 进行读取\n",
        "\n",
        "然后以json格式写入target_file"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "三个文件分别在\n",
        "\n",
        "清洁后的中文对话\n",
        "https://drive.google.com/file/d/1Oc3R5SCiVXL4mwKBx1Zh0PS9Fv6YeYos/view?usp=sharing\n",
        "\n",
        "对话抽取-负样例\n",
        "https://drive.google.com/file/d/1u4HuXgI7Ava8HIf_L57x1HRQygZ6_tiX/view?usp=sharing\n",
        "\n",
        "清洁后的英文对话\n",
        "https://drive.google.com/file/d/1--iJ_sqiECjwtcdLkbiAmvyP_RodTl4d/view?usp=sharing\n"
      ],
      "metadata": {
        "id": "cxEZBZDJWR6l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYZqDgizJdyy"
      },
      "outputs": [],
      "source": [
        "input_path = \"/content/drive/MyDrive/CardBuild/HaruhiZero/\"\n",
        "file_names = [\"chinese_dialogue_clean.jsonl\",\"dialogue_negative.jsonl\",\"english_dialogue_clean.jsonl\"]\n",
        "# file_names = [\"chinese_dialogue_clean.jsonl\"]\n",
        "target_path = \"/content/LLaMA-Factory/data/\"\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "target_file = \"/content/LLaMA-Factory/data/dialogue_merged.json\"\n",
        "\n",
        "datas = []\n",
        "for file_name in file_names:\n",
        "    with open(os.path.join(input_path, file_name), 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data = json.loads(line)\n",
        "            # delete the key except system and conversations\n",
        "            if \"source\" in data:\n",
        "                del data[\"source\"]\n",
        "            datas.append(data)\n",
        "\n",
        "with open(target_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "target_file_jsonl = \"/content/LLaMA-Factory/data/dialogue_merged.jsonl\"\n",
        "\n",
        "\n",
        "with open(target_file_jsonl, 'w', encoding='utf-8') as f:\n",
        "    for data in datas:\n",
        "        f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVFtJx-aMk9C"
      },
      "outputs": [],
      "source": [
        "for data in datas:\n",
        "    if \"system\" not in data:\n",
        "        print(\"system not in data\")\n",
        "    elif \"conversations\" not in data:\n",
        "        print(\"conversations not in data\")\n",
        "    elif len(data[\"conversations\"]) != 2:\n",
        "        print(\"conversations is empty\")\n",
        "    else:\n",
        "        text = data[\"conversations\"][0]\n",
        "        continue\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJnK1Za2J-0A"
      },
      "source": [
        "请为我实现一段python代码，对于/content/LLaMA-Factory/data/dataset_info.json 备份到dataset_info_back.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exqKxSYMLKMf",
        "outputId": "04679ff5-0e13-4af7-afc6-699d13eb8ab0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backup of /content/LLaMA-Factory/data/dataset_info.json to /content/LLaMA-Factory/data/dataset_info_back.json succeeded!\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "original_file = '/content/LLaMA-Factory/data/dataset_info.json'\n",
        "backup_file = '/content/LLaMA-Factory/data/dataset_info_back.json'\n",
        "\n",
        "# Make a copy of the original file\n",
        "shutil.copy(original_file, backup_file)\n",
        "\n",
        "# Verify copy succeeded\n",
        "if os.path.exists(backup_file):\n",
        "    print(f'Backup of {original_file} to {backup_file} succeeded!')\n",
        "else:\n",
        "    print(f'Backup of {original_file} to {backup_file} failed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48s-ojJQLOsZ"
      },
      "source": [
        "为我实现一段python代码，复制/content/LLaMA-Factory/data/dataset_info.json中的信息\n",
        "\n",
        "写入到 /content/LLaMA-Factory/data/dataset_info.json\n",
        "\n",
        "并且在末尾添加\n",
        "\n",
        "\n",
        "```json\n",
        "\"haruhi_dialogue_extract\": {\n",
        "  \"file_name\": \"dialogue_merged.json\",\n",
        "  \"columns\": {\n",
        "    \"messages\": \"conversations\",\n",
        "    \"system\": \"system\",\n",
        "    \"tools\": \"tools\"\n",
        "  },\n",
        "  \"tags\": {\n",
        "    \"role_tag\": \"from\",\n",
        "    \"content_tag\": \"value\"\n",
        "  }\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ik4Bvq1LNpE",
        "outputId": "221c1019-0a28-4f8e-ab9d-7464bd4d507c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data copied and new info appended to /content/LLaMA-Factory/data/dataset_info.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "original_file = '/content/LLaMA-Factory/data/dataset_info.json'\n",
        "new_file = '/content/LLaMA-Factory/data/dataset_info.json'\n",
        "\n",
        "with open(original_file, 'r') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "data['haruhi_dialogue_extract'] = {\n",
        "  \"file_name\": \"/content/LLaMA-Factory/data/dialogue_merged.jsonl\",\n",
        "  \"formatting\":\"sharegpt\",\n",
        "  \"columns\": {\n",
        "    \"messages\": \"conversations\",\n",
        "    \"system\": \"system\",\n",
        "  },\n",
        "  \"tags\": {\n",
        "    \"role_tag\": \"from\",\n",
        "    \"content_tag\": \"value\",\n",
        "    \"user_tag\": \"human\",\n",
        "    \"assistant_tag\":\"gpt\",\n",
        "  }\n",
        "}\n",
        "\n",
        "with open(new_file, 'w') as f:\n",
        "  json.dump(data, f, indent=2)\n",
        "\n",
        "print('Data copied and new info appended to', new_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaruMxYdL6QU",
        "outputId": "74872790-5baf-4c81-c2af-5bd6c39d1b7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers_stream_generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q tiktoken transformers_stream_generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqZswBFMN6-C"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhZPqKBcGHau",
        "outputId": "0a4999c8-4ebe-4ff3-9fa5-6f663481785c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-01-25 01:49:26.076683: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-25 01:49:26.076737: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-25 01:49:26.078081: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-25 01:49:27.281205: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "01/25/2024 01:49:29 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
            "[INFO|training_args.py:1828] 2024-01-25 01:49:29,415 >> PyTorch: setting up devices\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1741: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
            "  warnings.warn(\n",
            "01/25/2024 01:49:29 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1\n",
            "  distributed training: True, compute dtype: torch.float16\n",
            "01/25/2024 01:49:29 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0002,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=qwen_1_8-finetuned/runs/Jan25_01-49-29_29f45e3f8461,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=100,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=cosine,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=qwen_1_8-finetuned,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=qwen_1_8-finetuned,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=1000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-01-25 01:49:29,698 >> loading file qwen.tiktoken from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/qwen.tiktoken\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-01-25 01:49:29,698 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-01-25 01:49:29,698 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-01-25 01:49:29,698 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-01-25 01:49:29,698 >> loading file tokenizer.json from cache at None\n",
            "[INFO|configuration_utils.py:729] 2024-01-25 01:49:30,468 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/config.json\n",
            "[INFO|configuration_utils.py:729] 2024-01-25 01:49:30,653 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/config.json\n",
            "[INFO|configuration_utils.py:792] 2024-01-25 01:49:30,654 >> Model config QWenConfig {\n",
            "  \"_name_or_path\": \"Qwen/Qwen-1_8B-Chat\",\n",
            "  \"architectures\": [\n",
            "    \"QWenLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_dropout_prob\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"Qwen/Qwen-1_8B-Chat--configuration_qwen.QWenConfig\",\n",
            "    \"AutoModelForCausalLM\": \"Qwen/Qwen-1_8B-Chat--modeling_qwen.QWenLMHeadModel\"\n",
            "  },\n",
            "  \"bf16\": false,\n",
            "  \"emb_dropout_prob\": 0.0,\n",
            "  \"fp16\": false,\n",
            "  \"fp32\": false,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"kv_channels\": 128,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"qwen\",\n",
            "  \"no_bias\": true,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"onnx_safe\": null,\n",
            "  \"rotary_emb_base\": 10000,\n",
            "  \"rotary_pct\": 1.0,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"seq_length\": 8192,\n",
            "  \"softmax_in_fp32\": false,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"QWenTokenizer\",\n",
            "  \"transformers_version\": \"4.37.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_cache_kernel\": false,\n",
            "  \"use_cache_quantization\": false,\n",
            "  \"use_dynamic_ntk\": true,\n",
            "  \"use_flash_attn\": \"auto\",\n",
            "  \"use_logn_attn\": true,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3478] 2024-01-25 01:49:30,781 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1428] 2024-01-25 01:49:30,782 >> Instantiating QWenLMHeadModel model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:826] 2024-01-25 01:49:30,783 >> Generate config GenerationConfig {}\n",
            "\n",
            "Try importing flash-attention for faster inference...\n",
            "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
            "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
            "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
            "Loading checkpoint shards: 100% 2/2 [00:05<00:00,  2.73s/it]\n",
            "[INFO|modeling_utils.py:4352] 2024-01-25 01:49:36,597 >> All model checkpoint weights were used when initializing QWenLMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:4360] 2024-01-25 01:49:36,597 >> All the weights of QWenLMHeadModel were initialized from the model checkpoint at Qwen/Qwen-1_8B-Chat.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use QWenLMHeadModel for predictions without further training.\n",
            "[INFO|configuration_utils.py:781] 2024-01-25 01:49:36,687 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/generation_config.json\n",
            "[INFO|configuration_utils.py:826] 2024-01-25 01:49:36,688 >> Generate config GenerationConfig {\n",
            "  \"chat_format\": \"chatml\",\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"max_new_tokens\": 512,\n",
            "  \"max_window_size\": 6144,\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"top_k\": 0,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "[WARNING|modeling_utils.py:2134] 2024-01-25 01:49:36,689 >> You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
            "01/25/2024 01:49:36 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.\n",
            "01/25/2024 01:49:36 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
            "01/25/2024 01:49:36 - INFO - llmtuner.model.loader - trainable params: 3145728 || all params: 1839974400 || trainable%: 0.1710\n",
            "01/25/2024 01:49:36 - INFO - llmtuner.data.template - Add eos token: <|endoftext|>\n",
            "01/25/2024 01:49:36 - INFO - llmtuner.data.template - Add pad token: <|endoftext|>\n",
            "01/25/2024 01:49:36 - INFO - llmtuner.data.template - Replace eos token: <|im_end|>\n",
            "01/25/2024 01:49:36 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.\n",
            "Using custom data configuration default-7b563057c8b8ec89\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-7b563057c8b8ec89/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-7b563057c8b8ec89/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-7b563057c8b8ec89/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Converting format of dataset:   0% 0/72927 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7b563057c8b8ec89/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-eebdf306e7652ea7.arrow\n",
            "Converting format of dataset: 100% 72927/72927 [00:04<00:00, 17624.58 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/72927 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7b563057c8b8ec89/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43b5017f793f2e0d.arrow\n",
            "Running tokenizer on dataset: 100% 72927/72927 [02:57<00:00, 411.13 examples/s]\n",
            "input_ids:\n",
            "[151644, 8948, 198, 89012, 22382, 1355, 14311, 3837, 118797, 90919, 9370, 105051, 90395, 66017, 17714, 2236, 68805, 271, 10061, 594, 1744, 432, 3019, 553, 3019, 198, 16, 13, 62079, 1946, 14311, 1119, 17432, 3561, 3837, 105653, 18493, 1708, 44931, 198, 17, 13, 96155, 121, 18158, 104588, 99700, 105051, 104597, 21276, 3837, 104317, 104588, 100908, 9370, 104283, 17340, 1053, 553, 11, 53497, 246, 99871, 18493, 443, 72995, 15946, 151645, 198, 151644, 872, 198, 100039, 44729, 30858, 101882, 23990, 102482, 105444, 101056, 106183, 100363, 3837, 101889, 101882, 115083, 104808, 110878, 3837, 103941, 105705, 77540, 69442, 115978, 3837, 114808, 101888, 75437, 75437, 99733, 99733, 99424, 50404, 9370, 105051, 1773, 100039, 44729, 30858, 103941, 23031, 105705, 77540, 102565, 99938, 17714, 104629, 102401, 3837, 101882, 115083, 28951, 25710, 8997, 2073, 103933, 35946, 99405, 100146, 56568, 100854, 99938, 3837, 35946, 100684, 101482, 3837, 99519, 104806, 35946, 105715, 99938, 100363, 3837, 104662, 36993, 105214, 99677, 47534, 3837, 99190, 104355, 102410, 104789, 99899, 106800, 854, 100039, 44729, 30858, 44793, 8997, 115083, 100307, 103546, 2073, 43288, 102095, 106800, 3837, 56568, 106374, 100623, 99190, 104355, 115978, 99212, 101258, 104483, 106800, 3837, 113444, 100018, 101315, 101315, 101037, 88774, 106283, 3837, 116315, 101920, 99364, 99476, 107008, 8997, 100039, 44729, 30858, 102837, 73145, 57452, 115083, 3837, 107038, 104461, 28072, 102300, 106983, 23990, 102482, 3837, 118711, 9370, 118724, 102764, 34204, 77144, 20742, 2073, 101045, 3837, 103945, 101051, 75108, 99414, 99385, 104339, 107446, 3837, 99212, 102290, 101041, 44063, 99612, 113988, 105313, 120011, 63109, 103401, 99898, 3837, 98641, 110359, 101099, 69249, 102290, 88774, 2073, 56568, 100245, 854, 100369, 18600, 99694, 39426, 103427, 8997, 103969, 71134, 99190, 100039, 44729, 30858, 99476, 103206, 2073, 35946, 109916, 104447, 88774, 102068, 3837, 42411, 102068, 105958, 104447, 9370, 3837, 23990, 102482, 57218, 42411, 65676, 99396, 65676, 99535, 3837, 100002, 42411, 99883, 3837, 102347, 107516, 102688, 104970, 75108, 99414, 99385, 101102, 9370, 102011, 104606, 3837, 42411, 100678, 104447, 11319, 42411, 102347, 99486, 99730, 99654, 100854, 8997, 68536, 42411, 99786, 105745, 100006, 118433, 26939, 100648, 47606, 3837, 23031, 99283, 104718, 100531, 35727, 107690, 3837, 42411, 102130, 100006, 33108, 42411, 14777, 99191, 3837, 30440, 14777, 116244, 100363, 3837, 105799, 112356, 36993, 11622, 75108, 99414, 99385, 3837, 101982, 100710, 42411, 102196, 9370, 105288, 1773, 60894, 102157, 100006, 105921, 108165, 118433, 100623, 96050, 117093, 104461, 99670, 106262, 3837, 101885, 104501, 99650, 102747, 102323, 3837, 99947, 99541, 106262, 3837, 99517, 99509, 9370, 106742, 107202, 3837, 99670, 20412, 23990, 102482, 3837, 100136, 42411, 104871, 107169, 99733, 100039, 44729, 30858, 8997, 99999, 3837, 99246, 101373, 99916, 99998, 49828, 30534, 99904, 42411, 100638, 8997, 115083, 9370, 112948, 106823, 121967, 71618, 3837, 100155, 101192, 90663, 44793, 2073, 102068, 3837, 56568, 102068, 105958, 104447, 9370, 3837, 105365, 108089, 99411, 39426, 36587, 36587, 108809, 3837, 105622, 99260, 88051, 88774, 100039, 44729, 30858, 104203, 49238, 23990, 102482, 108231, 3837, 104203, 104094, 2073, 105509, 56568, 113955, 57443, 70927, 107484, 100363, 3837, 73670, 100640, 36587, 3837, 35946, 104047, 99319, 101413, 99319, 49187, 9370, 1773, 100009, 104515, 104211, 56568, 100380, 3837, 105665, 49828, 99236, 99234, 3837, 35946, 101413, 49828, 99236, 99234, 3837, 42411, 103939, 82647, 101883, 104573, 1773, 100155, 105665, 49828, 99843, 34187, 3837, 99212, 35946, 112639, 106162, 101149, 3837, 42411, 103977, 42140, 101883, 104573, 88774, 115083, 48934, 102210, 44793, 2073, 109111, 16530, 59879, 108792, 20221, 99327, 88774, 100039, 44729, 30858, 106823, 107319, 3837, 102135, 99461, 105962, 99212, 101492, 99612, 113988, 105313, 120011, 63109, 9370, 102188, 81812, 3837, 101077, 111798, 1773, 151645, 198, 151644, 77091, 198, 4913, 1708, 788, 6523, 40666, 237, 44729, 30858, 101882, 115083, 104808, 110878, 90395, 23031, 105705, 77540, 69442, 99938, 17714, 104629, 102401, 3837, 101882, 115083, 28951, 25710, 1773, 114808, 101888, 75437, 75437, 99733, 99733, 99424, 50404, 9370, 105051, 1773, 106283, 101197, 104432, 3837, 100039, 44729, 30858, 104643, 101041, 44063, 99612, 113988, 105313, 120011, 63109, 103401, 99898, 104339, 3837, 114808, 115083, 9370, 106319, 33108, 117978, 1773, 497, 330, 443, 72995, 788, 61753, 11817, 361, 788, 330, 103933, 35946, 99405, 100146, 56568, 100854, 99938, 3837, 35946, 100684, 101482, 3837, 99519, 104806, 35946, 105715, 99938, 100363, 3837, 104662, 36993, 105214, 99677, 47534, 3837, 99190, 104355, 102410, 104789, 99899, 106800, 497, 330, 83259, 3710, 788, 330, 100039, 44729, 30858, 14345, 5212, 11817, 361, 788, 330, 43288, 102095, 106800, 3837, 56568, 106374, 100623, 99190, 104355, 115978, 99212, 101258, 104483, 106800, 3837, 113444, 100018, 101315, 101315, 101037, 497, 330, 83259, 3710, 788, 330, 115083, 14345, 5212, 11817, 361, 788, 330, 101045, 3837, 103945, 101051, 75108, 99414, 99385, 104339, 107446, 3837, 99212, 102290, 101041, 44063, 99612, 113988, 105313, 120011, 63109, 103401, 99898, 3837, 98641, 110359, 101099, 69249, 102290, 497, 330, 83259, 3710, 788, 330, 100039, 44729, 30858, 14345, 5212, 11817, 361, 788, 330, 56568, 100245, 497, 330, 83259, 3710, 788, 330, 115083, 14345, 5212, 11817, 361, 788, 330, 35946, 109916, 104447, 497, 330, 83259, 3710, 788, 330, 100039, 44729, 30858, 14345, 5212, 11817, 361, 788, 330, 102068, 3837, 56568, 102068, 105958, 104447, 9370, 3837, 105365, 108089, 99411, 39426, 36587, 36587, 108809, 3837, 105622, 99260, 88051, 497, 330, 83259, 3710, 788, 330, 115083, 14345, 5212, 11817, 361, 788, 330, 105509, 56568, 113955, 57443, 70927, 107484, 100363, 3837, 35946, 104047, 99319, 101413, 99319, 49187, 9370, 1773, 100009, 104515, 104211, 56568, 100380, 3837, 105665, 49828, 99236, 99234, 3837, 35946, 101413, 49828, 99236, 99234, 3837, 42411, 103939, 82647, 101883, 104573, 1773, 100155, 105665, 49828, 99843, 34187, 3837, 99212, 35946, 112639, 106162, 101149, 3837, 42411, 103977, 42140, 101883, 104573, 497, 330, 83259, 3710, 788, 330, 100039, 44729, 30858, 14345, 5212, 11817, 361, 788, 330, 109111, 16530, 59879, 108792, 20221, 99327, 497, 330, 83259, 3710, 788, 330, 115083, 14345, 5212, 11817, 361, 788, 330, 101241, 99821, 12857, 74386, 3837, 101451, 99807, 45629, 26288, 102844, 3837, 35946, 104624, 100012, 105572, 99464, 99164, 99172, 497, 330, 83259, 3710, 788, 330, 100039, 44729, 30858, 14345, 5212, 11817, 361, 788, 330, 99494, 34187, 497, 330, 83259, 3710, 788, 330, 100039, 44729, 30858, 9207, 13989, 151645]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "给定input paragraph，抽取其中的对话，并输出为json格式\n",
            "\n",
            "Let's think it step by step\n",
            "1. summarize input paragraph into bullet format，存储在summary字段\n",
            "2. 抽取每一句对话的内容 dialogue，判断每一句话的说话人 said by, 存储在conversations中<|im_end|>\n",
            "<|im_start|>user\n",
            "夏子明要求单翔重复之前说过的话，然后要求黎明给他做饭，最终吃了两份饭菜，引发了关于打打杀杀生活选择的对话。夏子明最终以吃了两碗饭为胜利理由，要求黎明认输。\n",
            "“可是我吃的是你做的饭，我并不亏，因为要是我去做饭的话，不但会浪费灵力，做出来的有可能还没这么美味”夏子明道。\n",
            "黎明摇摇头“这并不是美味，你身后的人做出来的饭菜那才是真正的美味，有兴趣一起尝尝吗”\n",
            "气氛，刹那之间便冷了下来。\n",
            "夏子明目光直视黎明，一手却是提起了身边的单翔，血腥的笑意溢于言表“其实，想要得到五彩石的方法还有一个，那便是直接将七窍玲珑心掏出来，装在我的身体里便是”\n",
            "“你敢”两个字脱口而出。\n",
            "这次换做夏子明冷笑了“我有何不敢”\n",
            "确实，他确实没什么不敢的，单翔与他非亲非故，对于他来说，本来就是一个用来激发五彩石力量的工具而已，他为什么不敢？他本来就是应该这样做的。\n",
            "而他却没有什么能够威慑到他的存在，以自己现在的逆天之力，他或许能够和他一战，可一开战的话，他就势必会用五彩石，从而加快他动作的节奏。然唯一能够对他形成的威慑的人，在这一刻却是不会出手，而且就算他们陷入危机，沉静出手，她救的只会是他，不会是单翔，且他也不会动手杀夏子明。\n",
            "所以，眼下的事情还是得要靠他解决。\n",
            "黎明的眉头微微蹙起，若有所思道“确实，你确实没什么不敢的，你就当我随口说说罢了，不必较真”\n",
            "夏子明一边解单翔的衣服，一边说道“若是你还有什么话没说完的话，可以继续说，我也可以边挖边听的。只是我要提醒你一点，你说得越快，我挖得越快，他就会少一些痛苦。若你说得慢了，那我便会放下速度，他也会多一些痛苦”\n",
            "黎明微怒道“你怎么不按套路出牌”\n",
            "夏子明微微一笑，看来已经找到了那颗七窍玲珑心的准确位置，准备下手。<|im_end|>\n",
            "<|im_start|>assistant\n",
            "{\"summary\": \"- 夏子明要求黎明给他做饭，并以吃了两份饭为胜利理由，要求黎明认输。引发了关于打打杀杀生活选择的对话。气氛变得紧张，夏子明提出了直接将七窍玲珑心掏出来的方法，引发了黎明的警告和反驳。\", \"conversations\": [{\"dialogue\": \"可是我吃的是你做的饭，我并不亏，因为要是我去做饭的话，不但会浪费灵力，做出来的有可能还没这么美味\", \"said_by\": \"夏子明\"}, {\"dialogue\": \"这并不是美味，你身后的人做出来的饭菜那才是真正的美味，有兴趣一起尝尝吗\", \"said_by\": \"黎明\"}, {\"dialogue\": \"其实，想要得到五彩石的方法还有一个，那便是直接将七窍玲珑心掏出来，装在我的身体里便是\", \"said_by\": \"夏子明\"}, {\"dialogue\": \"你敢\", \"said_by\": \"黎明\"}, {\"dialogue\": \"我有何不敢\", \"said_by\": \"夏子明\"}, {\"dialogue\": \"确实，你确实没什么不敢的，你就当我随口说说罢了，不必较真\", \"said_by\": \"黎明\"}, {\"dialogue\": \"若是你还有什么话没说完的话，我也可以边挖边听的。只是我要提醒你一点，你说得越快，我挖得越快，他就会少一些痛苦。若你说得慢了，那我便会放下速度，他也会多一些痛苦\", \"said_by\": \"夏子明\"}, {\"dialogue\": \"你怎么不按套路出牌\", \"said_by\": \"黎明\"}, {\"dialogue\": \"墨守成规，乃兵家大忌，我这也是为了自身的安全着想\", \"said_by\": \"夏子明\"}, {\"dialogue\": \"怎么了\", \"said_by\": \"夏子明\"}]}<|im_end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4913, 1708, 788, 6523, 40666, 237, 44729, 30858, 101882, 115083, 104808, 110878, 90395, 23031, 105705, 77540, 69442, 99938, 17714, 104629, 102401, 3837, 101882, 115083, 28951, 25710, 1773, 114808, 101888, 75437, 75437, 99733, 99733, 99424, 50404, 9370, 105051, 1773, 106283, 101197, 104432, 3837, 100039, 44729, 30858, 104643, 101041, 44063, 99612, 113988, 105313, 120011, 63109, 103401, 99898, 104339, 3837, 114808, 115083, 9370, 106319, 33108, 117978, 1773, 497, 330, 443, 72995, 788, 61753, 11817, 361, 788, 330, 103933, 35946, 99405, 100146, 56568, 100854, 99938, 3837, 35946, 100684, 101482, 3837, 99519, 104806, 35946, 105715, 99938, 100363, 3837, 104662, 36993, 105214, 99677, 47534, 3837, 99190, 104355, 102410, 104789, 99899, 106800, 497, 330, 83259, 3710, 788, 330, 100039, 44729, 30858, 14345, 5212, 11817, 361, 788, 330, 43288, 102095, 106800, 3837, 56568, 106374, 100623, 99190, 104355, 115978, 99212, 101258, 104483, 106800, 3837, 113444, 100018, 101315, 101315, 101037, 497, 330, 83259, 3710, 788, 330, 115083, 14345, 5212, 11817, 361, 788, 330, 101045, 3837, 103945, 101051, 75108, 99414, 99385, 104339, 107446, 3837, 99212, 102290, 101041, 44063, 99612, 113988, 105313, 120011, 63109, 103401, 99898, 3837, 98641, 110359, 101099, 69249, 102290, 497, 330, 83259, 3710, 788, 330, 100039, 44729, 30858, 14345, 5212, 11817, 361, 788, 330, 56568, 100245, 497, 330, 83259, 3710, 788, 330, 115083, 14345, 5212, 11817, 361, 788, 330, 35946, 109916, 104447, 497, 330, 83259, 3710, 788, 330, 100039, 44729, 30858, 14345, 5212, 11817, 361, 788, 330, 102068, 3837, 56568, 102068, 105958, 104447, 9370, 3837, 105365, 108089, 99411, 39426, 36587, 36587, 108809, 3837, 105622, 99260, 88051, 497, 330, 83259, 3710, 788, 330, 115083, 14345, 5212, 11817, 361, 788, 330, 105509, 56568, 113955, 57443, 70927, 107484, 100363, 3837, 35946, 104047, 99319, 101413, 99319, 49187, 9370, 1773, 100009, 104515, 104211, 56568, 100380, 3837, 105665, 49828, 99236, 99234, 3837, 35946, 101413, 49828, 99236, 99234, 3837, 42411, 103939, 82647, 101883, 104573, 1773, 100155, 105665, 49828, 99843, 34187, 3837, 99212, 35946, 112639, 106162, 101149, 3837, 42411, 103977, 42140, 101883, 104573, 497, 330, 83259, 3710, 788, 330, 100039, 44729, 30858, 14345, 5212, 11817, 361, 788, 330, 109111, 16530, 59879, 108792, 20221, 99327, 497, 330, 83259, 3710, 788, 330, 115083, 14345, 5212, 11817, 361, 788, 330, 101241, 99821, 12857, 74386, 3837, 101451, 99807, 45629, 26288, 102844, 3837, 35946, 104624, 100012, 105572, 99464, 99164, 99172, 497, 330, 83259, 3710, 788, 330, 100039, 44729, 30858, 14345, 5212, 11817, 361, 788, 330, 99494, 34187, 497, 330, 83259, 3710, 788, 330, 100039, 44729, 30858, 9207, 13989, 151645]\n",
            "labels:\n",
            "{\"summary\": \"- 夏子明要求黎明给他做饭，并以吃了两份饭为胜利理由，要求黎明认输。引发了关于打打杀杀生活选择的对话。气氛变得紧张，夏子明提出了直接将七窍玲珑心掏出来的方法，引发了黎明的警告和反驳。\", \"conversations\": [{\"dialogue\": \"可是我吃的是你做的饭，我并不亏，因为要是我去做饭的话，不但会浪费灵力，做出来的有可能还没这么美味\", \"said_by\": \"夏子明\"}, {\"dialogue\": \"这并不是美味，你身后的人做出来的饭菜那才是真正的美味，有兴趣一起尝尝吗\", \"said_by\": \"黎明\"}, {\"dialogue\": \"其实，想要得到五彩石的方法还有一个，那便是直接将七窍玲珑心掏出来，装在我的身体里便是\", \"said_by\": \"夏子明\"}, {\"dialogue\": \"你敢\", \"said_by\": \"黎明\"}, {\"dialogue\": \"我有何不敢\", \"said_by\": \"夏子明\"}, {\"dialogue\": \"确实，你确实没什么不敢的，你就当我随口说说罢了，不必较真\", \"said_by\": \"黎明\"}, {\"dialogue\": \"若是你还有什么话没说完的话，我也可以边挖边听的。只是我要提醒你一点，你说得越快，我挖得越快，他就会少一些痛苦。若你说得慢了，那我便会放下速度，他也会多一些痛苦\", \"said_by\": \"夏子明\"}, {\"dialogue\": \"你怎么不按套路出牌\", \"said_by\": \"黎明\"}, {\"dialogue\": \"墨守成规，乃兵家大忌，我这也是为了自身的安全着想\", \"said_by\": \"夏子明\"}, {\"dialogue\": \"怎么了\", \"said_by\": \"夏子明\"}]}<|im_end|>\n",
            "[INFO|training_args.py:1828] 2024-01-25 01:52:41,236 >> PyTorch: setting up devices\n",
            "[INFO|trainer.py:571] 2024-01-25 01:52:42,631 >> Using auto half precision backend\n",
            "[INFO|trainer.py:1721] 2024-01-25 01:52:42,948 >> ***** Running training *****\n",
            "[INFO|trainer.py:1722] 2024-01-25 01:52:42,948 >>   Num examples = 72,927\n",
            "[INFO|trainer.py:1723] 2024-01-25 01:52:42,948 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1724] 2024-01-25 01:52:42,948 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:1727] 2024-01-25 01:52:42,948 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "[INFO|trainer.py:1728] 2024-01-25 01:52:42,948 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1729] 2024-01-25 01:52:42,948 >>   Total optimization steps = 54,696\n",
            "[INFO|trainer.py:1730] 2024-01-25 01:52:42,949 >>   Number of trainable parameters = 3,145,728\n",
            "  0% 0/54696 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "  0% 29/54696 [01:18<41:47:43,  2.75s/it]"
          ]
        }
      ],
      "source": [
        "! CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path  Qwen/Qwen-1_8B-Chat\\\n",
        "    --do_train True\\\n",
        "    --dataset haruhi_dialogue_extract \\\n",
        "    --template qwen \\\n",
        "    --finetuning_type lora \\\n",
        "    --lora_target c_attn \\\n",
        "    --lora_rank 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --output_dir qwen_1_8-finetuned \\\n",
        "    --overwrite_output_dir \\\n",
        "    --overwrite_cache \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --logging_steps 100 \\\n",
        "    --save_steps 1000 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --plot_loss \\\n",
        "    --fp16 True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9OaXCFQLze_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnCRoYDlBf1oKogyMcqT4d",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}