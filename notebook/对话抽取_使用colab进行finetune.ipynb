{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO99y2K83QiUFZWitH7ODhz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/Zero-Haruhi/blob/main/notebook/%E5%AF%B9%E8%AF%9D%E6%8A%BD%E5%8F%96_%E4%BD%BF%E7%94%A8colab%E8%BF%9B%E8%A1%8Cfinetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade huggingface_hub\n",
        "# !huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zElyM_0tGpZH",
        "outputId": "accd57fa-0c9e-4490-dd19-33c49e261bcf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.11.17)\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: read).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjFuGKxUFvbY",
        "outputId": "6a7f31e5-9972-4b60-e295-5e3cec59f7d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 6750, done.\u001b[K\n",
            "remote: Counting objects: 100% (769/769), done.\u001b[K\n",
            "remote: Compressing objects: 100% (316/316), done.\u001b[K\n",
            "remote: Total 6750 (delta 506), reused 661 (delta 451), pack-reused 5981\u001b[K\n",
            "Receiving objects: 100% (6750/6750), 204.48 MiB | 20.83 MiB/s, done.\n",
            "Resolving deltas: 100% (4857/4857), done.\n",
            "Updating files: 100% (140/140), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd LLaMA-Factory\n",
        "!pip install -q -r requirements.txt\n",
        "!pip install -q einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAcYKc9qGwcY",
        "outputId": "6782f84c-bfef-4a4c-dd78-73df2880ffb1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.7/79.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/dataset"
      ],
      "metadata": {
        "id": "9OSdjnmbHCSz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/LLaMA-Factory/dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIhsKveEIT7C",
        "outputId": "d8a401ce-d594-4ce9-a380-2fd18c511569"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/content/LLaMA-Factory/dataset': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFANKTp0JWSZ",
        "outputId": "ac06bc64-d529-4c0c-d23a-050eb4443cc2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/CardBuild/HaruhiZero/*dialogue*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfrKNsHhJbKC",
        "outputId": "197be821-47db-4b0b-e2b0-79126b9a0a08"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CardBuild/HaruhiZero/chinese_dialogue_clean.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/dialogue_extract.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/dialogue_negative.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/english_dialogue_clean.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/english_dialogue_extract.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "请为我实现一段python代码，把input_path下的几个file_names对应的jsonl文件(utf-8, ensure_ascii=False) 进行读取\n",
        "\n",
        "然后以json格式写入target_file"
      ],
      "metadata": {
        "id": "xDYOUoHDK4x9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = \"/content/drive/MyDrive/CardBuild/HaruhiZero/\"\n",
        "file_names = [\"chinese_dialogue_clean.jsonl\",\"dialogue_negative.jsonl\",\"english_dialogue_clean.jsonl\"]\n",
        "# file_names = [\"chinese_dialogue_clean.jsonl\"]\n",
        "target_path = \"/content/LLaMA-Factory/data/\"\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "target_file = \"/content/LLaMA-Factory/data/dialogue_merged.json\"\n",
        "\n",
        "datas = []\n",
        "for file_name in file_names:\n",
        "    with open(os.path.join(input_path, file_name), 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data = json.loads(line)\n",
        "            # delete the key except system and conversations\n",
        "            if \"source\" in data:\n",
        "                del data[\"source\"]\n",
        "            datas.append(data)\n",
        "\n",
        "with open(target_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "target_file_jsonl = \"/content/LLaMA-Factory/data/dialogue_merged.jsonl\"\n",
        "\n",
        "\n",
        "with open(target_file_jsonl, 'w', encoding='utf-8') as f:\n",
        "    for data in datas:\n",
        "        f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n"
      ],
      "metadata": {
        "id": "zYZqDgizJdyy"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data in datas:\n",
        "    if \"system\" not in data:\n",
        "        print(\"system not in data\")\n",
        "    elif \"conversations\" not in data:\n",
        "        print(\"conversations not in data\")\n",
        "    elif len(data[\"conversations\"]) != 2:\n",
        "        print(\"conversations is empty\")\n",
        "    else:\n",
        "        text = data[\"conversations\"][0]\n",
        "        continue\n",
        "    break"
      ],
      "metadata": {
        "id": "zVFtJx-aMk9C"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "请为我实现一段python代码，对于/content/LLaMA-Factory/data/dataset_info.json 备份到dataset_info_back.json"
      ],
      "metadata": {
        "id": "ZJnK1Za2J-0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "original_file = '/content/LLaMA-Factory/data/dataset_info.json'\n",
        "backup_file = '/content/LLaMA-Factory/data/dataset_info_back.json'\n",
        "\n",
        "# Make a copy of the original file\n",
        "shutil.copy(original_file, backup_file)\n",
        "\n",
        "# Verify copy succeeded\n",
        "if os.path.exists(backup_file):\n",
        "    print(f'Backup of {original_file} to {backup_file} succeeded!')\n",
        "else:\n",
        "    print(f'Backup of {original_file} to {backup_file} failed!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exqKxSYMLKMf",
        "outputId": "04679ff5-0e13-4af7-afc6-699d13eb8ab0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backup of /content/LLaMA-Factory/data/dataset_info.json to /content/LLaMA-Factory/data/dataset_info_back.json succeeded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "为我实现一段python代码，复制/content/LLaMA-Factory/data/dataset_info.json中的信息\n",
        "\n",
        "写入到 /content/LLaMA-Factory/data/dataset_info.json\n",
        "\n",
        "并且在末尾添加\n",
        "\n",
        "\n",
        "```json\n",
        "\"haruhi_dialogue_extract\": {\n",
        "  \"file_name\": \"dialogue_merged.json\",\n",
        "  \"columns\": {\n",
        "    \"messages\": \"conversations\",\n",
        "    \"system\": \"system\",\n",
        "    \"tools\": \"tools\"\n",
        "  },\n",
        "  \"tags\": {\n",
        "    \"role_tag\": \"from\",\n",
        "    \"content_tag\": \"value\"\n",
        "  }\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "48s-ojJQLOsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "original_file = '/content/LLaMA-Factory/data/dataset_info.json'\n",
        "new_file = '/content/LLaMA-Factory/data/dataset_info.json'\n",
        "\n",
        "with open(original_file, 'r') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "data['haruhi_dialogue_extract'] = {\n",
        "  \"file_name\": \"/content/LLaMA-Factory/data/dialogue_merged.jsonl\",\n",
        "  \"formatting\":\"sharegpt\",\n",
        "  \"columns\": {\n",
        "    \"messages\": \"conversations\",\n",
        "    \"system\": \"system\",\n",
        "  },\n",
        "  \"tags\": {\n",
        "    \"role_tag\": \"from\",\n",
        "    \"content_tag\": \"value\",\n",
        "    \"user_tag\": \"human\",\n",
        "    \"assistant_tag\":\"gpt\",\n",
        "  }\n",
        "}\n",
        "\n",
        "with open(new_file, 'w') as f:\n",
        "  json.dump(data, f, indent=2)\n",
        "\n",
        "print('Data copied and new info appended to', new_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ik4Bvq1LNpE",
        "outputId": "221c1019-0a28-4f8e-ab9d-7464bd4d507c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data copied and new info appended to /content/LLaMA-Factory/data/dataset_info.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tiktoken transformers_stream_generator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaruMxYdL6QU",
        "outputId": "74872790-5baf-4c81-c2af-5bd6c39d1b7c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers_stream_generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "nqZswBFMN6-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path  Qwen/Qwen-1_8B-Chat\\\n",
        "    --do_train True\\\n",
        "    --dataset haruhi_dialogue_extract \\\n",
        "    --template qwen \\\n",
        "    --finetuning_type lora \\\n",
        "    --lora_target c_attn \\\n",
        "    --lora_rank 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --output_dir qwen_1_8-finetuned \\\n",
        "    --overwrite_cache \\\n",
        "    --per_device_train_batch_size 8 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --logging_steps 100 \\\n",
        "    --save_steps 1000 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --plot_loss \\\n",
        "    --fp16 True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhZPqKBcGHau",
        "outputId": "9b691f6e-c1cc-4afd-aa64-698c695b1fb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-25 01:43:49.540722: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-25 01:43:49.540776: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-25 01:43:49.542653: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-25 01:43:50.884157: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "01/25/2024 01:43:53 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
            "[INFO|training_args.py:1828] 2024-01-25 01:43:53,121 >> PyTorch: setting up devices\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1741: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
            "  warnings.warn(\n",
            "01/25/2024 01:43:53 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1\n",
            "  distributed training: True, compute dtype: torch.float16\n",
            "01/25/2024 01:43:53 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0002,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=qwen_1_8-finetuned/runs/Jan25_01-43-53_29f45e3f8461,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=100,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=cosine,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=qwen_1_8-finetuned,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=qwen_1_8-finetuned,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=1000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-01-25 01:43:53,451 >> loading file qwen.tiktoken from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/qwen.tiktoken\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-01-25 01:43:53,451 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-01-25 01:43:53,451 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-01-25 01:43:53,451 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-01-25 01:43:53,451 >> loading file tokenizer.json from cache at None\n",
            "[INFO|configuration_utils.py:729] 2024-01-25 01:43:54,254 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/config.json\n",
            "[INFO|configuration_utils.py:729] 2024-01-25 01:43:54,463 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/config.json\n",
            "[INFO|configuration_utils.py:792] 2024-01-25 01:43:54,464 >> Model config QWenConfig {\n",
            "  \"_name_or_path\": \"Qwen/Qwen-1_8B-Chat\",\n",
            "  \"architectures\": [\n",
            "    \"QWenLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_dropout_prob\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"Qwen/Qwen-1_8B-Chat--configuration_qwen.QWenConfig\",\n",
            "    \"AutoModelForCausalLM\": \"Qwen/Qwen-1_8B-Chat--modeling_qwen.QWenLMHeadModel\"\n",
            "  },\n",
            "  \"bf16\": false,\n",
            "  \"emb_dropout_prob\": 0.0,\n",
            "  \"fp16\": false,\n",
            "  \"fp32\": false,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"kv_channels\": 128,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"qwen\",\n",
            "  \"no_bias\": true,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"onnx_safe\": null,\n",
            "  \"rotary_emb_base\": 10000,\n",
            "  \"rotary_pct\": 1.0,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"seq_length\": 8192,\n",
            "  \"softmax_in_fp32\": false,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"QWenTokenizer\",\n",
            "  \"transformers_version\": \"4.37.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_cache_kernel\": false,\n",
            "  \"use_cache_quantization\": false,\n",
            "  \"use_dynamic_ntk\": true,\n",
            "  \"use_flash_attn\": \"auto\",\n",
            "  \"use_logn_attn\": true,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3478] 2024-01-25 01:43:54,596 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1428] 2024-01-25 01:43:54,598 >> Instantiating QWenLMHeadModel model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:826] 2024-01-25 01:43:54,598 >> Generate config GenerationConfig {}\n",
            "\n",
            "Try importing flash-attention for faster inference...\n",
            "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
            "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
            "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
            "Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.01s/it]\n",
            "[INFO|modeling_utils.py:4352] 2024-01-25 01:43:58,993 >> All model checkpoint weights were used when initializing QWenLMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:4360] 2024-01-25 01:43:58,993 >> All the weights of QWenLMHeadModel were initialized from the model checkpoint at Qwen/Qwen-1_8B-Chat.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use QWenLMHeadModel for predictions without further training.\n",
            "[INFO|configuration_utils.py:781] 2024-01-25 01:43:59,111 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/generation_config.json\n",
            "[INFO|configuration_utils.py:826] 2024-01-25 01:43:59,112 >> Generate config GenerationConfig {\n",
            "  \"chat_format\": \"chatml\",\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"max_new_tokens\": 512,\n",
            "  \"max_window_size\": 6144,\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"top_k\": 0,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "[WARNING|modeling_utils.py:2134] 2024-01-25 01:43:59,113 >> You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
            "01/25/2024 01:43:59 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.\n",
            "01/25/2024 01:43:59 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
            "01/25/2024 01:43:59 - INFO - llmtuner.model.loader - trainable params: 3145728 || all params: 1839974400 || trainable%: 0.1710\n",
            "01/25/2024 01:43:59 - INFO - llmtuner.data.template - Add eos token: <|endoftext|>\n",
            "01/25/2024 01:43:59 - INFO - llmtuner.data.template - Add pad token: <|endoftext|>\n",
            "01/25/2024 01:43:59 - INFO - llmtuner.data.template - Replace eos token: <|im_end|>\n",
            "01/25/2024 01:43:59 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.\n",
            "Using custom data configuration default-7b563057c8b8ec89\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/root/.cache/huggingface/datasets/json/default-7b563057c8b8ec89/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-7b563057c8b8ec89/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "Generating train split\n",
            "Generating train split: 72927 examples [00:03, 21536.36 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-7b563057c8b8ec89/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "Converting format of dataset:   0% 0/72927 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7b563057c8b8ec89/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-eebdf306e7652ea7.arrow\n",
            "Converting format of dataset: 100% 72927/72927 [00:03<00:00, 20367.47 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/72927 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7b563057c8b8ec89/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43b5017f793f2e0d.arrow\n",
            "Running tokenizer on dataset:  19% 14000/72927 [00:33<02:20, 420.46 examples/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R9OaXCFQLze_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}