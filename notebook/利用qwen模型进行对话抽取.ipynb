{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM3kkwNDN9mIQHGLs/3RK/y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/Zero-Haruhi/blob/main/notebook/%E5%88%A9%E7%94%A8qwen%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E5%AF%B9%E8%AF%9D%E6%8A%BD%E5%8F%96.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [x] 找一本小说\n",
        "- [x] 切章节，600个token一组形成list of dict，标记章节头\n",
        "- [ ] 初始化2个或者4个qwen实例\n",
        "- [ ] 用async函数进行批量调用\n",
        "- [ ] 算一下peritem的时间"
      ],
      "metadata": {
        "id": "Wmg7NNToDp2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "先从drive上找到小说"
      ],
      "metadata": {
        "id": "wi4BvkROE8nN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrF34yqnDNlz",
        "outputId": "5fd7c490-1736-47e5-8c2e-d8cf5317a080"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/Wuxia/input/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8otj8xVYF-wB",
        "outputId": "9d814d13-2aea-48f1-f9c0-343fe2765b89"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1819部精校小说.zip  classic_50.zip  parts_1.zip  parts_3.zip  parts_5.zip  parts_7.zip\tparts_9.zip\n",
            "all_direct\t    parts_0.zip     parts_2.zip  parts_4.zip  parts_6.zip  parts_8.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "part_id = 11"
      ],
      "metadata": {
        "id": "3zsv18auGQ8C"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "这个zip在\n",
        "\n",
        "https://drive.google.com/file/d/101g9BfaKLNv10P1C1wuePTdbR6rwKRpb/view?usp=sharing"
      ],
      "metadata": {
        "id": "Tr0scWHsU5xX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# part_id = 10\n",
        "\n",
        "# input_file = f\"/content/drive/MyDrive/Wuxia/input/parts_\" + str(part_id) + \".zip\"\n",
        "\n",
        "input_file = \"/content/drive/MyDrive/Wuxia/input/classic_50.zip\"\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "output_dir = \"/content/input\"\n",
        "\n",
        "# 解压input_file到output_dir\n",
        "with zipfile.ZipFile(input_file,\"r\") as zip_ref:\n",
        "    zip_ref.extractall(output_dir)"
      ],
      "metadata": {
        "id": "aq2ftmCvE7Jn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import chardet\n",
        "\n",
        "output_folder = \"/content/input/output2\"\n",
        "# output_folder = f\"/content/output/parts_{part_id}\"\n",
        "\n",
        "\n",
        "file_names = []\n",
        "\n",
        "# 遍历所有txt文件\n",
        "for root, dirs, files in os.walk(output_folder):\n",
        "    for file in files:\n",
        "        if file.endswith(\".txt\"):\n",
        "            file_path = os.path.join(root, file)\n",
        "\n",
        "            with open(file_path, 'rb') as f:\n",
        "                content = f.read(1000)\n",
        "            result = chardet.detect(content)\n",
        "\n",
        "            if result['confidence'] < 0.9:\n",
        "                print(f\"Warning: {file_path} encoding confidence {result['confidence']:.0%} lower than 90%\")\n",
        "                continue\n",
        "\n",
        "            file_names.append({\n",
        "                \"file_name\": file_path,\n",
        "                \"encoding\": result['encoding'],\n",
        "                \"confidence\": result['confidence']\n",
        "            })"
      ],
      "metadata": {
        "id": "4gnjErqIFz7H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file_name in file_names[:5]:\n",
        "    print(file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot9yta93Ged1",
        "outputId": "6f5ef973-b3e0-4cc8-e535-6ee1a779d6c5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'file_name': '/content/input/output2/《红楼遗梦》（校对全本）作者：冬雪晚晴.txt', 'encoding': 'GB2312', 'confidence': 0.99}\n",
            "{'file_name': '/content/input/output2/《秦时明月》（精校1-8部）作者：温世仁.txt', 'encoding': 'GB2312', 'confidence': 0.99}\n",
            "{'file_name': '/content/input/output2/《人间正道是沧桑》（校对全本）作者：江奇涛.txt', 'encoding': 'UTF-16', 'confidence': 1.0}\n",
            "{'file_name': '/content/input/output2/《明朝那些事儿》（精校全本）作者：当年明月.txt', 'encoding': 'GB2312', 'confidence': 0.99}\n",
            "{'file_name': '/content/input/output2/《冰与火之歌05魔龙的狂舞》（精校全本）作者：乔治·R·R·马丁.txt', 'encoding': 'GB2312', 'confidence': 0.99}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def get_file_name(fname):\n",
        "    last_split = fname.split('/')[-1]\n",
        "\n",
        "    pattern = r'《(.+)》'\n",
        "    m = re.match(pattern, last_split)\n",
        "    if m:\n",
        "        # print(\"match\")\n",
        "        return m.group(1) + '.txt'\n",
        "    else:\n",
        "        return last_split\n",
        "\n",
        "fname = \"/content/output/output2/《凡人修仙传》(精校全本)作者:忘语.txt\"\n",
        "\n",
        "print(get_file_name(fname)[:-4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJi40TCcHweD",
        "outputId": "6a934ad7-0e9e-4d08-9b23-7831ec9f3000"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "凡人修仙传\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_chapater(line):\n",
        "    line = line.strip()\n",
        "    line = line.strip('-=*')\n",
        "    if len(line) > 100:\n",
        "        return -1\n",
        "    if len(line) == 0:\n",
        "        return -1\n",
        "    is_short = len(line) < 10\n",
        "\n",
        "    head_5_char = line[:min(5,len(line))]\n",
        "\n",
        "    small_line = line.lower()\n",
        "    line = line[:20]\n",
        "    has_di = line.find('第', 0 ) >= 0\n",
        "    di_in_head = head_5_char.find('第', 0) >= 0\n",
        "\n",
        "    has_zhang = False\n",
        "    zhang_in_head = False\n",
        "\n",
        "    zhang_word = ['章', '集', '卷','回','期','节']\n",
        "\n",
        "\n",
        "\n",
        "    for w in zhang_word:\n",
        "        if head_5_char.find(w, 0) >= 0:\n",
        "            zhang_in_head = True\n",
        "            break\n",
        "\n",
        "    for w in zhang_word:\n",
        "        if line.find(w, 0) >= 0:\n",
        "            has_zhang = True\n",
        "            break\n",
        "\n",
        "    has_digital = False\n",
        "    digital_word = ['0','1','2','3','4','5','6','7','8','9',\\\n",
        "                    '一','二','三','四','五','六','七','八','九']\n",
        "\n",
        "    only_digital = False\n",
        "    digital_count = 0\n",
        "    for ch in line:\n",
        "        if ch in digital_word:\n",
        "            digital_count += 1\n",
        "    if digital_count == len(line):\n",
        "        only_digital = True\n",
        "\n",
        "    digital_in_head = False\n",
        "\n",
        "    for w in digital_word:\n",
        "        if head_5_char.find(w, 0) >= 0:\n",
        "            digital_in_head = True\n",
        "            break\n",
        "    for w in digital_word:\n",
        "        if line.find(w, 0) >= 0:\n",
        "            has_digital = True\n",
        "            break\n",
        "    if has_di and has_zhang and has_digital and (di_in_head or digital_in_head or zhang_in_head):\n",
        "        return 3\n",
        "    if di_in_head and digital_in_head and is_short:\n",
        "        return 2\n",
        "    if zhang_in_head and digital_in_head and is_short:\n",
        "        return 2\n",
        "    # if only_digital and is_short:\n",
        "    #     return 2\n",
        "    return -1\n"
      ],
      "metadata": {
        "id": "0n7-2eLmIElG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q transformers accelerate tiktoken einops scipy transformers_stream_generator==0.0.4 peft deepspeed\n",
        "!pip install -q auto-gptq optimum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSL6w67YJxrQ",
        "outputId": "0b24ea40-4213-4899-affe-9cdcd49c844f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.2/183.2 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"silk-road/Haruhi-Dialogue-Speaker-Extract-csv\", trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"silk-road/Haruhi-Dialogue-Speaker-Extract-csv\", device_map=\"auto\", trust_remote_code=True)"
      ],
      "metadata": {
        "id": "uDLuw9nXJqCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokenizer.tokenize(\"我是一只猫\")))"
      ],
      "metadata": {
        "id": "0VGVVr_NKFyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  定义divide函数，用来切分超长文本\n",
        "def divide_str(s, sep=['\\n', '.', '。']):\n",
        "    mid_len = len(s) // 2  # 中心点位置\n",
        "    best_sep_pos = len(s) + 1  # 最接近中心点的分隔符位置\n",
        "    best_sep = None  # 最接近中心点的分隔符\n",
        "    for curr_sep in sep:\n",
        "        sep_pos = s.rfind(curr_sep, mid_len // 2, len(s)-mid_len // 2)  # 从中心点往左找分隔符\n",
        "        if sep_pos > 0 and abs(sep_pos - mid_len) < abs(best_sep_pos -\n",
        "                                                        mid_len):\n",
        "            best_sep_pos = sep_pos\n",
        "            best_sep = curr_sep\n",
        "    if not best_sep:  # 没有找到分隔符\n",
        "        return s, ''\n",
        "    return s[:best_sep_pos + 1], s[best_sep_pos + 1:]\n",
        "\n",
        "\n",
        "def strong_divide(s):\n",
        "    left, right = divide_str(s)\n",
        "\n",
        "    if right != '':\n",
        "        return left, right\n",
        "\n",
        "    whole_sep = ['\\n', '.', '，', '、', ';', ',', '；',\\\n",
        "                 '：', '！', '？', '(', ')', '”', '“', \\\n",
        "                 '’', '‘', '[', ']', '{', '}', '<', '>', \\\n",
        "                 '/', '''\\''', '|', '-', '=', '+', '*', '%', \\\n",
        "               '$', '''#''', '@', '&', '^', '_', '`', '~',\\\n",
        "                 '·', '…']\n",
        "    left, right = divide_str(s, sep=whole_sep)\n",
        "\n",
        "    if right != '':\n",
        "        return left, right\n",
        "\n",
        "    mid_len = len(s) // 2\n",
        "    return s[:mid_len], s[mid_len:]"
      ],
      "metadata": {
        "id": "gqhTO9b1Kt_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "save_folder = \"/content/output_in_chunk\"\n",
        "\n",
        "TOKEN_PER_TRUNK = 600\n",
        "\n",
        "if not os.path.exists(save_folder):\n",
        "    os.makedirs(save_folder)\n",
        "\n",
        "import codecs\n",
        "\n",
        "from tqdm import tqdm\n",
        "# for file_name_data in tqdm(file_names):\n",
        "for file_name_data in file_names:\n",
        "    file_name = file_name_data['file_name']\n",
        "    if file_name.find(\"《斗罗大陆》\",0)<0:\n",
        "        # 先找一本进行调试\n",
        "        continue\n",
        "\n",
        "    book_name = get_file_name(file_name)\n",
        "    if len(book_name) > 4:\n",
        "        book_name = book_name[:-4]\n",
        "    fencoding = file_name_data['encoding']\n",
        "    f = codecs.open(file_name, 'r', encoding=fencoding, errors = 'ignore')\n",
        "    raw_text = f.read()\n",
        "    lines = raw_text.splitlines()\n",
        "    f.close()\n",
        "\n",
        "    head_count = 0\n",
        "\n",
        "    is_heads = [predict_chapater(line) > 0 for line in lines]\n",
        "\n",
        "    token_per_line = []\n",
        "\n",
        "    for line in tqdm(lines):\n",
        "        if line.strip() == '':\n",
        "            token_per_line.append(0)\n",
        "            continue\n",
        "        n_token = len(tokenizer.tokenize(line.strip()))\n",
        "        token_per_line.append(n_token)\n",
        "\n",
        "    # 这里还要处理 如果单个line超过TOKEN_PER_TRUNK, 需要调用strong_divide进行分裂\n",
        "    # 同时第一个line保留原来的is_head, 其他的都是false\n",
        "\n",
        "\n",
        "    for line, is_head in zip(lines, is_heads):\n",
        "        if is_head:\n",
        "            head_count += 1\n",
        "\n",
        "\n",
        "\n",
        "    print(file_name_data)\n",
        "    print(head_count)\n",
        "    break"
      ],
      "metadata": {
        "id": "9eR8huGzGggl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(token_per_line)/600)"
      ],
      "metadata": {
        "id": "JN7NcJ6HHMkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JsKOw_CyMxCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = []\n",
        "\n",
        "current_n = 0\n",
        "current_chunk = \"\"\n",
        "current_chunk_is_head = True\n",
        "last_chunk_is_head = False\n",
        "\n",
        "for line, is_head, n_token in zip(lines, is_heads, token_per_line):\n",
        "    # 如果is_head且之前不是head\n",
        "    # 如果is_head且之前的current_n > TOKEN_PER_TRUNK // 2 需要结算\n",
        "    # 如果n_token + current_n > TOKEN_PER_TRUNK 需要结算\n",
        "    count_flag = False\n",
        "\n",
        "    if is_head and not last_chunk_is_head:\n",
        "        count_flag = True\n",
        "    elif is_head and last_chunk_is_head and current_n > TOKEN_PER_TRUNK // 2:\n",
        "        count_flag = True\n",
        "    elif n_token + current_n > TOKEN_PER_TRUNK:\n",
        "        count_flag = True\n",
        "\n",
        "    if count_flag and current_chunk.strip() != \"\":\n",
        "        chunk_data = {\n",
        "            \"text\": current_chunk,\n",
        "            \"is_head\": current_chunk_is_head\n",
        "        }\n",
        "        chunks.append(chunk_data)\n",
        "        last_chunk_is_head = current_chunk_is_head\n",
        "        current_chunk_is_head = is_head\n",
        "        current_chunk = \"\"\n",
        "        current_n = 0\n",
        "\n",
        "    current_chunk += line + \"\\n\"\n",
        "    current_n += n_token\n",
        "\n",
        "if current_chunk.strip() != \"\":\n",
        "    chunk_data = {\n",
        "        \"text\": current_chunk,\n",
        "        \"is_head\": current_chunk_is_head\n",
        "    }\n",
        "    chunks.append(chunk_data)"
      ],
      "metadata": {
        "id": "MQonoAgSKP9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum([ chunk[\"is_head\"] == True for chunk in chunks ]))\n",
        "print(len(chunks))"
      ],
      "metadata": {
        "id": "tFYjUriuOSmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_prompt = \"\"\"请对以下小说文本进行总结，原文:\"\"\"\n",
        "extract_prompt = \"\"\"给定input paragraph，以及相应的summary，抽取每一句对话的内容，判断每一句话的说话人 以 | said by | 的形式输出成csv格式\"\"\"\n"
      ],
      "metadata": {
        "id": "H1GDjnDcXCsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_summary = \"\"\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "model = model.eval()\n",
        "\n",
        "save_datas = []\n",
        "\n",
        "for chunk in tqdm(chunks[:100]):\n",
        "    # print(chunk.keys())\n",
        "    text = chunk[\"text\"]\n",
        "    save_text = chunk[\"text\"]\n",
        "    if not chunk[\"is_head\"] and last_summary.strip() != \"\":\n",
        "        text = last_summary + \"\\n\" + text\n",
        "\n",
        "    current_summary, _ = model.chat(tokenizer, summary_prompt + text, history=[])\n",
        "\n",
        "    input_text = f\"paragraph:\\n{text}\\n\\nsummary:\\n{current_summary}\"\n",
        "\n",
        "    csv_response, history = model.chat(tokenizer, input_text, system = extract_prompt, history=[])\n",
        "\n",
        "    save_data = {\n",
        "        \"text\": save_text,\n",
        "        \"summary\": current_summary,\n",
        "        \"csv_response\": csv_response\n",
        "    }\n",
        "\n",
        "    last_summary = current_summary\n",
        "    save_datas.append(save_data)\n"
      ],
      "metadata": {
        "id": "yI6s7ArfN5HZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_save_name = \"/content/drive/MyDrive/CardBuild/exp0130/douluo_first_100.txt\"\n",
        "\n",
        "with open(final_save_name, \"w\", encoding=\"utf-8\") as f:\n",
        "    for data in save_datas:\n",
        "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")"
      ],
      "metadata": {
        "id": "rCxKecL7N_OJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VQEWzRdBYJpQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}