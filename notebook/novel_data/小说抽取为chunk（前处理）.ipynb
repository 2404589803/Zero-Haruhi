{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMeklZb4IVwYxyxXXenBGn1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/Zero-Haruhi/blob/main/notebook/novel_data/%E5%B0%8F%E8%AF%B4%E6%8A%BD%E5%8F%96%E4%B8%BAchunk%EF%BC%88%E5%89%8D%E5%A4%84%E7%90%86%EF%BC%89.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [ ] 解压缩一个小说part包到content\n",
        "- [ ] 把单个小说到chunks的函数封装\n",
        "- [ ] 汇总一个jsonl到/content\n",
        "- [ ] 尝试在GPU下压缩看看能有多大"
      ],
      "metadata": {
        "id": "tVVE4GSzihrG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hepbXjFTicNo",
        "outputId": "b61231f8-c011-4dea-93ea-0b9ed8c10397"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "part_id = 0\n",
        "# input_file = f\"/content/drive/MyDrive/Wuxia/input/parts_{part_id}.zip\"\n",
        "\n",
        "input_file = f\"/content/drive/MyDrive/Wuxia/input/classic_50.zip\"\n"
      ],
      "metadata": {
        "id": "x-iIkY_LjL6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "output_dir = \"/content/\"\n",
        "\n",
        "# 解压input_file到output_dir\n",
        "with zipfile.ZipFile(input_file,\"r\") as zip_ref:\n",
        "    zip_ref.extractall(output_dir)"
      ],
      "metadata": {
        "id": "X92YC_9cjWpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import chardet\n",
        "\n",
        "output_folder = \"/content/output2\"\n",
        "\n",
        "file_names = []\n",
        "\n",
        "# 遍历所有txt文件\n",
        "for root, dirs, files in os.walk(output_folder):\n",
        "    for file in files:\n",
        "        if file.endswith(\".txt\"):\n",
        "            file_path = os.path.join(root, file)\n",
        "\n",
        "            with open(file_path, 'rb') as f:\n",
        "                content = f.read(1000)\n",
        "            result = chardet.detect(content)\n",
        "            # 检测编码\n",
        "            # result = chardet.detect(open(file_path, 'rb').read())\n",
        "\n",
        "            # 判断是否GB2312或UTF-8\n",
        "            # if result['encoding'] in ['GB2312', 'UTF-8']:\n",
        "            if result['confidence'] < 0.9:\n",
        "                print(f\"Warning: {file_path} encoding confidence {result['confidence']:.0%} lower than 90%\")\n",
        "                continue\n",
        "\n",
        "            file_names.append({\n",
        "                \"file_name\": file_path,\n",
        "                \"encoding\": result['encoding'],\n",
        "                \"confidence\": result['confidence']\n",
        "            })"
      ],
      "metadata": {
        "id": "pJgsFZz3jZD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(file_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv0KdUAzjiZz",
        "outputId": "efc8ee9a-ec8c-4920-b5a0-1d6ad56d2f27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'file_name': '/content/output2/《秦时明月》（精校1-8部）作者：温世仁.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《间客》（精校全本）作者：猫腻.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《娘要嫁人》（精校全本）作者：严歌苓.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《狼图腾》（校对全本）作者：姜戎.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《红楼遗梦》（校对全本）作者：冬雪晚晴.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《明朝那些事儿》（精校全本）作者：当年明月.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《冰与火之歌03冰雨的风暴》（精校全本）作者：乔治·R·R·马丁.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《步步惊心》（精校全本）作者：桐华.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《藏地密码》（精校全本）作者：何马.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《绾青丝》（校对全本）作者：波波.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《斗罗大陆Ⅱ绝世唐门》（精校全本）作者：唐家三少.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《叫魂》（校对全本）作者：孔飞力.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《白鹿原》（校对全本）作者：陈忠实.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《覆雨翻云》（校对全本）作者：黄易.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《冰与火之歌05魔龙的狂舞》（精校全本）作者：乔治·R·R·马丁.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《斗破苍穹》（校对全本）作者：天蚕土豆.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《全职高手》（校对全本）作者：蝴蝶蓝.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《冰与火之歌02列王的纷争》（精校全本）作者：乔治·R·R·马丁.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《冰与火之歌01权力的游戏》（精校全本）作者：乔治·R·R·马丁.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《人间正道是沧桑》（校对全本）作者：江奇涛.txt', 'encoding': 'UTF-16', 'confidence': 1.0}, {'file_name': '/content/output2/《金枝玉叶(九重凤阙)》（校对版全本+番外）作者：灯火阑珊.txt', 'encoding': 'UTF-16', 'confidence': 1.0}, {'file_name': '/content/output2/《冰与火之歌04群鸦的盛宴》（精校全本）作者：乔治·R·R·马丁.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《斗罗大陆》（精校全本）作者：唐家三少.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《活着》（校对全本）作者：余华.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《大染坊》（精校全本）作者：陈杰.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《凡人修仙传》（精校全本）作者：忘语.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《上海堡垒》（校对全本）作者：江南.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《盗墓笔记》（精校全本）作者：南派三叔.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《金陵十三钗》（校对全本）作者：严歌苓.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《官路风流（侯卫东官场笔记）》（校对全本）作者：小桥老树.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《琅琊榜》（精校全本）作者：海宴.txt', 'encoding': 'UTF-16', 'confidence': 1.0}, {'file_name': '/content/output2/《金粉世家》（校对全本）作者：张恨水.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《裸婚——80后的新结婚时代》（精校全本）作者：唐欣恬.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《我是猫》（校对全本）作者：夏目漱石.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《悟空传》（精校全本）作者：今何在.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《官道之色戒》（校对全本）作者：低手寂寞.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《庆余年》（精校全本）作者：猫腻.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《超级兵王》（校对全本）作者：明朝无酒.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《鬼吹灯Ⅱ》（精校全本）作者：天下霸唱.txt', 'encoding': 'UTF-16', 'confidence': 1.0}, {'file_name': '/content/output2/《武林三绝》（校对全本）作者：梁羽生.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《古剑奇谭·琴心剑魄》（精校全本）作者：宁昼&某树.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《灿烂千阳》（校对全本）作者：胡赛尼.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《闯关东》（精校全本）作者：高满堂＆孙建业.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《大漠谣》（精校全本）作者：桐华.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《诛仙》（精校全本）作者：萧鼎.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《三体3部全》（精校全本）作者：刘慈欣.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《鬼吹灯Ⅰ》（精校全本）作者：天下霸唱.txt', 'encoding': 'UTF-16', 'confidence': 1.0}, {'file_name': '/content/output2/《北京遇上西雅图》（精校全本）作者：薛晓璐.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《将夜》（精校全本）作者：猫腻.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《死人经》（精校全本）作者：冰临神下.txt', 'encoding': 'GB2312', 'confidence': 0.99}, {'file_name': '/content/output2/《凤囚凰》（精校全本+番外）作者：天衣有风.txt', 'encoding': 'GB2312', 'confidence': 0.99}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def get_file_name(fname):\n",
        "    last_split = fname.split('/')[-1]\n",
        "\n",
        "    pattern = r'《(.+)》'\n",
        "    m = re.match(pattern, last_split)\n",
        "    if m:\n",
        "        # print(\"match\")\n",
        "        return m.group(1) + '.txt'\n",
        "    else:\n",
        "        return last_split\n",
        "\n",
        "fname = \"/content/output/output2/《凡人修仙传》(精校全本)作者:忘语.txt\"\n",
        "\n",
        "print(get_file_name(fname)[:-4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Eu1jWlZjk0m",
        "outputId": "b59a5ab7-84f5-488e-8131-bfbaf7f2cd8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "凡人修仙传\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "切分小说"
      ],
      "metadata": {
        "id": "dAhUT2wIj7X7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_chapater(line):\n",
        "    line = line.strip()\n",
        "    line = line.strip('-=*')\n",
        "    if len(line) > 100:\n",
        "        return -1\n",
        "    if len(line) == 0:\n",
        "        return -1\n",
        "    is_short = len(line) < 10\n",
        "\n",
        "    head_5_char = line[:min(5,len(line))]\n",
        "\n",
        "    small_line = line.lower()\n",
        "    line = line[:20]\n",
        "    has_di = line.find('第', 0 ) >= 0\n",
        "    di_in_head = head_5_char.find('第', 0) >= 0\n",
        "\n",
        "    has_zhang = False\n",
        "    zhang_in_head = False\n",
        "\n",
        "    zhang_word = ['章', '集', '卷','回','期','节']\n",
        "\n",
        "\n",
        "\n",
        "    for w in zhang_word:\n",
        "        if head_5_char.find(w, 0) >= 0:\n",
        "            zhang_in_head = True\n",
        "            break\n",
        "\n",
        "    for w in zhang_word:\n",
        "        if line.find(w, 0) >= 0:\n",
        "            has_zhang = True\n",
        "            break\n",
        "\n",
        "    has_digital = False\n",
        "    digital_word = ['0','1','2','3','4','5','6','7','8','9',\\\n",
        "                    '一','二','三','四','五','六','七','八','九']\n",
        "\n",
        "    only_digital = False\n",
        "    digital_count = 0\n",
        "    for ch in line:\n",
        "        if ch in digital_word:\n",
        "            digital_count += 1\n",
        "    if digital_count == len(line):\n",
        "        only_digital = True\n",
        "\n",
        "    digital_in_head = False\n",
        "\n",
        "    for w in digital_word:\n",
        "        if head_5_char.find(w, 0) >= 0:\n",
        "            digital_in_head = True\n",
        "            break\n",
        "    for w in digital_word:\n",
        "        if line.find(w, 0) >= 0:\n",
        "            has_digital = True\n",
        "            break\n",
        "    if has_di and has_zhang and has_digital and (di_in_head or digital_in_head or zhang_in_head):\n",
        "        return 3\n",
        "    if di_in_head and digital_in_head and is_short:\n",
        "        return 2\n",
        "    if zhang_in_head and digital_in_head and is_short:\n",
        "        return 2\n",
        "    # if only_digital and is_short:\n",
        "    #     return 2\n",
        "    return -1\n",
        "\n",
        "#@title  定义divide函数，用来切分超长文本\n",
        "def divide_str(s, sep=['\\n', '.', '。']):\n",
        "    mid_len = len(s) // 2  # 中心点位置\n",
        "    best_sep_pos = len(s) + 1  # 最接近中心点的分隔符位置\n",
        "    best_sep = None  # 最接近中心点的分隔符\n",
        "    for curr_sep in sep:\n",
        "        sep_pos = s.rfind(curr_sep, mid_len // 2, len(s)-mid_len // 2)  # 从中心点往左找分隔符\n",
        "        if sep_pos > 0 and abs(sep_pos - mid_len) < abs(best_sep_pos -\n",
        "                                                        mid_len):\n",
        "            best_sep_pos = sep_pos\n",
        "            best_sep = curr_sep\n",
        "    if not best_sep:  # 没有找到分隔符\n",
        "        return s, ''\n",
        "    return s[:best_sep_pos + 1], s[best_sep_pos + 1:]\n",
        "\n",
        "\n",
        "def strong_divide(s):\n",
        "    left, right = divide_str(s)\n",
        "\n",
        "    if right != '':\n",
        "        return left, right\n",
        "\n",
        "    whole_sep = ['\\n', '.', '，', '、', ';', ',', '；',\\\n",
        "                 '：', '！', '？', '(', ')', '”', '“', \\\n",
        "                 '’', '‘', '[', ']', '{', '}', '<', '>', \\\n",
        "                 '/', '''\\''', '|', '-', '=', '+', '*', '%', \\\n",
        "               '$', '''#''', '@', '&', '^', '_', '`', '~',\\\n",
        "                 '·', '…']\n",
        "    left, right = divide_str(s, sep=whole_sep)\n",
        "\n",
        "    if right != '':\n",
        "        return left, right\n",
        "\n",
        "    mid_len = len(s) // 2\n",
        "    return s[:mid_len], s[mid_len:]\n"
      ],
      "metadata": {
        "id": "JZNQVpHPjzbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "\n",
        "for file_name_data in file_names:\n",
        "    file_name = file_name_data['file_name']\n",
        "    if file_name.find(\"大染坊\",0)<0:\n",
        "        # 先找一本进行调试\n",
        "        continue\n",
        "\n",
        "    break\n"
      ],
      "metadata": {
        "id": "HK1Xgjg2kDQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7kspczHklVj",
        "outputId": "e88d9832-8757-432c-f2d1-ad2427b446df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.8 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "enc= tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "Y9kCo0Qfkulf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = \"/content/output2/《大染坊》（精校全本）作者：陈杰.txt\"\n",
        "\n",
        "book_name = get_file_name(file_name)\n",
        "if len(book_name) > 4:\n",
        "    book_name = book_name[:-4]\n",
        "\n",
        "fencoding = file_name_data['encoding']\n",
        "f = codecs.open(file_name, 'r', encoding=fencoding, errors = 'ignore')\n",
        "raw_text = f.read()\n",
        "f.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "at4sVUtjkLWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def book2chunks(raw_text, book_name):\n",
        "    lines = raw_text.splitlines()\n",
        "\n",
        "    is_heads = [predict_chapater(line) > 0 for line in lines]\n",
        "\n",
        "    # token_per_line = []\n",
        "\n",
        "    # from tqdm import tqdm\n",
        "\n",
        "    token_per_line = [len(enc.encode(line)) for line in lines]\n",
        "\n",
        "    chunks = []\n",
        "\n",
        "    current_n = 0\n",
        "    current_chunk = \"\"\n",
        "    current_chunk_is_head = True\n",
        "    last_chunk_is_head = False\n",
        "\n",
        "    TOKEN_PER_TRUNK = 1000\n",
        "\n",
        "    id_count = 0\n",
        "\n",
        "    for line, is_head, n_token in zip(lines, is_heads, token_per_line):\n",
        "        # 如果is_head且之前不是head\n",
        "        # 如果is_head且之前的current_n > TOKEN_PER_TRUNK // 2 需要结算\n",
        "        # 如果n_token + current_n > TOKEN_PER_TRUNK 需要结算\n",
        "        count_flag = False\n",
        "\n",
        "        if is_head and not last_chunk_is_head:\n",
        "            count_flag = True\n",
        "        elif is_head and last_chunk_is_head and current_n > TOKEN_PER_TRUNK // 2:\n",
        "            count_flag = True\n",
        "        elif n_token + current_n > TOKEN_PER_TRUNK:\n",
        "            count_flag = True\n",
        "\n",
        "        if count_flag and current_chunk.strip() != \"\":\n",
        "            chunk_data = {\n",
        "                \"text\": current_chunk,\n",
        "                \"is_head\": current_chunk_is_head,\n",
        "                \"id\": book_name + \"_\" + str(id_count)\n",
        "            }\n",
        "            id_count += 1\n",
        "            chunks.append(chunk_data)\n",
        "            last_chunk_is_head = current_chunk_is_head\n",
        "            current_chunk_is_head = is_head\n",
        "            current_chunk = \"\"\n",
        "            current_n = 0\n",
        "\n",
        "        current_chunk += line + \"\\n\"\n",
        "        current_n += n_token\n",
        "\n",
        "    if current_chunk.strip() != \"\":\n",
        "        chunk_data = {\n",
        "            \"text\": current_chunk,\n",
        "            \"is_head\": current_chunk_is_head,\n",
        "            \"id\": book_name + \"_\" + str(id_count)\n",
        "        }\n",
        "        chunks.append(chunk_data)\n",
        "\n",
        "    for chunk in chunks:\n",
        "        text = chunk['text']\n",
        "        text = text.replace(\"\\u3000\", \"\\n\")\n",
        "        for i in range(3):\n",
        "            text = text.replace(\"\\n\\n\", \"\\n\")\n",
        "        # text = text.replace(\"\\n\\n\", \"\\n\")\n",
        "        chunk['text'] = text.strip()\n",
        "        del chunk['is_head']\n",
        "    return chunks\n",
        "\n",
        "chunks = book2chunks(raw_text, book_name)"
      ],
      "metadata": {
        "id": "x4kasWRRkNS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_datas = []\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "for file_name_data in tqdm(file_names):\n",
        "    file_name = file_name_data['file_name']\n",
        "\n",
        "    book_name = get_file_name(file_name)\n",
        "    if len(book_name) > 4:\n",
        "        book_name = book_name[:-4]\n",
        "\n",
        "    fencoding = file_name_data['encoding']\n",
        "    f = codecs.open(file_name, 'r', encoding=fencoding, errors = 'ignore')\n",
        "    raw_text = f.read()\n",
        "    f.close()\n",
        "\n",
        "    chunks = book2chunks(raw_text, book_name)\n",
        "\n",
        "    save_datas.extend(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey3vnXG3k9jJ",
        "outputId": "2a64cb9c-3ecf-4fec-ad6c-a87031a3d9b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 51/51 [01:21<00:00,  1.59s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(save_datas))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtYTDMMYm9r2",
        "outputId": "581d93e2-9ff7-44b8-8d61-fb9e35b6de33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "118997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "save_name = \"/content/classic_50.jsonl\"\n",
        "\n",
        "with open(save_name, 'w') as f:\n",
        "    for data in save_datas:\n",
        "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")"
      ],
      "metadata": {
        "id": "dDmn34IglquW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "请为我实现一段python程序\n",
        "\n",
        "将 /content/classic_50.jsonl\n",
        "\n",
        "压缩到classic_50.zip\n",
        "\n",
        "并且print压缩后的文件的大小"
      ],
      "metadata": {
        "id": "ORUvySlInBwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 首先，将指定的 JSONL 文件压缩为 ZIP 文件\n",
        "# 然后，计算并打印压缩后的文件大小\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# 指定要压缩的文件路径和目标 ZIP 文件路径\n",
        "source_file_path = '/content/classic_50.jsonl'\n",
        "target_zip_path = '/content/classic_50.zip'\n",
        "\n",
        "# 使用 zipfile 模块创建 ZIP 文件并添加指定的文件\n",
        "with zipfile.ZipFile(target_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    zipf.write(source_file_path, os.path.basename(source_file_path))\n",
        "\n",
        "# 计算并打印压缩后的文件大小\n",
        "compressed_file_size = os.path.getsize(target_zip_path)\n",
        "print(compressed_file_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKfqlZ7jnBT8",
        "outputId": "6719969d-414e-444e-8407-66b021798949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96417330\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wzXWS4MgnYJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "为我实现一段python程序，将/content/classic_50.zip\n",
        "\n",
        "拷贝到/content/drive/MyDrive/CardBuild/exp0210/dialogue_extract_input"
      ],
      "metadata": {
        "id": "TnuubfRQnwUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# 定义源 ZIP 文件路径和目标目录路径\n",
        "source_zip_path = '/content/classic_50.zip'\n",
        "target_directory_path = '/content/drive/MyDrive/CardBuild/exp0210/dialogue_extract_input/classic_50.zip'\n",
        "\n",
        "# 尝试拷贝文件\n",
        "# 注意：这里的路径和操作基于一个假设的环境，实际上在当前环境中不可能直接访问 `/content` 目录\n",
        "# 因此，我们将模拟这个过程的逻辑，但不会实际执行\n",
        "\n",
        "# 模拟的过程\n",
        "shutil.copy(source_zip_path, target_directory_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WdKL3R1Tn_-K",
        "outputId": "6522b7c3-6584-453f-d9f2-cc80d25ad2c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/CardBuild/exp0210/dialogue_extract_input/classic_50.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# 定义源 ZIP 文件路径和目标目录路径\n",
        "source_zip_path = '/content/classic_50.jsonl'\n",
        "target_directory_path = '/content/drive/MyDrive/CardBuild/exp0210/dialogue_extract_input/classic_50.jsonl'\n",
        "\n",
        "# 尝试拷贝文件\n",
        "# 注意：这里的路径和操作基于一个假设的环境，实际上在当前环境中不可能直接访问 `/content` 目录\n",
        "# 因此，我们将模拟这个过程的逻辑，但不会实际执行\n",
        "\n",
        "# 模拟的过程\n",
        "shutil.copy(source_zip_path, target_directory_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5VrnWYXooig7",
        "outputId": "e18b59fa-7a30-4914-c0e2-0e970639219f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/CardBuild/exp0210/dialogue_extract_input/classic_50.jsonl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(save_datas))\n",
        "print(save_datas[0].keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCPyX-R2oyMk",
        "outputId": "e10f43a4-c761-4525-87ad-0021877bf8c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "118997\n",
            "dict_keys(['text', 'id'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "for _ in range(3):\n",
        "    random_id = random.randint(0, len(save_datas) - 1)\n",
        "    print(save_datas[random_id]['id'])\n",
        "    print(save_datas[random_id]['text'])\n",
        "    print('---')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK1XV8NVoEQ3",
        "outputId": "8637467f-c456-4c5b-825f-49d6ba8bec43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "超级兵王_1830\n",
            "张士天不说监狱还好，一说监狱就让郭璞心头憋火，自己被冤枉关了四年，他妈的，这笔账正好找不到人算呢，你这孙子居然敢说管小爷一辈子！\n",
            "“呵呵！这位大爷！你想关我一辈子？不过你不行！尤其是像你们这群蛆虫，渣滓！或者都是多余的王八蛋，我要是你们的爹，当时就直接把你们射在墙上了！现在回炉也晚了！不过好在还有机会，我可以给你们一个重新做人的机会！”\n",
            "张士天和文化还有洪志东加上其他的十几个二世祖同时呆了！\n",
            "静！\n",
            "周围死一般的安静，只有海风在轻轻地吹动着树梢！\n",
            "所有的人都张大着嘴巴看着眼前这个笑嘻嘻说出一大堆叫人忍不住勃然变色的话的家伙！\n",
            "那些二世祖们一时之间也没能反应过来，郭璞呵呵一笑，继续说道：“这位大爷，难道我说错了？”\n",
            "看到郭璞的又在说话，二世祖们才反应过来。纷纷嚷嚷着指着郭璞就是一阵的开骂！\n",
            "“哪里来的狗东西？居然敢这样羞辱我们？”\n",
            "“就是，把他做了，他妈的，又不是没杀过人！”\n",
            "“狗东西，居然敢羞辱张公子，真是在找死啊！”\n",
            "“上，打死这个狗东西！”\n",
            "郭璞突然哈哈大笑起来，看着被一群二世祖围在中间的张士天，眼里突然闪过一道残忍的杀意，但是没有人看到他的眼神！\n",
            "“好吧好吧！我是狗东西！不过这位张公子，你可要看好了！这五个美女，都是我的女人，你确定你还要继续惹我吗？”\n",
            "“呸！你的女人？真是大言不惭！”\n",
            "“就是，我看这五个女人就是夜总会的小姐！他妈的，估计是上海哪个夜总会的！嘿嘿，居然跑到我们的地盘上来了！兄弟们，有口福了！那个归我，就是胸最大的那个女人，哈哈，正有气质！”\n",
            "“对，那我要那个冷冰冰的，哈哈，好久没玩这么冷的女人了！看看有什么不一样饿滋味！”\n",
            "米兰气得柳眉倒竖，但是随即她就是眉眼一转，装着害怕的样子，走到郭璞的身边，颤声说道：“老板，救救我们！我怕！！”\n",
            "“老板，我也怕！”\n",
            "“老板！我怕怕！”\n",
            "---\n",
            "凡人修仙传_6972\n",
            "第一千五百三十九章 大难逃脱\n",
            "“是不是简道友弄错了，那新出现的玄天之宝并未在我们飞灵族区域。如此的话，无法用血祭之术召来此宝也是正常之事。”一名淡淡的女子声音也从祭坛上响起。\n",
            "说话之人却是一名身披白色斗篷的中年美妇，背后一对羽翅五色闪亮。刚才的言语中，明显带有一丝幸灾乐祸之意。\n",
            "“步夫人！刚才召唤法阵是我和简兄一起主持的，通过血祭之力的确都感应到了那件玄天斩灵剑的存在。但是在传回时似乎出了什么差错，竟然在途中忽然消失了。难道此剑已经通灵，可以自行斩破虚空不成？”另一名青年忽然开口了。\n",
            "此人头上一根白角闪闪发光，正是那名半年前，韩立在巨岛上见过的角蚩族独角青年。\n",
            "而一开始说话之人，自然就是那名海王族的鱼眼人了。\n",
            "只是这时的鱼眼人脸上隐带几分气急败坏之色。\n",
            "“是不是真有玄天之宝，自然只有主持法阵的二位道友知道了。我等只是奉命配合二位道友的。”一名身穿皂袍，但背生两对灰翅的老者，也淡淡的说了一句。\n",
            "他单手拄着一根龙头拐杖，毫不掩饰其话语里暗藏的讥讽之意。\n",
            "至于其余同样背生翅膀的飞灵族人，虽然没有说什么，但看向独角青年和鱼眼人的目光也均都有些不善。\n",
            "独角青年却没有再回答什么，而是和鱼眼人互望了一眼，二人嘴唇微动，却毫无声音发出，竟传音秘密交谈起来。\n",
            "看到这二人如此肆无忌惮的样子，在场的飞灵族人，面色越发的难看了。\n",
            "“既然第一次失败，我们马上再举行一次血祭。现在血气尚未散尽，应该可以办到此事的。只要那件玄天灵宝还在你们飞灵族区域内，就绝对会响应召唤的。”二名异族似乎商量完毕，独角青年忽然扭首的说道，神色有些阴沉。\n",
            "---\n",
            "间客_3114\n",
            "沉默旁观的许乐逐渐明白了那些叛军机甲的用意，脸色越来越苍白，却依然操控着桃瘴机甲，如帝王般不可一世，无法击倒的怀草诗，也明白那些叛军的用意，可他没有办法改变这种局面。\n",
            "桃瘴机甲动如闪电，趋避进退天下无双，近战长枪迅猛无俦，没有一台叛军机甲能在它面前支撑超过十秒钟。在常规战场上，这场机甲战毫无疑问将以桃瘴机甲的胜利而告终，因为他可以像一头鲨鱼般，冷漠地四处周游，残忍寻机攻击鱼群，然后再次远避，就这样简单地重复下去，叛军机甲群终将灭亡。\n",
            "然而在这片桑树海中，桃瘴机甲无法做到这一点，因为这里没有后勤基地，没有机甲修理所需要的构件材料。\n",
            "叛军的机甲群不停不歇地疯狂追击，就是要逼迫他们最敬畏的那人，永远没有休息的时间，让伤势与疲惫不停地蚕食那人的强大实力，同时更是要不停地损耗桃瘴机甲的机身，在这片原始的农业区中，机甲无法修复，那便只能被鱼群一口口咬到死亡……\n",
            "在逃亡的过程中，许乐一直认真地观看或者学习着这名帝国强者的机甲操控，哪怕明天凌晨或者说下一刻，他这只池中的无辜鱼儿便会被烧死，可他依然将很大的精力放在这个工作上，因为对方的机控水平实在是非常高妙，在他这种程度的人看来，能体会到更多的东西，甚至有那么一种凛冽强悍的美感。\n",
            "最令他感到震惊或者说惊惧的是，逃亡了十几个昼夜，身旁这名帝国军官居然一直保持着强悍的战斗力，还有他无法理解的旺盛的战斗欲望，虽然此人的脸颊显得越来越消瘦，可那双并不大的眼睛里神彩却从来没有黯淡过。\n",
            "然而就如疯狂追袭的数支机甲部队期望的那般，伤重难复，疲惫入骨的怀草诗强悍地坚持了下来，而桃瘴机甲，却终究是无法承受越来越多的零件损耗和金属疲劳，在某个安静的黄昏时刻倒了下来。\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iDA3QzVboNe-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}