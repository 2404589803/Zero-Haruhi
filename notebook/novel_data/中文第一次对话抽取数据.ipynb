{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3Jgt54kgcxCk646/NptoQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/Zero-Haruhi/blob/main/notebook/%E4%B8%AD%E6%96%87%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AF%B9%E8%AF%9D%E6%8A%BD%E5%8F%96%E6%95%B0%E6%8D%AE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqEYW8boPaXn",
        "outputId": "f73af0f6-48a6-4955-d7f0-86921c07d0b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.3/222.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import httpx\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"]  = \"sk-JEB\"\n",
        "\n",
        "import openai\n",
        "from openai import AsyncOpenAI\n"
      ],
      "metadata": {
        "id": "aMr5D92qPcdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Z_hevNCrRIJ",
        "outputId": "d5573d93-f88c-4c65-f92d-8655e5df0d2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_c1yqgApHy3",
        "outputId": "7b955784-f2ed-4b73-df65-4611e1e89e2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dialogueful_novel.jsonl  part_10.txt  part_2.txt  part_4.txt  part_6.txt  part_8.txt\n",
            "part_0.txt\t\t part_1.txt   part_3.txt  part_5.txt  part_7.txt  part_9.txt\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/Wuxia/input/all_direct"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_quotes(text):\n",
        "\n",
        "    quote_chars = '''「」\"“”'''\n",
        "    count = 0\n",
        "    for char in text:\n",
        "        if char in quote_chars:\n",
        "            count += 1\n",
        "    return count"
      ],
      "metadata": {
        "id": "BYP_NSrwKi56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "novel2count = {}\n",
        "\n",
        "source_pool = set()\n",
        "\n",
        "import json\n",
        "\n",
        "datas = []\n",
        "for part_id in tqdm(range(0, 10)):\n",
        "    fname = \"/content/drive/MyDrive/Wuxia/input/all_direct/part_\"+ str(part_id) +\".txt\"\n",
        "\n",
        "\n",
        "    with open(fname, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line.strip() == \"\":\n",
        "                continue\n",
        "            data = json.loads(line)\n",
        "            source = data[\"source\"]\n",
        "            if source in source_pool:\n",
        "                continue\n",
        "\n",
        "            if source not in novel2count:\n",
        "                novel2count[source] = 0\n",
        "\n",
        "            if novel2count[source] > 100:\n",
        "                continue\n",
        "\n",
        "            n_quote = count_quotes(data[\"text\"])\n",
        "            if n_quote < 100:\n",
        "                continue\n",
        "\n",
        "            novel2count[source] += 1\n",
        "            datas.append(data)\n",
        "\n",
        "    print(len(datas))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoNeoQPu83aN",
        "outputId": "31e9cddf-698b-4308-cb55-994d83aa80d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [00:39<05:51, 39.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [01:12<04:47, 35.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [01:48<04:11, 36.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10770\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [02:20<03:24, 34.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13941\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [02:58<02:58, 35.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [03:33<02:21, 35.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [04:06<01:44, 34.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [04:40<01:08, 34.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26417\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [05:18<00:35, 35.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [05:58<00:00, 35.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datas = datas"
      ],
      "metadata": {
        "id": "09SQR7r_kt6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(datas[100]['text'])"
      ],
      "metadata": {
        "id": "3iikVQevk2Xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "给定Paragraph，抽取其中的对话，并输出为json格式\n",
        "\n",
        "Let's think it step by step\n",
        "1. 对Paragraph进行总结，存储在summary字段\n",
        "2. 抽取每一句对话的内容 dialogue，以及对话的说话人 said by, 存储在conversations中\n",
        "\n"
      ],
      "metadata": {
        "id": "39lFGhV8NE72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task_prompt = \"\"\"给定input paragraph，抽取其中的对话，并输出为json格式\n",
        "\n",
        "Let's think it step by step\n",
        "1. summarize input paragraph into bullet format，存储在summary字段\n",
        "2. 抽取每一句对话的内容 dialogue，判断每一句话的说话人 said by, 存储在conversations中\"\"\"\n",
        "\n",
        "example_input = \"\"\"input paragraph:\n",
        "“你认为明天的天气怎么样？”，来人是一个女士。李回答说：“Alice，据天气预报，明天会下雨。”Alice叹了口气，继续说：“那我们的野餐计划可能要取消了。”\"\"\"\n",
        "\n",
        "example_output = {\n",
        "  \"summary\": \"- Alice和李在公园讨论明天的天气和他们的野餐计划。\",\n",
        "  \"conversations\": [\n",
        "    {\n",
        "      \"dialogue\": \"你认为明天的天气怎么样？\",\n",
        "      \"said_by\": \"Alice\"\n",
        "    },\n",
        "    {\n",
        "      \"dialogue\": \"Alice，据天气预报，明天会下雨。\",\n",
        "      \"said_by\": \"李\"\n",
        "    },\n",
        "    {\n",
        "      \"dialogue\": \"那我们的野餐计划可能要取消了。\",\n",
        "      \"said_by\": \"Alice\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\n",
        "long_task_prompt = \"\"\"给定input paragraph，抽取其中的对话，并输出为json格式\n",
        "\n",
        "Let's think it step by step\n",
        "1. summarize input paragraph into bullet format，存储在summary字段\n",
        "2. 抽取每一句对话的内容 dialogue，判断每一句话的说话人 said by, 存储在conversations中\n",
        "\n",
        "Example input paragraph:\n",
        "“你认为明天的天气怎么样？”，来人是一个女士。李回答说：“Alice，据天气预报，明天会下雨。”Alice叹了口气，继续说：“那我们的野餐计划可能要取消了。”\n",
        "\n",
        "Example Output:\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "example_output_str = json.dumps(example_output, ensure_ascii=False,indent = 4)\n",
        "# print(example_output_str)\n",
        "\n",
        "long_task_prompt += example_output_str\n",
        "\n",
        "long_task_prompt = \"\"\"给定input paragraph，抽取其中的对话，并输出为json格式\n",
        "\n",
        "Let's think it step by step\n",
        "1. summarize input paragraph into bullet format，存储在summary字段\n",
        "2. 抽取每一句对话的内容 dialogue，判断每一句话的说话人 said by, 存储在conversations中\n",
        "\n",
        "Example input paragraph:\n",
        "许媚娘怒瞪着杨浩，好半响，她才说道：“你连姑姑的话都敢不听？”\n",
        "杨浩汗啊，这都什么跟什么？这许媚娘也太自以为是了吧？\n",
        "“有个问题我不明白，你既是龙家的人，为什么就不姓龙呢？”杨浩丝毫没有把许媚娘的怒意放在眼里，美人火，还是别有一番味道的。\n",
        "“老爷子对我有恩，我就给他做女儿了，正好他也没有女儿。”许媚娘说道。\n",
        "“哦，那请问你贵庚？”杨浩接着问道。\n",
        "“小子，你不想混了是不是？姑姑的芳龄是你能随便问的吗？”许媚娘被气得不轻，以她的聪明，当然知道杨浩是想借故转移话题。\n",
        "杨浩相当的无语，这女人，脸皮真厚，开口闭口都是以他的姑姑自居，不知道的人还真的以为她是他的姑姑呢。\n",
        "只不过，杨浩随后又想到，好象还真的是那么回事，名义上来说，这个许媚娘真的是他姑姑。\n",
        "“我是不会认他们的，他们也没有这个资格，你回去告诉他们，我现在对自己的生活很满意，不想打破现在的生活，让他们以后不要再来烦我。”杨浩说道。\n",
        "“小宝贝，我真不知道你心里在想什么，龙家有什么不好？他们有什么不好？为什么你就不肯认回他们？”许媚娘就想明白杨浩是怎么想的。\n",
        "“等等，你刚才喊我什么？”杨浩真不敢相信，他这么一个大老爷们的，被人喊什么小宝贝？\n",
        "\n",
        "Example Output:\n",
        "{\n",
        "    \"summary\": \"- 许媚娘对杨浩的行为表现出愤怒和不满。\\n- 杨浩对许媚娘的态度感到困惑，同时试图转移话题。\\n- 许媚娘坚称自己是杨浩的姑姑，而杨浩则对此表示不满。\",\n",
        "    \"conversations\": [\n",
        "        {\n",
        "            \"dialogue\": \"你连姑姑的话都敢不听？\",\n",
        "            \"said_by\": \"许媚娘\"\n",
        "        },\n",
        "        {\n",
        "            \"dialogue\": \"有个问题我不明白，你既是龙家的人，为什么就不姓龙呢？\",\n",
        "            \"said_by\": \"杨浩\"\n",
        "        },\n",
        "        {\n",
        "            \"dialogue\": \"老爷子对我有恩，我就给他做女儿了，正好他也没有女儿。\",\n",
        "            \"said_by\": \"许媚娘\"\n",
        "        },\n",
        "        {\n",
        "            \"dialogue\": \"哦，那请问你贵庚？\",\n",
        "            \"said_by\": \"杨浩\"\n",
        "        },\n",
        "        {\n",
        "            \"dialogue\": \"小子，你不想混了是不是？姑姑的芳龄是你能随便问的吗？\",\n",
        "            \"said_by\": \"许媚娘\"\n",
        "        },\n",
        "        {\n",
        "            \"dialogue\": \"我是不会认他们的，他们也没有这个资格，你回去告诉他们，我现在对自己的生活很满意，不想打破现在的生活，让他们以后不要再来烦我。\",\n",
        "            \"said_by\": \"杨浩\"\n",
        "        },\n",
        "        {\n",
        "            \"dialogue\": \"小宝贝，我真不知道你心里在想什么，龙家有什么不好？他们有什么不好？为什么你就不肯认回他们？\",\n",
        "            \"said_by\": \"许媚娘\"\n",
        "        },\n",
        "        {\n",
        "            \"dialogue\": \"等等，你刚才喊我什么？\",\n",
        "            \"said_by\": \"杨浩\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "7l8sC21JN1FV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q aiofiles tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdXZX7KgRlj0",
        "outputId": "9161029c-3aa6-4ff8-ec77-866bbceed9e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "# import openai\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "aclient = AsyncOpenAI()\n",
        "\n",
        "import asyncio\n",
        "import aiofiles\n",
        "import tiktoken\n",
        "import hashlib\n",
        "# from connector import AsyncPGConnector\n",
        "from tqdm.asyncio import tqdm as tqdm\n",
        "\n",
        "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "en2zh_ratio = 2.3\n",
        "\n",
        "delay = 1\n",
        "concurrency_limit = 16\n",
        "\n",
        "max_file_size = 1024**3"
      ],
      "metadata": {
        "id": "z1oHyqNsELxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, data in enumerate(datas):\n",
        "    data['id'] = str(i)\n",
        "\n",
        "print(datas[0].keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV4cdQGtSWgr",
        "outputId": "91a589bd-1fe2-4597-b04c-b3ef0e9972da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['source', 'text', 'id'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def dealing_messages(messages):\n",
        "    try:\n",
        "        # request_token = sum([len(enc.encode(msg['content'])) for msg in messages])\n",
        "        # response_token = int(len(enc.encode(text)) * en2zh_ratio) + 64\n",
        "\n",
        "        model = \"gpt-3.5-turbo-1106\"\n",
        "\n",
        "        resp = await aclient.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            temperature=0,\n",
        "            response_format={ \"type\": \"json_object\" }\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            result = resp.choices[0].message.content\n",
        "            result = result.strip()\n",
        "            return result\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Invalid json: \", result)\n",
        "            return None\n",
        "        except:\n",
        "            raise Exception(f\"Invalid API response: {resp}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[Error] {e}\")\n",
        "        return None\n",
        "\n",
        "def data2messages( data ):\n",
        "    messages = [\n",
        "        {\"role\":\"system\",\"content\":task_prompt},\n",
        "        {\"role\":\"user\",\"content\":example_input},\n",
        "        {\"role\":\"assistant\",\"content\":example_output_str},\n",
        "        {\"role\":\"user\",\"content\":f\"input paragraph:\\n{data['text']}\"}\n",
        "    ]\n",
        "    return messages\n",
        "\n",
        "def data2messages_no_example( data ):\n",
        "    messages = [\n",
        "        {\"role\":\"system\",\"content\":long_task_prompt},\n",
        "        {\"role\":\"user\",\"content\":f\"input paragraph:\\n{data['text']}\"}\n",
        "    ]\n",
        "    return messages"
      ],
      "metadata": {
        "id": "wgtJmUwmQitK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# messages = data2messages_no_example(datas[1])\n",
        "# response = await dealing_messages(messages)\n",
        "# print(response)"
      ],
      "metadata": {
        "id": "MYtdj_i3bI5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def getTranslation(item):\n",
        "    if \"messages\" not in item:\n",
        "        return None\n",
        "    else:\n",
        "        for i in range(3):\n",
        "            result = await dealing_messages(item['messages'])\n",
        "            if result is not None:\n",
        "                item[\"response\"] = result\n",
        "                return item\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "    return None\n",
        "\n",
        "async def process(item, semaphore):\n",
        "    async with semaphore:\n",
        "        try:\n",
        "            output_folder = \"/content/output\"\n",
        "            if \"output_folder\" in item:\n",
        "                output_folder = item[\"output_folder\"]\n",
        "            if not os.path.exists(output_folder):\n",
        "                os.makedirs(output_folder)\n",
        "\n",
        "            file_path = os.path.join(output_folder, f\"{item['id']}.txt\")\n",
        "\n",
        "            if os.path.exists(file_path):\n",
        "                return\n",
        "\n",
        "            it = await getTranslation(item)\n",
        "            if it is None:\n",
        "                raise Exception(item['id'])\n",
        "\n",
        "            with open(file_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(it, f, ensure_ascii=False)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing entry: {e}\")"
      ],
      "metadata": {
        "id": "vHXrYc0DVgxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/output"
      ],
      "metadata": {
        "id": "czTeEW4HaWhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(datas[0]['text']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jLw0EnAoMhn",
        "outputId": "3700c908-41ce-4e9d-9ba9-5ee0ea4f854c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2898\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data2messages_no_example_with_short_input( data ):\n",
        "    text = data['text']\n",
        "    lines = text.split('\\n')\n",
        "    n_token = 0\n",
        "    paragraph = \"\"\n",
        "    for line in lines:\n",
        "        token_len = len( enc.encode(line) )\n",
        "        if token_len + n_token > 1200:\n",
        "            break\n",
        "        n_token += token_len\n",
        "        paragraph += line + '\\n'\n",
        "\n",
        "    if n_token < 600:\n",
        "        paragraph = text[:600]\n",
        "    paragraph = paragraph.strip()\n",
        "\n",
        "    messages = [\n",
        "        {\"role\":\"system\",\"content\":long_task_prompt},\n",
        "        {\"role\":\"user\",\"content\":f\"input paragraph:\\n{paragraph}\"}\n",
        "    ]\n",
        "    return messages"
      ],
      "metadata": {
        "id": "aLSQTetwoigY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def main(datas):\n",
        "\n",
        "    output_folder = \"/content/output\"\n",
        "\n",
        "    process_data = []\n",
        "\n",
        "    for data in datas:\n",
        "        id = data['id']\n",
        "        process_data.append({\n",
        "            \"id\": id,\n",
        "            \"messages\": data2messages_no_example_with_short_input(data),\n",
        "            \"output_folder\": output_folder\n",
        "        })\n",
        "\n",
        "    tasks = []\n",
        "\n",
        "    semaphore = asyncio.Semaphore(concurrency_limit)\n",
        "\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # # print(f\"Already processed {len(exist_list)} items...\")\n",
        "\n",
        "    # id = set()\n",
        "\n",
        "    for item in process_data:\n",
        "        file_path = os.path.join(output_folder, f\"{item['id']}.txt\")\n",
        "\n",
        "        if os.path.exists(file_path):\n",
        "            continue\n",
        "\n",
        "        tasks.append(asyncio.create_task(process(item, semaphore)))\n",
        "\n",
        "    async for task in tqdm(tasks, total=len(tasks), desc=\"Processing items\"):\n",
        "        await task\n",
        "        time.sleep(delay)\n",
        "\n"
      ],
      "metadata": {
        "id": "lApSic3YTC8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/output"
      ],
      "metadata": {
        "id": "jBVnBE-br_WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/CardBuild/dialogue_extract"
      ],
      "metadata": {
        "id": "7g0GkOlib_Hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_id = 10\n",
        "end_id = 20\n",
        "\n",
        "current_tasks = datas[start_id:end_id]\n",
        "\n",
        "await main(current_tasks)\n",
        "\n",
        "temp_output_folder = \"/content/output\"\n",
        "\n",
        "for id in range(start_id, end_id):\n",
        "    file_path = os.path.join(temp_output_folder, f\"{id}.txt\")\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            try:\n",
        "                data = json.load(f)\n",
        "                response = data[\"response\"]\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        if response is not None:\n",
        "            datas[id][\"response\"] = response\n",
        "            datas[id][\"input\"] = data[\"messages\"][1][\"content\"]\n",
        "    # break\n",
        "\n",
        "final_save_name = \"/content/drive/MyDrive/CardBuild/dialogue_extract/\" + str(start_id) + \"_to_\" + str(end_id) + \".txt\"\n",
        "\n",
        "with open(final_save_name, 'w', encoding='utf-8') as f:\n",
        "    for id in range(start_id, end_id):\n",
        "        json.dump(datas[id], f, ensure_ascii=False)\n",
        "        f.write('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uDvRq6TUCQP",
        "outputId": "7b52ee98-b044-445c-bc3a-d78b53ae0222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing items: 100%|██████████| 10/10 [00:29<00:00,  2.96s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwbjpArQrlGi",
        "outputId": "83580f50-d2c2-4813-8030-b324c36af2f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': '19', 'messages': [{'role': 'system', 'content': '给定input paragraph，抽取其中的对话，并输出为json格式\\n\\nLet\\'s think it step by step\\n1. summarize input paragraph into bullet format，存储在summary字段\\n2. 抽取每一句对话的内容 dialogue，判断每一句话的说话人 said by, 存储在conversations中\\n\\nExample input paragraph:\\n许媚娘怒瞪着杨浩，好半响，她才说道：“你连姑姑的话都敢不听？”\\n杨浩汗啊，这都什么跟什么？这许媚娘也太自以为是了吧？\\n“有个问题我不明白，你既是龙家的人，为什么就不姓龙呢？”杨浩丝毫没有把许媚娘的怒意放在眼里，美人火，还是别有一番味道的。\\n“老爷子对我有恩，我就给他做女儿了，正好他也没有女儿。”许媚娘说道。\\n“哦，那请问你贵庚？”杨浩接着问道。\\n“小子，你不想混了是不是？姑姑的芳龄是你能随便问的吗？”许媚娘被气得不轻，以她的聪明，当然知道杨浩是想借故转移话题。\\n杨浩相当的无语，这女人，脸皮真厚，开口闭口都是以他的姑姑自居，不知道的人还真的以为她是他的姑姑呢。\\n只不过，杨浩随后又想到，好象还真的是那么回事，名义上来说，这个许媚娘真的是他姑姑。\\n“我是不会认他们的，他们也没有这个资格，你回去告诉他们，我现在对自己的生活很满意，不想打破现在的生活，让他们以后不要再来烦我。”杨浩说道。\\n“小宝贝，我真不知道你心里在想什么，龙家有什么不好？他们有什么不好？为什么你就不肯认回他们？”许媚娘就想明白杨浩是怎么想的。\\n“等等，你刚才喊我什么？”杨浩真不敢相信，他这么一个大老爷们的，被人喊什么小宝贝？\\n\\nExample Output:\\n{\\n    \"summary\": \"- 许媚娘对杨浩的行为表现出愤怒和不满。\\n- 杨浩对许媚娘的态度感到困惑，同时试图转移话题。\\n- 许媚娘坚称自己是杨浩的姑姑，而杨浩则对此表示不满。\",\\n    \"conversations\": [\\n        {\\n            \"dialogue\": \"你连姑姑的话都敢不听？\",\\n            \"said_by\": \"许媚娘\"\\n        },\\n        {\\n            \"dialogue\": \"有个问题我不明白，你既是龙家的人，为什么就不姓龙呢？\",\\n            \"said_by\": \"杨浩\"\\n        },\\n        {\\n            \"dialogue\": \"老爷子对我有恩，我就给他做女儿了，正好他也没有女儿。\",\\n            \"said_by\": \"许媚娘\"\\n        },\\n        {\\n            \"dialogue\": \"哦，那请问你贵庚？\",\\n            \"said_by\": \"杨浩\"\\n        },\\n        {\\n            \"dialogue\": \"小子，你不想混了是不是？姑姑的芳龄是你能随便问的吗？\",\\n            \"said_by\": \"许媚娘\"\\n        },\\n        {\\n            \"dialogue\": \"我是不会认他们的，他们也没有这个资格，你回去告诉他们，我现在对自己的生活很满意，不想打破现在的生活，让他们以后不要再来烦我。\",\\n            \"said_by\": \"杨浩\"\\n        },\\n        {\\n            \"dialogue\": \"小宝贝，我真不知道你心里在想什么，龙家有什么不好？他们有什么不好？为什么你就不肯认回他们？\",\\n            \"said_by\": \"许媚娘\"\\n        },\\n        {\\n            \"dialogue\": \"等等，你刚才喊我什么？\",\\n            \"said_by\": \"杨浩\"\\n        }\\n    ]\\n}\\n'}, {'role': 'user', 'content': 'input paragraph:\\n来的不是别人，正是每次见面几乎都是会让杨辰麻烦缠身的蔡妍，蔡局长同志。\\n不过今天的蔡妍并没穿警服，上身是件白色的针织外套，宽口的圆领露出胸前一片雪白的肌肤，带几分秋冬的性感。一对饱满修长的玲珑美腿包裹在一条牛仔裤内，一如既往的干练短发，但因为穿着双淡咖啡色的羊皮高跟筒靴，整个人的气质不再是那英姿的女警，更像一名独立自主的都市白领。\\n与林若溪简单随意的居家装束一比，蔡妍像是精心打扮了一番，让她本是逊色于林若溪的外貌，变得旗鼓相当起来。\\n“蔡局长，真是……稀客，稀客。”杨辰笑得有些不自然，他担心蔡妍是不是又找自己什么麻烦，大礼拜天的，自己可不想去警局录口供。\\n蔡妍见到杨辰的防备眼神，微微脸上有几分不易察觉的失落，恬淡地笑道：“我是来跟若溪谈一下关于高国雄的事。”\\n杨辰仔细想了下才想起高国雄是谁，正是前日大半夜意图害林若溪的那新加坡富商，如今还在警局被拘留着呢。\\n“怎么了，他又想什么歪法子了？”杨辰不会放过那个想打自己女人主意的恶心男人。\\n林若溪因为早上的事，对杨辰态度稍微好了些，温和地说道：“他想让他的律师来告我，妍妍跟我谈接下去的法律程序呢。”\\n“告你！？”\\n杨辰大怒地拍了下沙发，“不让他立刻被枪毙就是便宜他！他敢告你，我就去牢里杀了他！”\\n林若溪见杨辰如此生气，心里有些甜滋滋的，但脸上还是冷淡地说：“胡说什么，杀什么人，还牢里杀人，你当是科幻电影么？”\\n蔡妍则是异样地看了杨辰一眼，她通过之前的一些事，和自己姐姐蔡凝的谈话，大概已经对杨辰有些了解，她有一种感觉――如果高国雄真的敢告林若溪，他没准真会死牢里。\\n“放心吧，高国雄想告若溪，但他的律师拒绝了，现在他想告若溪，还得另外请，但他一新加坡的商人，而且案子这么明显，大陆谁愿意帮他打官司。”蔡妍笑着说。\\n“那他新加坡的律师为什么拒绝？”杨辰有些不解。\\n蔡妍微微意外，“你不知道吗，你老婆是超级大富婆哟。”\\n“妍妍！难听死了！”林若溪俏脸泛红地扭了蔡妍胳膊一下。\\n杨辰略一思索，忍俊不禁地道：“若溪宝贝，你不会是把他的律师给收买了吧？”\\n林若溪有些不好意思，当着蔡妍的面被杨辰这么称呼，她都不敢抬头了，“律师也是要吃饭的，我出钱多，他当然听我的。”\\n“你这是在耍高国雄啊。”杨辰莞尔，毕竟林若溪完全可以找别的律师，偏偏抢走高国雄的，想来高国雄一口老血都在牢房里喷出来了。\\n“谁叫他辜负我对他的信任，亏我们玉蕾从我奶奶那时候就开始跟他合作。”林若溪愤愤地道。\\n杨辰想起一句话，这个社会，有钱才能打赢官司。\\n三人说了会儿话，王妈终于做完了午饭，将饭菜慢慢端了出来。\\n蔡妍与王妈也是熟识，很亲热地去厨房帮着将菜端出来。\\n“哎哟，蔡小姐，你就别动手了，来了就是客人，怎么好麻烦你呢。”王妈客气着说。\\n蔡妍展颜道：“让王妈一个人累着，我可没这胆子呢。”\\n“瞧你说的，我就一下人，有什么不能累的。”王妈笑吟吟地说。\\n蔡妍低声在王妈耳边说道：“王妈，那个人有没有回来过？”\\n王妈一听“那个人”，神情微微不自然，泛苦地道：“蔡小姐，有些事情过去就过去吧，就当什么也没发生，人老了，我记性也不太好了。”\\n蔡妍乖巧地点了点头，“这种事情，女人总是受委屈的。”\\n这句话，似是说王妈，又似是说自己，蔡妍心中也是迷惑着。\\n看到蔡妍与王妈亲切地说着话在厨房餐厅走动着，杨辰望向林若溪，“若溪宝贝，你看人家蔡局长多懂事，你也是女人，得去帮帮王妈。”\\n林若溪似乎没这样的习惯，听杨辰一说，有些不乐意地说：“为什么你自己不去？”\\n“因为我是男人。”杨辰说。\\n“嘁……大男子主义。”林若溪嘀咕了一声，但还是嘟着嘴，站起身，去帮王妈拿碗筷。\\n杨辰微微一愣，刚才这算是听自己的话了？\\n一餐午饭，四人坐在一起也不怎么说话，因为又都不喝酒，所以很快就吃完了。\\n下午蔡妍还要回警局加班，毕竟东兴刚被夷平，整个中海的很多地下秩序都要重新建立，政府警方与红荆会的合作还在继续，所以蔡妍也会较为忙碌。\\n出门的时候，蔡妍犹豫了下，说道：“杨辰，能出来单独说几句话吗？”\\n杨辰正准备上楼睡午觉，听到蔡妍的问话，点了点头。\\n林若溪刚跟蔡妍道别，见蔡妍要跟杨辰避开自己去谈话，眼里有些疑惑，但没多想，帮着王妈收拾餐桌，虽然动作很笨拙，却让王妈开心不已。\\n杨辰跟蔡妍走出屋外，问道：“蔡局长，不会我又惹麻烦了吧。”\\n蔡妍神色黯淡，幽幽说道：“杨辰，虽然我们以前有过很多不开心的过去，但我可以向你诚挚地道歉，我知道是我错怪了你。所以，看在我们一起经历过生死，我又是若溪的好姐妹，你能不能接受我。”\\n“接受你？”杨辰干巴巴地笑了下，“蔡局长……你这话什么意思。”\\n蔡妍仿佛也感觉到自己的话有歧义，面色微红，“我……我的意思是，你能不能叫我的名字，不要总是蔡局长蔡局长……好像很生分。”\\n“这样……”杨辰体会到蔡妍倒是真心实意，再保持距离地对话，有些残忍了，“好吧，那我就叫你蔡妍吧。”\\n“谢谢”，蔡妍眼里焕发了几分神采，“那我们是朋友了，对吗？”\\n“嗯，朋友。”杨辰突然觉得这女人也挺可爱的，若不是家庭背景，她估计还真当不上局长，毕竟单纯了些。\\n蔡妍开心地笑了起来，“我还要谢谢你，谢谢你上次不顾生命地救了我，如果不是你，我可能已经被乱枪打死了。”\\n“蔡局……哦不，蔡妍，你这话不能乱说，我可没救过你，什么乱枪打死之类的，不关我的事……”杨辰冲她眨眨眼。\\n蔡妍会意，掩嘴轻笑，“知道了，我会当什么都没发生过的。”\\n似乎解开了一个心结，蔡妍开着那辆橘黄色奥迪离开的时候，感觉气色都好了许多。\\n杨辰刚要走回屋里，却见林若溪拿着一个黑色皮质的小包包走了出来，张望了下，问道：“妍妍走了？”\\n“是啊，怎么了？”\\n“她的包包落在沙发边上，她忘记拿走了。”林若溪皱眉说。\\n杨辰有些疑惑，难道那女人一高兴，连随身的包包都忘记拿走？\\n“你看看她包里有没有重要的东西。”\\n“这样……不好吧，怎么可以翻她的包……”林若溪说。\\n杨辰苦笑，这妞在这种时候倒有股子憨厚样儿，“你不翻我翻。”\\n“不行！”林若溪把包收紧了，谨慎地说：“那还是我来吧……如果有重要的东西，得立刻通知她回来拿。”\\n等林若溪简单一翻看，稍稍松了口气，“没什么重要的证件，手机之类的，下次抽空还给她，或者等她来拿吧。”\\n两人回屋后，杨辰看了会儿电视，便准备上楼午睡，却见林若溪又换了身明快的白色秋装套裙，挎着一只淡褐色的小包，打算出门。\\n看到林若溪这身装束，杨辰可不认为她是去公司，但想到高国雄那事，还是忍不住问道：“去哪？”\\n林若溪一边穿着平底鞋，一边淡淡说：“跟一个人约好去一个地方，晚饭也不回来了，你跟王妈吃吧。”\\n约会！？\\n杨辰有些不爽，大礼拜天的，怎么不跟自己约？\\n林若溪快出门的时候，突然脚步顿了下，回头冲杨辰说道：“是个女孩子。”说完，跟只小兔子一样仓皇地跑掉了。'}], 'output_folder': '/content/output', 'response': '{\\n    \"summary\": \"- 蔡妍以一身打扮展现出都市白领的气质，与林若溪形成鲜明对比，引起杨辰的注意和不安。蔡妍前来谈论关于高国雄的事情，引发了杨辰的愤怒和担忧。林若溪通过收买高国雄的律师，成功化解了高国雄的告状。蔡妍与王妈的交谈暗示着过去的不愉快经历。蔡妍向杨辰道歉并表达希望成为朋友的愿望。林若溪与一个女孩子约会，让杨辰感到不爽。\",\\n    \"conversations\": [\\n        {\\n            \"dialogue\": \"蔡局长，真是……稀客，稀客。\",\\n            \"said_by\": \"杨辰\"\\n        },\\n        {\\n            \"dialogue\": \"我是来跟若溪谈一下关于高国雄的事。\",\\n            \"said_by\": \"蔡妍\"\\n        },\\n        {\\n            \"dialogue\": \"怎么了，他又想什么歪法子了？\",\\n            \"said_by\": \"杨辰\"\\n        },\\n        {\\n            \"dialogue\": \"他想让他的律师来告我，妍妍跟我谈接下去的法律程序呢。\",\\n            \"said_by\": \"林若溪\"\\n        },\\n        {\\n            \"dialogue\": \"告你！？\",\\n            \"said_by\": \"杨辰\"\\n        },\\n        {\\n            \"dialogue\": \"放心吧，高国雄想告若溪，但他的律师拒绝了，现在他想告若溪，还得另外请，但他一新加坡的商人，而且案子这么明显，大陆谁愿意帮他打官司。\",\\n            \"said_by\": \"蔡妍\"\\n        },\\n        {\\n            \"dialogue\": \"那他新加坡的律师为什么拒绝？\",\\n            \"said_by\": \"杨辰\"\\n        },\\n        {\\n            \"dialogue\": \"你不知道吗，你老婆是超级大富婆哟。\",\\n            \"said_by\": \"蔡妍\"\\n        },\\n        {\\n            \"dialogue\": \"妍妍！难听死了！\",\\n            \"said_by\": \"林若溪\"\\n        },\\n        {\\n            \"dialogue\": \"你这是在耍高国雄啊。\",\\n            \"said_by\": \"杨辰\"\\n        },\\n        {\\n            \"dialogue\": \"这个社会，有钱才能打赢官司。\",\\n            \"said_by\": \"杨辰\"\\n        },\\n        {\\n            \"dialogue\": \"蔡小姐，你就别动手了，来了就是客人，怎么好麻烦你呢。\",\\n            \"said_by\": \"王妈\"\\n        },\\n        {\\n            \"dialogue\": \"让王妈一个人累着，我可没这胆子呢。\",\\n            \"said_by\": \"蔡妍\"\\n        },\\n        {\\n            \"dialogue\": \"那个人有没有回来过？\",\\n            \"said_by\": \"蔡妍\"\\n        },\\n        {\\n            \"dialogue\": \"有些事情过去就过去吧，就当什么也没发生，人老了，我记性也不太好了。\",\\n            \"said_by\": \"王妈\"\\n        },\\n        {\\n            \"dialogue\": \"这种事情，女人总是受委屈的。\",\\n            \"said_by\": \"蔡妍\"\\n        },\\n        {\\n            \"dialogue\": \"若溪宝贝，你看人家蔡局长多懂事，你也是女人，得去帮帮王妈。\",\\n            \"said_by\": \"杨辰\"\\n        },\\n        {\\n            \"dialogue\": \"为什么你自己不去？\",\\n            \"said_by\": \"林若溪\"\\n        },\\n        {\\n            \"dialogue\": \"因为我是男人。\",\\n            \"said_by\": \"杨辰\"\\n        },\\n        {\\n            \"dialogue\": \"嘁……大男子主义。\",\\n            \"said_by\": \"林若溪\"\\n        },\\n        {\\n            \"dialogue\": \"蔡局长，不会我又惹麻烦了吧。\",\\n            \"said_by\": \"杨辰\"\\n        },\\n        {\\n            \"dialogue\": \"杨辰，虽然我们以前有过很多不开心的过去，但我可以向你诚挚地道歉，我知道是我错怪了你。所以，看在我们一起经历过生死，我能不能接受我。\",\\n            \"said_by\": \"蔡妍\"\\n        },\\n        {\\n            \"dialogue\": \"接受你？\",\\n            \"said_by\": \"杨辰\"\\n        },\\n        {\\n            \"dialogue\": \"我……我的意思是，你能不能叫我的名字，不要总是蔡局长蔡局长……好像很生分。\",\\n            \"said_by\": \"蔡妍\"\\n        },\\n        {\\n            \"dialogue\": \"这样……\",\\n            \"said_by\": \"杨辰\"\\n        },\\n        {\\n            \"dialogue\": \"谢谢，那我们是朋友了，对吗？\",\\n            \"said_by\": \"蔡妍\"\\n        },\\n        {\\n            \"dialogue\": \"嗯，朋友。\",\\n            \"said_by\": \"杨辰\"\\n        },\\n        {\\n            \"dialogue\": \"我还要谢谢你，谢谢你上次不顾生命地救了我，如果不是你，我可能已经被乱枪打死了。\",\\n            \"said_by\": \"蔡妍\"\\n        },\\n        {\\n            \"dialogue\": \"知道了，我会当什么都没发生过的。\",\\n            \"said_by\": \"蔡妍\"\\n        },\\n        {\\n            \"dialogue\": \"她的包包落在沙发边上，她忘记拿走了。\",\\n            \"said_by\": \"林若溪\"\\n        },\\n        {\\n            \"dialogue\": \"没什么重要的证件，手机之类的，下次抽空还给她，或者等她来拿吧。\",\\n            \"said_by\": \"林若溪\"\\n        },\\n        {\\n            \"dialogue\": \"去哪？\",\\n            \"said_by\": \"杨辰\"\\n        },\\n        {\\n            \"dialogue\": \"跟一个人约好去一个地方，晚饭也不回来了，你跟王妈吃吧。\",\\n            \"said_by\": \"林若溪\"\\n        }\\n    ]\\n}'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "super_start = 20\n",
        "super_end = len(datas)\n",
        "\n",
        "n_bag = 50\n",
        "\n",
        "# 计算每个子区间的长度\n",
        "interval_length = (super_end - super_start) // n_bag\n",
        "\n",
        "# 生成元组列表\n",
        "se_tuples_list = [(super_start + i * interval_length, super_start + (i + 1) * interval_length) for i in range(n_bag - 1)]\n",
        "\n",
        "# 添加最后一个元组，确保最后一个end等于super_end\n",
        "se_tuples_list.append((se_tuples_list[-1][1], super_end))\n",
        "\n",
        "# 打印生成的元组列表\n",
        "print(se_tuples_list[0:2])\n",
        "print(se_tuples_list[-2:])\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "random.shuffle(se_tuples_list)\n",
        "\n",
        "print(se_tuples_list[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bVbVGwudXu4",
        "outputId": "28c54beb-b6af-4340-b1d4-51c43319f867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(20, 674), (674, 1328)]\n",
            "[(31412, 32066), (32066, 32732)]\n",
            "(20, 674)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "for start_id, end_id in se_tuples_list:\n",
        "    current_tasks = datas[start_id:end_id]\n",
        "\n",
        "    await main(current_tasks)\n",
        "    await main(current_tasks)\n",
        "\n",
        "    temp_output_folder = \"/content/output\"\n",
        "\n",
        "    for id in range(start_id, end_id):\n",
        "        file_path = os.path.join(temp_output_folder, f\"{id}.txt\")\n",
        "        if os.path.exists(file_path):\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                try:\n",
        "                    data = json.load(f)\n",
        "                    response = data[\"response\"]\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if response is not None:\n",
        "                datas[id][\"response\"] = response\n",
        "        # break\n",
        "\n",
        "    final_save_name = \"/content/drive/MyDrive/CardBuild/dialogue_extract/\" + str(start_id) + \"_to_\" + str(end_id) + \".txt\"\n",
        "\n",
        "    with open(final_save_name, 'w', encoding='utf-8') as f:\n",
        "        for id in range(start_id, end_id):\n",
        "            json.dump(datas[id], f, ensure_ascii=False)\n",
        "            f.write('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2QVvvRjdkn4",
        "outputId": "5957ec23-62d0-4aed-ab5b-f31b47d20ede"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing items: 100%|██████████| 654/654 [19:04<00:00,  1.75s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [23:42<00:00,  2.18s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [28:41<00:00,  2.63s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items:   6%|▌         | 37/654 [01:03<10:22,  1.01s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Error] <html>\r\n",
            "<head><title>502 Bad Gateway</title></head>\r\n",
            "<body>\r\n",
            "<center><h1>502 Bad Gateway</h1></center>\r\n",
            "<hr><center>cloudflare</center>\r\n",
            "</body>\r\n",
            "</html>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing items: 100%|██████████| 654/654 [32:12<00:00,  2.96s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [27:46<00:00,  2.55s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [20:08<00:00,  1.85s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [19:39<00:00,  1.80s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [24:53<00:00,  2.28s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [18:56<00:00,  1.74s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [20:19<00:00,  1.86s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [18:02<00:00,  1.65s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [19:11<00:00,  1.76s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [18:57<00:00,  1.74s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [22:13<00:00,  2.04s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [19:38<00:00,  1.80s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [19:23<00:00,  1.78s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [17:39<00:00,  1.62s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [17:09<00:00,  1.57s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [17:32<00:00,  1.61s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [17:38<00:00,  1.62s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [17:16<00:00,  1.58s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [20:41<00:00,  1.90s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [19:17<00:00,  1.77s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [17:01<00:00,  1.56s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [16:21<00:00,  1.50s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [17:12<00:00,  1.58s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 653/653 [17:16<00:00,  1.59s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [15:56<00:00,  1.46s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [17:02<00:00,  1.56s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [16:49<00:00,  1.54s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [15:42<00:00,  1.44s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [16:45<00:00,  1.54s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [15:55<00:00,  1.46s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [16:24<00:00,  1.51s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [15:42<00:00,  1.44s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [15:49<00:00,  1.45s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [17:52<00:00,  1.64s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [16:08<00:00,  1.48s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [16:30<00:00,  1.51s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [16:01<00:00,  1.47s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [16:40<00:00,  1.53s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [16:09<00:00,  1.48s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [17:01<00:00,  1.56s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [17:11<00:00,  1.58s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [16:54<00:00,  1.55s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 666/666 [17:12<00:00,  1.55s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [17:09<00:00,  1.57s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [17:08<00:00,  1.57s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 654/654 [17:45<00:00,  1.63s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items:  83%|████████▎ | 542/654 [13:27<01:57,  1.05s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H7fnHrAritFt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}