{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOPdxcsx9lHqzrjuU9zdgqA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/Zero-Haruhi/blob/main/notebook/%E5%88%A9%E7%94%A8qwen%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E5%AF%B9%E8%AF%9D%E6%8A%BD%E5%8F%96_async%E6%8B%93%E5%B1%95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [ ] 换另一本小说\n",
        "- [ ] 制作一个foo的summary和一个foo的抽取函数\n",
        "- [ ] 模拟抽取流程"
      ],
      "metadata": {
        "id": "Wmg7NNToDp2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "先从drive上找到小说"
      ],
      "metadata": {
        "id": "wi4BvkROE8nN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrF34yqnDNlz",
        "outputId": "cf8ea98c-638f-431e-bfba-a138ff0de41a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/Wuxia/input/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8otj8xVYF-wB",
        "outputId": "eb38203a-5aa7-40b5-ee93-07e45b86c9e6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1819部精校小说.zip  classic_50.zip  parts_1.zip  parts_3.zip  parts_5.zip  parts_7.zip\tparts_9.zip\n",
            "all_direct\t    parts_0.zip     parts_2.zip  parts_4.zip  parts_6.zip  parts_8.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "part_id = 11"
      ],
      "metadata": {
        "id": "3zsv18auGQ8C"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "这个zip在\n",
        "\n",
        "https://drive.google.com/file/d/101g9BfaKLNv10P1C1wuePTdbR6rwKRpb/view?usp=sharing"
      ],
      "metadata": {
        "id": "Tr0scWHsU5xX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# part_id = 10\n",
        "\n",
        "# input_file = f\"/content/drive/MyDrive/Wuxia/input/parts_\" + str(part_id) + \".zip\"\n",
        "\n",
        "input_file = \"/content/drive/MyDrive/Wuxia/input/classic_50.zip\"\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "output_dir = \"/content/input\"\n",
        "\n",
        "# 解压input_file到output_dir\n",
        "with zipfile.ZipFile(input_file,\"r\") as zip_ref:\n",
        "    zip_ref.extractall(output_dir)"
      ],
      "metadata": {
        "id": "aq2ftmCvE7Jn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import chardet\n",
        "\n",
        "output_folder = \"/content/input/output2\"\n",
        "# output_folder = f\"/content/output/parts_{part_id}\"\n",
        "\n",
        "\n",
        "file_names = []\n",
        "\n",
        "# 遍历所有txt文件\n",
        "for root, dirs, files in os.walk(output_folder):\n",
        "    for file in files:\n",
        "        if file.endswith(\".txt\"):\n",
        "            file_path = os.path.join(root, file)\n",
        "\n",
        "            with open(file_path, 'rb') as f:\n",
        "                content = f.read(1000)\n",
        "            result = chardet.detect(content)\n",
        "\n",
        "            if result['confidence'] < 0.9:\n",
        "                print(f\"Warning: {file_path} encoding confidence {result['confidence']:.0%} lower than 90%\")\n",
        "                continue\n",
        "\n",
        "            file_names.append({\n",
        "                \"file_name\": file_path,\n",
        "                \"encoding\": result['encoding'],\n",
        "                \"confidence\": result['confidence']\n",
        "            })"
      ],
      "metadata": {
        "id": "4gnjErqIFz7H"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file_name in file_names[:5]:\n",
        "    print(file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot9yta93Ged1",
        "outputId": "79fca8b4-8279-4ee3-ac09-df4f1a92b193"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'file_name': '/content/input/output2/《盗墓笔记》（精校全本）作者：南派三叔.txt', 'encoding': 'GB2312', 'confidence': 0.99}\n",
            "{'file_name': '/content/input/output2/《北京遇上西雅图》（精校全本）作者：薛晓璐.txt', 'encoding': 'GB2312', 'confidence': 0.99}\n",
            "{'file_name': '/content/input/output2/《冰与火之歌05魔龙的狂舞》（精校全本）作者：乔治·R·R·马丁.txt', 'encoding': 'GB2312', 'confidence': 0.99}\n",
            "{'file_name': '/content/input/output2/《全职高手》（校对全本）作者：蝴蝶蓝.txt', 'encoding': 'GB2312', 'confidence': 0.99}\n",
            "{'file_name': '/content/input/output2/《大染坊》（精校全本）作者：陈杰.txt', 'encoding': 'GB2312', 'confidence': 0.99}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def get_file_name(fname):\n",
        "    last_split = fname.split('/')[-1]\n",
        "\n",
        "    pattern = r'《(.+)》'\n",
        "    m = re.match(pattern, last_split)\n",
        "    if m:\n",
        "        # print(\"match\")\n",
        "        return m.group(1) + '.txt'\n",
        "    else:\n",
        "        return last_split\n",
        "\n",
        "fname = \"/content/output/output2/《凡人修仙传》(精校全本)作者:忘语.txt\"\n",
        "\n",
        "print(get_file_name(fname)[:-4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJi40TCcHweD",
        "outputId": "fcd3b902-480a-4b29-8219-3c2d52b68357"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "凡人修仙传\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_chapater(line):\n",
        "    line = line.strip()\n",
        "    line = line.strip('-=*')\n",
        "    if len(line) > 100:\n",
        "        return -1\n",
        "    if len(line) == 0:\n",
        "        return -1\n",
        "    is_short = len(line) < 10\n",
        "\n",
        "    head_5_char = line[:min(5,len(line))]\n",
        "\n",
        "    small_line = line.lower()\n",
        "    line = line[:20]\n",
        "    has_di = line.find('第', 0 ) >= 0\n",
        "    di_in_head = head_5_char.find('第', 0) >= 0\n",
        "\n",
        "    has_zhang = False\n",
        "    zhang_in_head = False\n",
        "\n",
        "    zhang_word = ['章', '集', '卷','回','期','节']\n",
        "\n",
        "\n",
        "\n",
        "    for w in zhang_word:\n",
        "        if head_5_char.find(w, 0) >= 0:\n",
        "            zhang_in_head = True\n",
        "            break\n",
        "\n",
        "    for w in zhang_word:\n",
        "        if line.find(w, 0) >= 0:\n",
        "            has_zhang = True\n",
        "            break\n",
        "\n",
        "    has_digital = False\n",
        "    digital_word = ['0','1','2','3','4','5','6','7','8','9',\\\n",
        "                    '一','二','三','四','五','六','七','八','九']\n",
        "\n",
        "    only_digital = False\n",
        "    digital_count = 0\n",
        "    for ch in line:\n",
        "        if ch in digital_word:\n",
        "            digital_count += 1\n",
        "    if digital_count == len(line):\n",
        "        only_digital = True\n",
        "\n",
        "    digital_in_head = False\n",
        "\n",
        "    for w in digital_word:\n",
        "        if head_5_char.find(w, 0) >= 0:\n",
        "            digital_in_head = True\n",
        "            break\n",
        "    for w in digital_word:\n",
        "        if line.find(w, 0) >= 0:\n",
        "            has_digital = True\n",
        "            break\n",
        "    if has_di and has_zhang and has_digital and (di_in_head or digital_in_head or zhang_in_head):\n",
        "        return 3\n",
        "    if di_in_head and digital_in_head and is_short:\n",
        "        return 2\n",
        "    if zhang_in_head and digital_in_head and is_short:\n",
        "        return 2\n",
        "    # if only_digital and is_short:\n",
        "    #     return 2\n",
        "    return -1\n"
      ],
      "metadata": {
        "id": "0n7-2eLmIElG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q transformers accelerate tiktoken einops scipy transformers_stream_generator==0.0.4 peft deepspeed\n",
        "!pip install -q auto-gptq optimum"
      ],
      "metadata": {
        "id": "uSL6w67YJxrQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"silk-road/Haruhi-Dialogue-Speaker-Extract-csv\", trust_remote_code=True)\n",
        "\n",
        "num_workers = 2\n",
        "\n",
        "models = []\n",
        "\n",
        "for i in range(num_workers):\n",
        "    models.append(None)\n",
        "    models[i] = AutoModelForCausalLM.from_pretrained(\"silk-road/Haruhi-Dialogue-Speaker-Extract-csv\", device_map=\"auto\", trust_remote_code=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDLuw9nXJqCb",
        "outputId": "768796f0-ffeb-4a4a-96c7-2a57f487a265"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.silk-road.Haruhi-Dialogue-Speaker-Extract-csv.739e106ee811b915db1869ae6cb2ffac2da8df24.modeling_qwen:Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
            "WARNING:transformers_modules.silk-road.Haruhi-Dialogue-Speaker-Extract-csv.739e106ee811b915db1869ae6cb2ffac2da8df24.modeling_qwen:Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
            "WARNING:transformers_modules.silk-road.Haruhi-Dialogue-Speaker-Extract-csv.739e106ee811b915db1869ae6cb2ffac2da8df24.modeling_qwen:Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
            "WARNING:transformers_modules.silk-road.Haruhi-Dialogue-Speaker-Extract-csv.739e106ee811b915db1869ae6cb2ffac2da8df24.modeling_qwen:Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
            "WARNING:transformers_modules.silk-road.Haruhi-Dialogue-Speaker-Extract-csv.739e106ee811b915db1869ae6cb2ffac2da8df24.modeling_qwen:Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
            "WARNING:transformers_modules.silk-road.Haruhi-Dialogue-Speaker-Extract-csv.739e106ee811b915db1869ae6cb2ffac2da8df24.modeling_qwen:Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model in models:\n",
        "    model = model.eval()"
      ],
      "metadata": {
        "id": "mKhX0opRmKLM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FEWudMD6mJ_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokenizer.tokenize(\"我是一只猫\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VGVVr_NKFyz",
        "outputId": "b12a506f-f38b-4b4d-e290-469f55b8105c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(models[0].generation_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvSexkI9j1Ef",
        "outputId": "a80fa1aa-7b9a-4e84-d32a-2c94d0c12df5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"chat_format\": \"chatml\",\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"max_new_tokens\": 512,\n",
            "  \"max_window_size\": 6144,\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"top_k\": 0,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  定义divide函数，用来切分超长文本\n",
        "def divide_str(s, sep=['\\n', '.', '。']):\n",
        "    mid_len = len(s) // 2  # 中心点位置\n",
        "    best_sep_pos = len(s) + 1  # 最接近中心点的分隔符位置\n",
        "    best_sep = None  # 最接近中心点的分隔符\n",
        "    for curr_sep in sep:\n",
        "        sep_pos = s.rfind(curr_sep, mid_len // 2, len(s)-mid_len // 2)  # 从中心点往左找分隔符\n",
        "        if sep_pos > 0 and abs(sep_pos - mid_len) < abs(best_sep_pos -\n",
        "                                                        mid_len):\n",
        "            best_sep_pos = sep_pos\n",
        "            best_sep = curr_sep\n",
        "    if not best_sep:  # 没有找到分隔符\n",
        "        return s, ''\n",
        "    return s[:best_sep_pos + 1], s[best_sep_pos + 1:]\n",
        "\n",
        "\n",
        "def strong_divide(s):\n",
        "    left, right = divide_str(s)\n",
        "\n",
        "    if right != '':\n",
        "        return left, right\n",
        "\n",
        "    whole_sep = ['\\n', '.', '，', '、', ';', ',', '；',\\\n",
        "                 '：', '！', '？', '(', ')', '”', '“', \\\n",
        "                 '’', '‘', '[', ']', '{', '}', '<', '>', \\\n",
        "                 '/', '''\\''', '|', '-', '=', '+', '*', '%', \\\n",
        "               '$', '''#''', '@', '&', '^', '_', '`', '~',\\\n",
        "                 '·', '…']\n",
        "    left, right = divide_str(s, sep=whole_sep)\n",
        "\n",
        "    if right != '':\n",
        "        return left, right\n",
        "\n",
        "    mid_len = len(s) // 2\n",
        "    return s[:mid_len], s[mid_len:]"
      ],
      "metadata": {
        "id": "gqhTO9b1Kt_6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "save_folder = \"/content/output_in_chunk\"\n",
        "\n",
        "TOKEN_PER_TRUNK = 600\n",
        "\n",
        "if not os.path.exists(save_folder):\n",
        "    os.makedirs(save_folder)\n",
        "\n",
        "import codecs\n",
        "\n",
        "from tqdm import tqdm\n",
        "# for file_name_data in tqdm(file_names):\n",
        "for file_name_data in file_names:\n",
        "    file_name = file_name_data['file_name']\n",
        "    if file_name.find(\"《大染坊》\",0)<0:\n",
        "        # 先找一本进行调试\n",
        "        continue\n",
        "\n",
        "    book_name = get_file_name(file_name)\n",
        "    if len(book_name) > 4:\n",
        "        book_name = book_name[:-4]\n",
        "    fencoding = file_name_data['encoding']\n",
        "    f = codecs.open(file_name, 'r', encoding=fencoding, errors = 'ignore')\n",
        "    raw_text = f.read()\n",
        "    lines = raw_text.splitlines()\n",
        "    f.close()\n",
        "\n",
        "    head_count = 0\n",
        "\n",
        "    is_heads = [predict_chapater(line) > 0 for line in lines]\n",
        "\n",
        "    token_per_line = []\n",
        "\n",
        "    for line in tqdm(lines):\n",
        "        if line.strip() == '':\n",
        "            token_per_line.append(0)\n",
        "            continue\n",
        "        n_token = len(tokenizer.tokenize(line.strip()))\n",
        "        token_per_line.append(n_token)\n",
        "\n",
        "    # 这里还要处理 如果单个line超过TOKEN_PER_TRUNK, 需要调用strong_divide进行分裂\n",
        "    # 同时第一个line保留原来的is_head, 其他的都是false\n",
        "\n",
        "\n",
        "    for line, is_head in zip(lines, is_heads):\n",
        "        if is_head:\n",
        "            head_count += 1\n",
        "\n",
        "\n",
        "\n",
        "    print(file_name_data)\n",
        "    print(head_count)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eR8huGzGggl",
        "outputId": "008fe71e-8bb0-47c1-96af-a0196eb8f561"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7492/7492 [00:00<00:00, 16838.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'file_name': '/content/input/output2/《大染坊》（精校全本）作者：陈杰.txt', 'encoding': 'GB2312', 'confidence': 0.99}\n",
            "30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(token_per_line)/600)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JN7NcJ6HHMkh",
        "outputId": "282be109-9b22-4b75-d59c-01ee2fa4b08f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "475.4166666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JsKOw_CyMxCy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = []\n",
        "\n",
        "current_n = 0\n",
        "current_chunk = \"\"\n",
        "current_chunk_is_head = True\n",
        "last_chunk_is_head = False\n",
        "\n",
        "for line, is_head, n_token in zip(lines, is_heads, token_per_line):\n",
        "    # 如果is_head且之前不是head\n",
        "    # 如果is_head且之前的current_n > TOKEN_PER_TRUNK // 2 需要结算\n",
        "    # 如果n_token + current_n > TOKEN_PER_TRUNK 需要结算\n",
        "    count_flag = False\n",
        "\n",
        "    if is_head and not last_chunk_is_head:\n",
        "        count_flag = True\n",
        "    elif is_head and last_chunk_is_head and current_n > TOKEN_PER_TRUNK // 2:\n",
        "        count_flag = True\n",
        "    elif n_token + current_n > TOKEN_PER_TRUNK:\n",
        "        count_flag = True\n",
        "\n",
        "    if count_flag and current_chunk.strip() != \"\":\n",
        "        chunk_data = {\n",
        "            \"text\": current_chunk,\n",
        "            \"is_head\": current_chunk_is_head\n",
        "        }\n",
        "        chunks.append(chunk_data)\n",
        "        last_chunk_is_head = current_chunk_is_head\n",
        "        current_chunk_is_head = is_head\n",
        "        current_chunk = \"\"\n",
        "        current_n = 0\n",
        "\n",
        "    current_chunk += line + \"\\n\"\n",
        "    current_n += n_token\n",
        "\n",
        "if current_chunk.strip() != \"\":\n",
        "    chunk_data = {\n",
        "        \"text\": current_chunk,\n",
        "        \"is_head\": current_chunk_is_head\n",
        "    }\n",
        "    chunks.append(chunk_data)"
      ],
      "metadata": {
        "id": "MQonoAgSKP9k"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum([n_token>TOKEN_PER_TRUNK for n_token in token_per_line]))\n",
        "print(sum([ chunk[\"is_head\"] == True for chunk in chunks ]))\n",
        "print(len(chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFYjUriuOSmG",
        "outputId": "9d600bf8-111e-41ae-ffe8-5cb3827992f7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "31\n",
            "524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "# 做个备份\n",
        "org_chunk = copy.deepcopy(chunks)"
      ],
      "metadata": {
        "id": "zf48xLezXleQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def foo_summary( text ):\n",
        "    return text[:10] + \"...\" + text[-10:]\n",
        "\n",
        "def foo_extract( text):\n",
        "    return \"dialogue text | said by | speaker\\n\" + text[:20]"
      ],
      "metadata": {
        "id": "GW9KAGazXpxs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_prompt = \"\"\"请对以下小说文本进行总结，原文:\"\"\"\n",
        "extract_prompt = \"\"\"给定input paragraph，以及相应的summary，抽取每一句对话的内容，判断每一句话的说话人 以<dialogue text> | said by | <speaker>的形式输出成csv格式\"\"\"\n"
      ],
      "metadata": {
        "id": "kuYRQdXIg8HI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for data in chunks:\n",
        "#     if \"summary\" in data:\n",
        "#         del data[\"summary\"]\n",
        "\n",
        "plausible_start_ids = [ id for id, chunk in enumerate(chunks) if chunk[\"is_head\"] ]\n",
        "\n",
        "\n",
        "# 先解决所有的summary\n",
        "\n",
        "if len(plausible_start_ids) < num_workers:\n",
        "    step = len(chunks) // (num_workers*2)\n",
        "    for i in range(1,(num_workers*2)):\n",
        "        plausible_start_ids.append(i * step)\n",
        "\n",
        "# 记录last summary\n",
        "\n",
        "valid_ids = [(id,\"\") for id in plausible_start_ids]\n",
        "\n",
        "count_deal = 0\n",
        "\n",
        "import random\n",
        "\n",
        "# 定义异步函数来处理输入\n",
        "async def process_summary(model, input):\n",
        "    current_summary, _ = model.chat(tokenizer, input, history=[])\n",
        "    return current_summary\n",
        "\n",
        "import asyncio\n",
        "# 主函数来并行处理输入\n",
        "async def process_summaires_in_async(datas):\n",
        "    results = await asyncio.gather( \\\n",
        "        *(process_summary( models[id], data['input']) for id, data in enumerate(datas)))\n",
        "\n",
        "    for data, result in zip(datas,results):\n",
        "        data['output'] = result\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "for _ in tqdm(range( len(chunks) )):\n",
        "    if count_deal >= len(chunks):\n",
        "        if count_deal != len(chunks):\n",
        "            print('warning! invalid count_deal ', count_deal)\n",
        "        break\n",
        "\n",
        "    # 仍然valid的ids是 plausible_ids_tuple 中且对应的 tuple的第二个字段不为\"FINISHED\"的\n",
        "    valid_ids = [ (id,last_summary) for id,last_summary in valid_ids if last_summary.strip() != \"FINISHED\" ]\n",
        "\n",
        "    if len(valid_ids)>num_workers:\n",
        "        job_ids = random.sample(range(len(valid_ids)), num_workers)\n",
        "    else:\n",
        "        job_ids = range(len(valid_ids))\n",
        "\n",
        "    batch_inputs = []\n",
        "\n",
        "    for job_id in job_ids:\n",
        "        id = valid_ids[job_id][0]\n",
        "        last_summary = valid_ids[job_id][1]\n",
        "\n",
        "        #批量处理摘要\n",
        "        text = chunks[id][\"text\"]\n",
        "        if last_summary.strip() != \"\":\n",
        "            text = last_summary + \"\\n\" + text\n",
        "\n",
        "        batch_inputs.append(summary_prompt + text)\n",
        "\n",
        "    datas = [{\"input\":input} for input in batch_inputs]\n",
        "    await process_summaires_in_async(datas)\n",
        "    batch_outputs = [ data[\"output\"] for data in datas]\n",
        "\n",
        "    for job_id, current_summary in zip(job_ids,batch_outputs):\n",
        "        id = valid_ids[job_id][0]\n",
        "\n",
        "        chunks[id][\"summary\"] = current_summary\n",
        "        chunks[id][\"summary_order\"] = count_deal\n",
        "        count_deal += 1\n",
        "\n",
        "        finished_flag = False\n",
        "\n",
        "        if id + 1 >= len(chunks):\n",
        "            finished_flag = True\n",
        "        elif chunks[id + 1][\"is_head\"]:\n",
        "            finished_flag = True\n",
        "        elif \"summary\" in chunks[id + 1]:\n",
        "            print(\"warning! , 冲撞 id = \",id)\n",
        "            finished_flag = True\n",
        "\n",
        "        if finished_flag:\n",
        "            valid_ids[job_id] = (id,\"FINISHED\")\n",
        "        else:\n",
        "            valid_ids[job_id] = (id+1,current_summary)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LzOfBBQZPNC",
        "outputId": "59c927a2-738a-400f-8934-249ee65d14fc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 263/524 [57:44<57:18, 13.17s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们来实验双模型并行的方法"
      ],
      "metadata": {
        "id": "0sZxjOsehhxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = len(chunks)\n",
        "\n",
        "# 定义异步函数来处理输入\n",
        "async def process_extract(model, input_text):\n",
        "    csv_response, _ = model.chat(tokenizer, input_text, system = extract_prompt, history=[])\n",
        "    return csv_response\n",
        "\n",
        "import asyncio\n",
        "# 主函数来并行处理输入\n",
        "async def process_extract_in_async(datas):\n",
        "    results = await asyncio.gather( \\\n",
        "        *(process_extract( models[id], data['input']) for id, data in enumerate(datas)))\n",
        "\n",
        "    for data, result in zip(datas,results):\n",
        "        data['output'] = result\n",
        "\n",
        "\n",
        "\n",
        "for id in tqdm(range(0,n,2)):\n",
        "    right_id = min(n, id + 2)\n",
        "    datas = []\n",
        "    for chunk in chunks[id:right_id]:\n",
        "        text = chunk[\"text\"]\n",
        "        summary = chunk[\"summary\"]\n",
        "        input_text = f\"paragraph:\\n{text}\\n\\nsummary:\\n{summary}\"\n",
        "        datas.append({\"input\":input_text})\n",
        "\n",
        "    await process_extract_in_async(datas)\n",
        "\n",
        "    for sub_id, data in enumerate(datas):\n",
        "        chunks[id+sub_id][\"csv_response\"] = data[\"output\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sh5ue_-o6ELh",
        "outputId": "fffb8405-f206-4601-861a-52ead4f7792e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 6/262 [01:53<1:32:28, 21.67s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_save_name = f\"/content/drive/MyDrive/CardBuild/exp0130/daranfang.txt\"\n",
        "\n",
        "with open(final_save_name, \"w\", encoding=\"utf-8\") as f:\n",
        "    for chunk in chunks:\n",
        "        f.write(json.dumps(chunk, ensure_ascii=False) + \"\\n\")"
      ],
      "metadata": {
        "id": "6H4hBuOt72fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "it5x_CVE8FyK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}