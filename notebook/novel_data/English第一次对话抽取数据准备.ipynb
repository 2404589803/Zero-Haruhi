{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPshL/2qmKnV+N9aATKtGQ+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/Zero-Haruhi/blob/main/notebook/English%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AF%B9%E8%AF%9D%E6%8A%BD%E5%8F%96%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqEYW8boPaXn",
        "outputId": "ab6449f1-08f1-4697-896b-43add93e4d4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.4/223.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import httpx\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"]  = \"sk-JEBRU\"\n",
        "\n",
        "import openai\n",
        "from openai import AsyncOpenAI\n"
      ],
      "metadata": {
        "id": "aMr5D92qPcdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Z_hevNCrRIJ",
        "outputId": "57acd1f8-3c9f-4fa2-d84d-a2e8e118d166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_name = \"/content/drive/MyDrive/CardBuild/exp0122/english_segs.txt\"\n",
        "\n",
        "import json\n",
        "\n",
        "datas = []\n",
        "\n",
        "with open(input_name, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        if line.strip() == \"\":\n",
        "            continue\n",
        "        data = json.loads(line)\n",
        "        datas.append(data)\n"
      ],
      "metadata": {
        "id": "45cH9j3fM12v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "因为是第一次抽取，所以我们只要每个seg的第一个"
      ],
      "metadata": {
        "id": "4Cw4ig6FNFhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(datas[0].keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xLvOqkpNMNW",
        "outputId": "c9f18c5f-18b3-41fb-9807-bac2513d48c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['id', 'raw_seg'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_quotes(text):\n",
        "\n",
        "    quote_chars = '''「」\"“”'''\n",
        "    count = 0\n",
        "    for char in text:\n",
        "        if char in quote_chars:\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "input_datas = []\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "for data in tqdm(datas):\n",
        "    if data[\"raw_seg\"] == []:\n",
        "        continue\n",
        "    text = data[\"raw_seg\"][0]\n",
        "    input_data = {\n",
        "        \"id\": str(data[\"id\"]) + '_first',\n",
        "        \"text\": text,\n",
        "        \"n_quote\": count_quotes(text)\n",
        "    }\n",
        "    input_datas.append(input_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og2nVbjONDsU",
        "outputId": "cd7fea20-c648-4fb1-ae7f-8b01e578eb00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17613/17613 [00:04<00:00, 4237.32it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_datas[0]['id'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICH3VwiGQPjB",
        "outputId": "a5ef420a-8512-4a60-ae69-5ce9dec8627a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0_first\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "给定Paragraph，抽取其中的对话，并输出为json格式\n",
        "\n",
        "Let's think it step by step\n",
        "1. 对Paragraph进行总结，存储在summary字段\n",
        "2. 抽取每一句对话的内容 dialogue，以及对话的说话人 said by, 存储在conversations中\n",
        "\n"
      ],
      "metadata": {
        "id": "39lFGhV8NE72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_order = list(range(len(input_datas)))\n",
        "import random\n",
        "random.shuffle(random_order)\n",
        "\n",
        "for id in random_order:\n",
        "    data = input_datas[id]\n",
        "    if data[\"n_quote\"] > 30:\n",
        "        print(data[\"text\"])\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZO23Or1N6Fv",
        "outputId": "03cadee5-58b8-4690-d871-cd3ec1cea265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter 1\n",
            "It's been one week since Deuce has been born and Sarah and Damion are sitting in the hospital room alone and Sarah says,\"Are you going to change me now that Deuce has been born?\"  \"No. I absolutely refuse to change you,\" Damion said.  \"But what happens if they find us and the elder finds out that I had the baby and you haven't changed me yet?\" Sarah asked.  \"I-,\" Damion replied.  \"Well? Are you going to answer me?\" Sarah asked.  \"I don't know what's going to happen. If they find us they arer going to want me dead,\" Damion replied.  \"Why would they want you dead?\" Sarah asked.  \"Cause I killed Erika,\" Damion replied,\"When are you going to take Deuce home anyway?\"  \"I don't know,\" Sarah said in a pissed off tone.  \"Are you mad at me now?\" Damion asked.  \"What do you think?\" Sarah replied,\"Are you going to change me or let them kill me if they find us?\"  \"I would really hate to see you killed. But I also don't want this life for you,\" Damion said.  \"I understand that. But I don't want to life my life on the run all the time either,\" Sarah said.  \"I know you don't. That's why I made the decision to leave and never come back,\" Damion said,\"And you are going to stay here with your mom.\"  \"But Damion, I don't want you to leave. I want you to stay here with me,\" Sarah said, a tear slipping out of her eye.  \"I know. But you and Deuce will be safer if I'm gone,\" Damion said.  \"So you're just going to leave,\" Sarah said, another tear slipping from her eye.  \"I'm leaving cause I want you to be safe and live a human life. Trust me leaving you hurts me too. But it's the only way that I know you and Deuce will be safe,\" Damion said.  \"But Damion, will I ever see you again?\" Sarah said.  \"Only in your dreams. I'm never going to return here,\" Damion said.  \"Okay. When are you leaving?\" Sarah asked.  \"This is the last time you will ever see me,\" Damion replied.  \"Okay,\" Sarah said, Tears falling from her eyes.  Damion bends down and kisses Deuce on the forhead and then presses his lips to Sarah's with as much passion as he can. After a few min they break apart and Damion says,\"Goodbye Sarah. I will always love you.\"  \"Goodbye. Me too,\" Sarah said. Damion leaves and Sarah breaks down in tears and cries herself to sleep. The next morning A nurse walks in and says,\"You are going home today.\"  \"Okay. Thank you,\" Sarah said in a depressed tone. A few hours pass by and Sarah is at home feeding Deuce. While she is feeding Deuce her mom notices that she is depressed and says,\"Is everything okay dear?\"  \"No. Damion left and he isn't coming back,\" Sarah said.  \"It'll be okay,\" Chloe said.  \"I know. But the last words he said to me before he left yesterday are-,\" Sarah started but wasn't able to finish.  \"What were his last words to you?\" Chloe asked.  \"His lat words were, 'I will always love you.' It hurts to know that I'll never see him again other then in my dreams,\" Sarah repled.   \"Wow. At least you know that he still loves you,\" Chloe said.   \"Yeah. I guess,\" Sarah said.  Sarah finishes feeding Deuce and says,\"I'm going to put Deuce to bed. I'll talk to you later mom.\"  \"Okay. Talk to you later dear,\" Chloe said. Sarah goes up and puts Deuce in his crib and thinks, \"Why did you have to leave Damion?\"  After Sarah puts Deuce in his crib she goes to bed herself.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "task_prompt = \"\"\"\"Given an input paragraph, extract the dialogues within it, and output them in JSON format.\n",
        "\n",
        "Let's think about it step by step:\n",
        "- Summarize the input paragraph into bullet points and store it in the 'summary' field.\n",
        "- recall the line number('num'), Extract the content of each dialogue ('dialogue'), identify the speaker for each sentence ('said by'), and store these in 'conversations'.\"\"\"\n",
        "\n",
        "\n",
        "long_task_prompt_prefix = \"\"\"Given an input paragraph, extract the dialogues within it, and output them in JSON format.\n",
        "\n",
        "Let's think about it step by step:\n",
        "- Summarize the input paragraph into bullet points and store it in the 'summary' field.\n",
        "- recall the line number('num'), Extract the content of each dialogue ('dialogue'), identify the speaker for each sentence ('said by'), and store these in 'conversations'.\"\"\"\n",
        "\n",
        "no_dialogue_prompt_hint = \"if it is not a dialogue, output conversation as an empty list\"\n",
        "\n",
        "long_task_prompt_example = \"\"\"\n",
        "Example input paragraph:\n",
        "1 The sun was setting behind the hills, casting long shadows across the valley. Birds chirped their evening songs, and a gentle breeze stirred the leaves.\n",
        "2 Amidst this serene scene, Alex and Jamie were having a heated argument. Alex exclaimed, 'I can't believe you didn't tell me about the meeting yesterday!' Jamie retorted, 'Well, I thought you already knew since it was on the calendar!'\n",
        "3 Their voices rose above the tranquil sounds of nature. Alex, trying to calm down, said, 'We need to communicate better in the future.'\n",
        "4 Jamie agreed, replying, 'You're right, I should have double-checked with you.'\n",
        "\n",
        "example output:\n",
        "{\n",
        "    \"summary\": \"Sunset behind hills, casting shadows in the valley. Birds chirping, gentle breeze stirring leaves. Alex and Jamie having a heated argument. Discussion about a missed meeting and communication issues.\",\n",
        "    \"conversations\": [\n",
        "        {\n",
        "            \"num\": 2,\n",
        "            \"dialogue\": \"I can't believe you didn't tell me about the meeting yesterday!\",\n",
        "            \"said by\": \"Alex\"\n",
        "        },\n",
        "        {\n",
        "            \"num\": 2,\n",
        "            \"dialogue\": \"Well, I thought you already knew since it was on the calendar!\",\n",
        "            \"said by\": \"Jamie\"\n",
        "        },\n",
        "        {\n",
        "            \"num\": 3,\n",
        "            \"dialogue\": \"We need to communicate better in the future.\",\n",
        "            \"said by\": \"Alex\"\n",
        "        },\n",
        "        {\n",
        "            \"num\": 4,\n",
        "            \"dialogue\": \"You're right, I should have double-checked with you.\",\n",
        "            \"said by\": \"Jamie\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "7l8sC21JN1FV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q aiofiles tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdXZX7KgRlj0",
        "outputId": "63daac9f-7ee7-4d76-da3f-46f88fd752b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "# import openai\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "aclient = AsyncOpenAI()\n",
        "\n",
        "import asyncio\n",
        "import aiofiles\n",
        "import tiktoken\n",
        "import hashlib\n",
        "# from connector import AsyncPGConnector\n",
        "from tqdm.asyncio import tqdm as tqdm\n",
        "\n",
        "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "en2zh_ratio = 2.3\n",
        "\n",
        "delay = 1\n",
        "concurrency_limit = 16\n",
        "\n",
        "max_file_size = 1024**3"
      ],
      "metadata": {
        "id": "z1oHyqNsELxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def dealing_messages(messages):\n",
        "    try:\n",
        "        # request_token = sum([len(enc.encode(msg['content'])) for msg in messages])\n",
        "        # response_token = int(len(enc.encode(text)) * en2zh_ratio) + 64\n",
        "\n",
        "        model = \"gpt-3.5-turbo-1106\"\n",
        "\n",
        "        resp = await aclient.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            temperature=0,\n",
        "            response_format={ \"type\": \"json_object\" }\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            result = resp.choices[0].message.content\n",
        "            result = result.strip()\n",
        "            return result\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Invalid json: \", result)\n",
        "            return None\n",
        "        except:\n",
        "            raise Exception(f\"Invalid API response: {resp}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[Error] {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def add_line_num( text ):\n",
        "    new_text = \"\"\n",
        "    lines = text.split(\"\\n\")\n",
        "    line_count = 1\n",
        "    for line in lines:\n",
        "        if line.strip() == \"\":\n",
        "            new_text += \"\\n\"\n",
        "        else:\n",
        "            new_text += f\"{line_count} {line}\\n\"\n",
        "            line_count += 1\n",
        "    return new_text\n",
        "\n",
        "def data2messages( data ):\n",
        "    n_quote = data[\"n_quote\"]\n",
        "    if n_quote >= 20:\n",
        "        task_prompt = long_task_prompt_prefix + \"\\n\" + long_task_prompt_example\n",
        "    else:\n",
        "        task_prompt = long_task_prompt_prefix + \"\\n\" + no_dialogue_prompt_hint + \"\\n\" + long_task_prompt_example\n",
        "    input_text = add_line_num(data[\"text\"])\n",
        "    messages = [\n",
        "        {\"role\":\"system\",\"content\":task_prompt},\n",
        "        {\"role\":\"user\",\"content\":f\"input paragraph:\\n{input_text}\"}\n",
        "    ]\n",
        "    return messages"
      ],
      "metadata": {
        "id": "wgtJmUwmQitK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def getTranslation(item):\n",
        "    if \"messages\" not in item:\n",
        "        return None\n",
        "    else:\n",
        "        for i in range(3):\n",
        "            result = await dealing_messages(item['messages'])\n",
        "            if result is not None:\n",
        "                item[\"response\"] = result\n",
        "                return item\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "    return None\n",
        "\n",
        "async def process(item, semaphore):\n",
        "    async with semaphore:\n",
        "        try:\n",
        "            output_folder = \"/content/output\"\n",
        "            if \"output_folder\" in item:\n",
        "                output_folder = item[\"output_folder\"]\n",
        "            if not os.path.exists(output_folder):\n",
        "                os.makedirs(output_folder)\n",
        "\n",
        "            file_path = os.path.join(output_folder, f\"{item['id']}.txt\")\n",
        "\n",
        "            if os.path.exists(file_path):\n",
        "                return\n",
        "\n",
        "            it = await getTranslation(item)\n",
        "            if it is None:\n",
        "                raise Exception(item['id'])\n",
        "\n",
        "            with open(file_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(it, f, ensure_ascii=False)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing entry: {e}\")"
      ],
      "metadata": {
        "id": "vHXrYc0DVgxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/output"
      ],
      "metadata": {
        "id": "czTeEW4HaWhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(input_datas[0]['text']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jLw0EnAoMhn",
        "outputId": "497910f8-b7ca-4154-96c0-84ceb6e00709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def main(datas):\n",
        "\n",
        "    output_folder = \"/content/output\"\n",
        "\n",
        "    process_data = []\n",
        "\n",
        "    for data in datas:\n",
        "        id = data['id']\n",
        "        process_data.append({\n",
        "            \"id\": id,\n",
        "            \"messages\": data2messages(data),\n",
        "            \"output_folder\": output_folder\n",
        "        })\n",
        "\n",
        "    tasks = []\n",
        "\n",
        "    semaphore = asyncio.Semaphore(concurrency_limit)\n",
        "\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # # print(f\"Already processed {len(exist_list)} items...\")\n",
        "\n",
        "    # id = set()\n",
        "\n",
        "    for item in process_data:\n",
        "        file_path = os.path.join(output_folder, f\"{item['id']}.txt\")\n",
        "\n",
        "        if os.path.exists(file_path):\n",
        "            continue\n",
        "\n",
        "        tasks.append(asyncio.create_task(process(item, semaphore)))\n",
        "\n",
        "    async for task in tqdm(tasks, total=len(tasks), desc=\"Processing items\"):\n",
        "        await task\n",
        "        time.sleep(delay)\n",
        "\n"
      ],
      "metadata": {
        "id": "lApSic3YTC8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/output"
      ],
      "metadata": {
        "id": "jBVnBE-br_WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/CardBuild/dialogue_extract/0122english_*"
      ],
      "metadata": {
        "id": "7g0GkOlib_Hs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "912acc9d-5342-447c-d91a-3cbc64fa7217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CardBuild/dialogue_extract/0122english_0_to_10.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(input_datas))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgvbVsn6ApRw",
        "outputId": "f2c6180c-b425-4450-d039-b0e0282b29c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_id = 0\n",
        "end_id = 10\n",
        "\n",
        "current_tasks = input_datas[start_id:end_id]\n",
        "\n",
        "await main(current_tasks)\n",
        "\n",
        "temp_output_folder = \"/content/output\"\n",
        "\n",
        "for id in range(start_id, end_id):\n",
        "    id_str = input_datas[id][\"id\"]\n",
        "    file_path = os.path.join(temp_output_folder, f\"{id_str}.txt\")\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            try:\n",
        "                data = json.load(f)\n",
        "                response = data[\"response\"]\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        if response is not None:\n",
        "            datas[id][\"response\"] = response\n",
        "    # break\n",
        "\n",
        "final_save_name = \"/content/drive/MyDrive/CardBuild/exp0122/first/0122english_\" + str(start_id) + \"_to_\" + str(end_id) + \".txt\"\n",
        "\n",
        "with open(final_save_name, 'w', encoding='utf-8') as f:\n",
        "    for id in range(start_id, end_id):\n",
        "        json.dump(datas[id], f, ensure_ascii=False)\n",
        "        f.write('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uDvRq6TUCQP",
        "outputId": "b62d9045-d106-4aa1-f04e-009f50f27f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing items: 0it [00:00, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datas = input_datas"
      ],
      "metadata": {
        "id": "VwbjpArQrlGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "super_start = 10\n",
        "super_end = len(datas)\n",
        "\n",
        "n_bag = 50\n",
        "\n",
        "# 计算每个子区间的长度\n",
        "interval_length = (super_end - super_start) // n_bag\n",
        "\n",
        "# 生成元组列表\n",
        "se_tuples_list = [(super_start + i * interval_length, super_start + (i + 1) * interval_length) for i in range(n_bag - 1)]\n",
        "\n",
        "# 添加最后一个元组，确保最后一个end等于super_end\n",
        "se_tuples_list.append((se_tuples_list[-1][1], super_end))\n",
        "\n",
        "# 打印生成的元组列表\n",
        "print(se_tuples_list[0:2])\n",
        "print(se_tuples_list[-2:])\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "random.shuffle(se_tuples_list)\n",
        "\n",
        "print(se_tuples_list[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bVbVGwudXu4",
        "outputId": "04d12dd2-51ba-4964-e692-6d2ae4db0771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(10, 362), (362, 714)]\n",
            "[(16906, 17258), (17258, 17613)]\n",
            "(10218, 10570)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "for start_id, end_id in se_tuples_list:\n",
        "    current_tasks = datas[start_id:end_id]\n",
        "\n",
        "    await main(current_tasks)\n",
        "    await main(current_tasks)\n",
        "\n",
        "    temp_output_folder = \"/content/output\"\n",
        "\n",
        "    for id in range(start_id, end_id):\n",
        "        id_str = input_datas[id][\"id\"]\n",
        "        file_path = os.path.join(temp_output_folder, f\"{id_str}.txt\")\n",
        "        if os.path.exists(file_path):\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                try:\n",
        "                    data = json.load(f)\n",
        "                    response = data[\"response\"]\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if response is not None:\n",
        "                datas[id][\"response\"] = response\n",
        "        # break\n",
        "\n",
        "    final_save_name = \"/content/drive/MyDrive/CardBuild/exp0122/first/0122english_\" + str(start_id) + \"_to_\" + str(end_id) + \".txt\"\n",
        "\n",
        "    with open(final_save_name, 'w', encoding='utf-8') as f:\n",
        "        for id in range(start_id, end_id):\n",
        "            json.dump(datas[id], f, ensure_ascii=False)\n",
        "            f.write('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2QVvvRjdkn4",
        "outputId": "0b97482b-d9e6-4ff4-92e5-c373db69924d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing items: 100%|██████████| 352/352 [07:55<00:00,  1.35s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:48<00:00,  1.50s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:49<00:00,  1.51s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:35<00:00,  1.46s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [09:19<00:00,  1.59s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:12<00:00,  1.40s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:38<00:00,  1.47s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [16:39<00:00,  2.84s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:17<00:00,  1.41s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:17<00:00,  1.41s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [17:29<00:00,  2.98s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:47<00:00,  1.50s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:56<00:00,  1.53s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:25<00:00,  1.43s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:34<00:00,  1.46s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [09:08<00:00,  1.56s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:33<00:00,  1.46s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:07<00:00,  1.38s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [07:57<00:00,  1.36s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [11:24<00:00,  1.94s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:40<00:00,  1.48s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:25<00:00,  1.44s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:15<00:00,  1.41s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [18:13<00:00,  3.11s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:29<00:00,  1.45s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:22<00:00,  1.43s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:33<00:00,  1.46s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:30<00:00,  1.45s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:00<00:00,  1.37s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:40<00:00,  1.48s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:13<00:00,  1.40s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:15<00:00,  1.41s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:43<00:00,  1.49s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [09:04<00:00,  1.55s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:17<00:00,  1.41s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:33<00:00,  1.46s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:25<00:00,  1.43s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:36<00:00,  1.47s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [07:45<00:00,  1.32s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [07:50<00:00,  1.34s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [07:45<00:00,  1.32s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:30<00:00,  1.45s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:14<00:00,  1.40s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:46<00:00,  1.50s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 355/355 [08:31<00:00,  1.44s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [07:46<00:00,  1.33s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [07:23<00:00,  1.26s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:31<00:00,  1.45s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:14<00:00,  1.41s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n",
            "Processing items: 100%|██████████| 352/352 [08:08<00:00,  1.39s/it]\n",
            "Processing items: 0it [00:00, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H7fnHrAritFt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}