{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcK1345rIc422XGKUBCBvf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/Zero-Haruhi/blob/main/notebook/novel_data/%E8%AF%BB%E5%8F%96%E5%B9%B6%E6%89%B9%E9%87%8F%E6%8A%BD%E5%8F%96%E5%8D%A1%E7%89%87.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfwDktxc-ad6",
        "outputId": "33a97c04-97c8-43af-899f-6b8d90188e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "这个pkl在\n",
        "\n",
        "https://drive.google.com/file/d/1LSVaMmKkfuHp1rm3DK0U2zosgZiv34dJ/view?usp=sharing"
      ],
      "metadata": {
        "id": "ZT9ZassQCEgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_name = \"/content/drive/MyDrive/CardBuild/exp0212/card2extract.pkl\" # 冷子昂改为你下载后的目录\n",
        "\n",
        "import pickle\n",
        "\n",
        "with open(input_name, 'rb') as f:\n",
        "    datas = pickle.load(f)\n",
        "\n"
      ],
      "metadata": {
        "id": "zcotVj-h_VIg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jsonl_save_folder = \"/content/drive/MyDrive/CardBuild/exp0212/\" # 冷子昂改为你要保存的目录"
      ],
      "metadata": {
        "id": "VueYjOxYBVWp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "载入openai"
      ],
      "metadata": {
        "id": "qCVtbOgy_izj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtxdCWKp_XvX",
        "outputId": "ab8a682c-0eba-44b5-d084-f9ad7b3255c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/226.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/226.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import httpx\n",
        "\n",
        "# from google.colab import userdata # 冷子昂 不是colab就注释掉\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"]  = \"\"  # 冷子昂改这个key\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"\" # 冷子昂 这个不设置就注释掉\n",
        "\n",
        "import openai\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "model_name = \"gpt-4-0125-preview\" # 冷子昂改为 gpt-4-0125-preview"
      ],
      "metadata": {
        "id": "-253yQjl_lck"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q aiofiles tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1OHBVga_mk4",
        "outputId": "f8f97fc8-aadd-4014-89fd-7a53ae91b620"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.8 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m1.5/1.8 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "# import openai\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
        "    aclient = AsyncOpenAI( api_key = os.getenv(\"OPENAI_API_KEY\" ) )\n",
        "else:\n",
        "    aclient = AsyncOpenAI( base_url = os.getenv(\"OPENAI_API_BASE\"), api_key = os.getenv(\"OPENAI_API_KEY\") )\n",
        "\n",
        "import asyncio\n",
        "import aiofiles\n",
        "import tiktoken\n",
        "import hashlib\n",
        "# from connector import AsyncPGConnector\n",
        "from tqdm.asyncio import tqdm as tqdm\n",
        "\n",
        "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "en2zh_ratio = 2.3\n",
        "\n",
        "delay = 1\n",
        "concurrency_limit = 16\n",
        "\n",
        "max_file_size = 1024**3"
      ],
      "metadata": {
        "id": "Wif-bqqW_ytP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def dealing_messages(messages):\n",
        "    try:\n",
        "        # request_token = sum([len(enc.encode(msg['content'])) for msg in messages])\n",
        "        # response_token = int(len(enc.encode(text)) * en2zh_ratio) + 64\n",
        "\n",
        "        model = model_name\n",
        "\n",
        "        resp = await aclient.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            temperature=0,\n",
        "            response_format={ \"type\": \"json_object\" },\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            result = resp.choices[0].message.content\n",
        "            result = result.strip()\n",
        "            return result\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Invalid json: \", result)\n",
        "            return None\n",
        "        except:\n",
        "            raise Exception(f\"Invalid API response: {resp}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[Error] {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "_2EzquAa_6-G"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def getTranslation(item):\n",
        "    if \"messages\" not in item:\n",
        "        return None\n",
        "    else:\n",
        "        for i in range(3):\n",
        "            result = await dealing_messages(item['messages'])\n",
        "            if result is not None:\n",
        "                item[\"response\"] = result\n",
        "                return item\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "    return None\n",
        "\n",
        "async def process(item, semaphore):\n",
        "    async with semaphore:\n",
        "        try:\n",
        "            output_folder = \"/content/output\"\n",
        "            if \"output_folder\" in item:\n",
        "                output_folder = item[\"output_folder\"]\n",
        "            if not os.path.exists(output_folder):\n",
        "                os.makedirs(output_folder)\n",
        "\n",
        "            file_path = os.path.join(output_folder, f\"{item['id']}.txt\")\n",
        "\n",
        "            if os.path.exists(file_path):\n",
        "                return\n",
        "\n",
        "            it = await getTranslation(item)\n",
        "            if it is None:\n",
        "                raise Exception(item['id'])\n",
        "\n",
        "            with open(file_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(it, f, ensure_ascii=False)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing entry: {e}\")"
      ],
      "metadata": {
        "id": "YbBFEOt9ANQm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(datas[0].keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKfxA-lhAVx2",
        "outputId": "888689ef-4c6a-42e8-d6a0-3d02f2aae162"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['id', 'input', 'response'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def main(datas):\n",
        "\n",
        "    output_folder = \"/content/output\"\n",
        "\n",
        "    process_data = []\n",
        "\n",
        "    for data in datas:\n",
        "        id = data['id']\n",
        "        process_data.append({\n",
        "            \"id\": id,\n",
        "            \"messages\": [{\"role\":\"user\",\"content\":data[\"input\"]}],\n",
        "            \"output_folder\": output_folder\n",
        "        })\n",
        "\n",
        "    tasks = []\n",
        "\n",
        "    semaphore = asyncio.Semaphore(concurrency_limit)\n",
        "\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # # print(f\"Already processed {len(exist_list)} items...\")\n",
        "\n",
        "    # id = set()\n",
        "\n",
        "    for item in process_data:\n",
        "        file_path = os.path.join(output_folder, f\"{item['id']}.txt\")\n",
        "\n",
        "        if os.path.exists(file_path):\n",
        "            continue\n",
        "\n",
        "        tasks.append(asyncio.create_task(process(item, semaphore)))\n",
        "\n",
        "    async for task in tqdm(tasks, total=len(tasks), desc=\"Processing items\"):\n",
        "        await task\n",
        "        time.sleep(delay)"
      ],
      "metadata": {
        "id": "oHUa1dUGAQ5t"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_id = 0\n",
        "end_id = len(datas)\n",
        "\n",
        "current_tasks = datas[start_id:end_id]\n",
        "\n",
        "await main(current_tasks)\n",
        "\n",
        "temp_output_folder = \"/content/output\"\n",
        "\n",
        "for id in range(start_id, end_id):\n",
        "    id_str = datas[id][\"id\"]\n",
        "    file_path = os.path.join(temp_output_folder, f\"{id_str}.txt\")\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            try:\n",
        "                data = json.load(f)\n",
        "                response = data[\"response\"]\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        if response is not None:\n",
        "            datas[id][\"response\"] = response\n",
        "\n",
        "\n",
        "final_save_name = jsonl_save_folder + str(start_id) + \"_to_\" + str(end_id) + \".txt\"\n",
        "\n",
        "with open(final_save_name, 'w', encoding='utf-8') as f:\n",
        "    for id in range(start_id, end_id):\n",
        "        json.dump(datas[id], f, ensure_ascii=False)\n",
        "        f.write('\\n')"
      ],
      "metadata": {
        "id": "pLAyYy_9AkKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(datas[3]['response'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Rbl6UomBDZz",
        "outputId": "c6bf9ba9-130f-4efa-a556-dbd26f0de7fd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"analysis\": {\n",
            "    \"人物设定\": \"霍雨浩是一个具有特殊能力的角色，他拥有极致之冰魂力和魂灵伙伴雪女，同时还有九级炼丹炉和亡灵半位面等神器，是一个非常重要的角色\",\n",
            "    \"人物特点\": \"他聪明、机智，善于利用自己的能力和工具解决问题，同时对自己的伙伴和朋友非常关心\",\n",
            "    \"语言风格\": \"他的语言风格比较成熟稳重，但在面对朋友和伙伴时也会展现出幽默和温暖的一面\"\n",
            "  },\n",
            "  \"most_related_items\": {\n",
            "    \"经历\": \"霍雨浩的经历包括炼丹、使用神器、与魂兽交流等\",\n",
            "    \"身份\": \"他是一个拥有特殊能力和神器的角色，在故事中扮演着重要的角色\",\n",
            "    \"性格特点\": \"他聪明、机智，善于利用自己的能力和工具解决问题，同时对自己的伙伴和朋友非常关心\",\n",
            "    \"经典台词\": \"['第一次遇到', '好奇异啊！', '渐渐感到有些不对了', '开启', '现在让我们开始']\",\n",
            "    \"恋爱状态\": \"对萧炎有特殊好感\"\n",
            "  },\n",
            "  \"claim\": \"我将从dialogue中总结角色的经历, 身份, 性格特点, 经典台词, 恋爱状态信息\",\n",
            "  \"attributes\": {\n",
            "    \"经历\": \"霍雨浩的经历包括炼丹、使用神器、与魂兽交流等\",\n",
            "    \"身份\": \"他是一个拥有特殊能力和神器的角色，在故事中扮演着重要的角色\",\n",
            "    \"性格特点\": \"他聪明、机智，善于利用自己的能力和工具解决问题，同时对自己的伙伴和朋友非常关心\",\n",
            "    \"经典台词\": \"['第一次遇到', '好奇异啊！', '渐渐感到有些不对了', '开启', '现在让我们开始']\",\n",
            "    \"恋爱状态\": \"对萧炎有特殊好感\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4tZEQ4lHDKHt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}