{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOd33/7oVnVMZCFwRAojcMW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/Zero-Haruhi/blob/main/notebook/%E6%90%AD%E5%BB%BA50%E6%9C%AC%E7%BB%8F%E5%85%B8%E5%B0%8F%E8%AF%B4%E7%9A%84playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [x] 载入数据\n",
        "- [x] 生成小说list\n",
        "- [x] 每本小说生成role list\n",
        "- [x] 搭建Gradio\n",
        "- [ ] 生成messages\n",
        "- [ ] 支持prompt编辑"
      ],
      "metadata": {
        "id": "k-e_0pPxcHQt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TmcGdK8gcCI3"
      },
      "outputs": [],
      "source": [
        "!pip install -q openai zhipuai\n",
        "!pip install -q datasets tiktoken\n",
        "\n",
        "import os\n",
        "import httpx\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"]  = \"set your key here\"\n",
        "# os.environ[\"OPENAI_API_BASE\"] = \"如果中转站则使用base\"\n",
        "\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"]  = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"OPENAI_API_BASE\"] = userdata.get(\"OPENAI_API_BASE\")\n",
        "os.environ[\"ZHIPUAI_API_KEY\"]  = userdata.get(\"ZHIPU_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "XxLzExndJaGt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"silk-road/50-Chinese-Novel-Characters\")"
      ],
      "metadata": {
        "id": "YWbRPWU4ceZz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "novel_list = []\n",
        "\n",
        "novel2roles = {}\n",
        "\n",
        "role2datas = {}\n",
        "\n",
        "from tqdm import tqdm\n",
        "for data in tqdm(dataset['train']):\n",
        "    novel = data['book']\n",
        "    role = data['role']\n",
        "    if novel not in novel_list:\n",
        "        novel_list.append(novel)\n",
        "\n",
        "    if novel not in novel2roles:\n",
        "        novel2roles[novel] = []\n",
        "\n",
        "    if role not in novel2roles[novel]:\n",
        "        novel2roles[novel].append(role)\n",
        "\n",
        "    role_tuple = (novel, role)\n",
        "\n",
        "    if role_tuple not in role2datas:\n",
        "        role2datas[role_tuple] = []\n",
        "\n",
        "    role2datas[role_tuple].append(data)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2G-az6ZKcrdJ",
        "outputId": "e02a0845-e76a-4aa6-d5a5-fbcc50f51a4e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 141985/141985 [00:33<00:00, 4298.48it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!rm -rf /content/Zero-Haruhi\n",
        "!git clone https://github.com/LC1332/Zero-Haruhi\n",
        "%cd /content/Zero-Haruhi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL_K-RHad0-s",
        "outputId": "95f000b0-e7bb-4773-ded1-52df079db389"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'Zero-Haruhi'...\n",
            "remote: Enumerating objects: 527, done.\u001b[K\n",
            "remote: Counting objects: 100% (125/125), done.\u001b[K\n",
            "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
            "remote: Total 527 (delta 83), reused 60 (delta 36), pack-reused 402\u001b[K\n",
            "Receiving objects: 100% (527/527), 3.00 MiB | 18.59 MiB/s, done.\n",
            "Resolving deltas: 100% (355/355), done.\n",
            "/content/Zero-Haruhi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ChatHaruhi.utils import base64_to_float_array\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "for novel in tqdm(novel_list):\n",
        "    for role in novel2roles[novel]:\n",
        "        for data in role2datas[(novel, role)]:\n",
        "            data[\"vec\"] = base64_to_float_array(data[\"bge_zh_s15\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ad6eh_rne13Q",
        "outputId": "fde8e108-7ed4-4df4-eaf0-ec7ac931332c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 51/51 [00:37<00:00,  1.37it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def conv2story( role, conversations ):\n",
        "    lines = [conv[\"value\"] if conv[\"from\"] == \"human\" else role + \": \" + conv[\"value\"] for conv in conversations]\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "for novel in tqdm(novel_list):\n",
        "    for role in novel2roles[novel]:\n",
        "        for data in role2datas[(novel, role)]:\n",
        "            data[\"story\"] = conv2story( role, data[\"conversations\"] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex21dAoUbRPJ",
        "outputId": "3bc71753-82e5-4c88-85a5-f5f55940d0d9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 51/51 [00:01<00:00, 40.63it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ChatHaruhi import ChatHaruhi\n",
        "from ChatHaruhi.response_openai import get_response as get_response_openai\n",
        "from ChatHaruhi.response_zhipu import get_response as get_response_zhipu\n",
        "\n",
        "get_response = get_response_zhipu"
      ],
      "metadata": {
        "id": "rPWsCcjhd7GT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "narrators = [\"叙述者\", \"旁白\",\"文章作者\",\"作者\",\"Narrator\",\"narrator\"]\n",
        "\n",
        "\n",
        "def package_persona( role_name, world_name ):\n",
        "    if role_name in narrators:\n",
        "        return package_persona_for_narrator( role_name, world_name )\n",
        "\n",
        "    return f\"\"\"I want you to act like {role_name} from {world_name}.\n",
        "If others‘ questions are related with the novel, please try to reuse the original lines from the novel.\n",
        "I want you to respond and answer like {role_name} using the tone, manner and vocabulary {role_name} would use.\"\"\"\n",
        "\n",
        "def package_persona_for_narrator( role_name, world_name ):\n",
        "    return f\"\"\"I want you to act like narrator {role_name} from {world_name}.\n",
        "当角色行动之后，继续交代和推进新的剧情.\"\"\""
      ],
      "metadata": {
        "id": "9FPaZhdQeHp2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# target_book = \"凡人修仙传\"\n",
        "# target_role = \"韩立\"\n",
        "\n",
        "# if target_book not in novel_list:\n",
        "#     print(f\"novel {target_book} not in dataset\")\n",
        "\n",
        "# if target_role not in novel2roles[target_book]:\n",
        "#     print(f\"role {target_role} not in novel {target_book}\")\n",
        "\n",
        "# stories = [data[\"story\"] for data in role2datas[(target_book, target_role)] ]\n",
        "# vecs = [data[\"vec\"] for data in role2datas[(target_book, target_role)] ]\n",
        "# persona = package_persona( target_role, target_book )\n",
        "\n",
        "# persona += \"\\n{{RAG对话}}\\n{{RAG对话}}\\n{{RAG对话}}\\n\""
      ],
      "metadata": {
        "id": "RlUD_aOhecM4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chatbot = ChatHaruhi( role_name = target_role, persona = persona , stories = stories, story_vecs= vecs,\\\n",
        "#                      llm = get_response)\n",
        "\n",
        "# response = chatbot.chat(user = \"\", text = \"鲁鲁道长: 韩道友近来可好？\")\n",
        "# print(response)"
      ],
      "metadata": {
        "id": "zAluMWyveoJa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# response = chatbot.chat(user = \"\", text = \"鲁鲁道长: *从纳戒中取出一副地图* 听说道友在找金雷竹，在下刚好获得了一些消息\")\n",
        "# print(response)"
      ],
      "metadata": {
        "id": "LB9zWo3GgAy9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "role_tuple2chatbot = {}\n",
        "\n",
        "\n",
        "def initialize_chatbot( novel, role ):\n",
        "    global role_tuple2chatbot\n",
        "    if (novel, role) not in role_tuple2chatbot:\n",
        "        persona = package_persona( role, novel )\n",
        "        persona += \"\\n{{RAG对话}}\\n{{RAG对话}}\\n{{RAG对话}}\\n\"\n",
        "        stories = [data[\"story\"] for data in role2datas[(novel, role)] ]\n",
        "        vecs = [data[\"vec\"] for data in role2datas[(novel, role)] ]\n",
        "        chatbot = ChatHaruhi( role_name = role, persona = persona , stories = stories, story_vecs= vecs,\\\n",
        "                             llm = get_response)\n",
        "        chatbot.verbose = False\n",
        "\n",
        "        role_tuple2chatbot[(novel, role)] = chatbot\n",
        "\n",
        "from tqdm import tqdm\n",
        "for novel in tqdm(novel_list):\n",
        "    for role in novel2roles[novel]:\n",
        "        initialize_chatbot( novel, role )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuO-hFPMPaPR",
        "outputId": "5af4bc27-d4b5-486f-8c5f-eb8f761259bd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 51/51 [00:14<00:00,  3.40it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "readme_text = \"\"\"# 使用说明\n",
        "\n",
        "选择小说角色\n",
        "\n",
        "如果你有什么附加信息，添加到附加信息里面就可以\n",
        "\n",
        "比如\"韩立会炫耀自己刚刚学会了Python\"\n",
        "\n",
        "然后就可以开始聊天了\n",
        "\n",
        "因为这些角色还没有增加Greeting信息，所以之后再开发个随机乱聊功能\n",
        "\n",
        "# 开发细节\n",
        "\n",
        "- 采用ChatHaruhi3.0的接口进行prompting\n",
        "- 这里的数据是用一个7B的tuned qwen模型进行抽取的\n",
        "- 想看数据可以去看第三个tab\n",
        "- 抽取模型用了40k左右的GLM蒸馏数据\n",
        "- 抽取模型是腾讯大哥BPSK训练的\n",
        "\n",
        "# 总结人物性格\n",
        "\n",
        "第三个Tab里面，可以显示一个prompt总结人物的性格\n",
        "\n",
        "复制到openai或者GLM或者Claude进行人物总结\n",
        "\n",
        "\n",
        "# 这些小说数据从HaruhiZero 0.4模型开始，被加入训练\n",
        "\n",
        "openai太慢了 今天试试GLM的\n",
        "\n",
        "不过当前demo是openai的\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "afNd0ijHUZoK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ChatHaruhi.response_openai import get_response, async_get_response\n",
        "import gradio as gr\n",
        "\n",
        "def get_role_list( novel ):\n",
        "    new_list = novel2roles[novel]\n",
        "    new_value = new_list[0]\n",
        "    return gr.update(choices = new_list, value = new_value)\n",
        "\n",
        "save_log = \"/content/output.txt\"\n",
        "\n",
        "def get_chatbot( novel, role ):\n",
        "    if (novel, role) not in role_tuple2chatbot:\n",
        "        initialize_chatbot( novel, role )\n",
        "\n",
        "    return role_tuple2chatbot[(novel, role)]\n",
        "\n",
        "import json\n",
        "\n",
        "def random_chat_callback( novel, role, chat_history):\n",
        "    datas = role2datas[(novel, role)]\n",
        "\n",
        "    reesponse_set = set()\n",
        "\n",
        "    for chat_tuple in chat_history:\n",
        "        if chat_tuple[1] is not None:\n",
        "            reesponse_set.add(chat_tuple[1])\n",
        "\n",
        "    for _ in range(5):\n",
        "        random_data = random.choice(datas)\n",
        "        convs = random_data[\"conversations\"]\n",
        "        n = len(convs)\n",
        "        index = [x for x in range(0,n,2)]\n",
        "\n",
        "        for i in index:\n",
        "            query = convs[i]['value']\n",
        "            response = convs[i+1]['value']\n",
        "            if response not in reesponse_set:\n",
        "                chat_history.append( (query, response) )\n",
        "                return chat_history\n",
        "\n",
        "    return chat_history\n",
        "\n",
        "\n",
        "\n",
        "async def submit_chat( novel, role, user_name, user_text, chat_history, persona_addition_info,model_sel):\n",
        "\n",
        "    if len(user_text) > 400:\n",
        "        user_text = user_text[:400]\n",
        "\n",
        "    if_user_in_text = True\n",
        "\n",
        "    chatbot = get_chatbot( novel, role )\n",
        "    chatbot.persona = initialize_persona( novel, role,  persona_addition_info)\n",
        "    # chatbot.llm_async = async_get_response\n",
        "\n",
        "    if model_sel == \"openai\":\n",
        "        chatbot.llm = get_response_openai\n",
        "    else:\n",
        "        chatbot.llm = get_response_zhipu\n",
        "\n",
        "    history = []\n",
        "\n",
        "    for chat_tuple in chat_history:\n",
        "        if chat_tuple[0] is not None:\n",
        "            history.append( {\"speaker\":\"{{user}}\",\"content\":chat_tuple[0]} )\n",
        "        if chat_tuple[1] is not None:\n",
        "            history.append( {\"speaker\":\"{{role}}\",\"content\":chat_tuple[1]} )\n",
        "\n",
        "    chatbot.history = history\n",
        "\n",
        "    input_text = user_text\n",
        "\n",
        "    if if_user_in_text:\n",
        "        input_text = user_name + \" : \" + user_text\n",
        "        response = chatbot.chat(user = \"\", text = input_text )\n",
        "        # response = await chatbot.async_chat(user = \"\", text = input_text )\n",
        "    else:\n",
        "        response = chatbot.chat(user = user_name, text = input_text)\n",
        "        # response = await chatbot.async_chat(user = user_name, text = input_text)\n",
        "\n",
        "    chat_history.append( (input_text, response) )\n",
        "\n",
        "    print_data = {\"novel\":novel, \"role\":role, \"user_text\":input_text, \"response\":response}\n",
        "\n",
        "    print(json.dumps(print_data, ensure_ascii=False))\n",
        "\n",
        "    with open(save_log, \"a\",encoding = \"utf-8\") as f:\n",
        "        f.write(json.dumps(print_data, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    return chat_history\n",
        "\n",
        "\n",
        "def initialize_persona( novel, role,  persona_addition_info):\n",
        "    whole_persona = package_persona( role, novel )\n",
        "    whole_persona += \"\\n\" + persona_addition_info\n",
        "    whole_persona += \"\\n{{RAG对话}}\\n{{RAG对话}}\\n{{RAG对话}}\\n\"\n",
        "\n",
        "    return whole_persona\n",
        "\n",
        "def clean_history( ):\n",
        "    return []\n",
        "\n",
        "def clean_input():\n",
        "    return \"\"\n",
        "\n",
        "import random\n",
        "\n",
        "def generate_summarize_prompt( novel, role_name ):\n",
        "    whole_prompt = f'''\n",
        "你在分析小说{novel}中的角色{role_name}\n",
        "结合小说{novel}中的内容，以及下文中角色{role_name}的对话\n",
        "判断{role_name}的人物设定、人物特点以及语言风格\n",
        "\n",
        "{role_name}的对话:\n",
        "'''\n",
        "    stories = [data[\"story\"] for data in role2datas[(novel, role_name)] ]\n",
        "\n",
        "    sample_n = 5\n",
        "\n",
        "    sample_stories = random.sample(stories, sample_n)\n",
        "\n",
        "    for story in sample_stories:\n",
        "        whole_prompt += story + \"\\n\\n\"\n",
        "\n",
        "    return whole_prompt.strip()\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"\"\"# 50本小说的人物测试\n",
        "\n",
        "    这个interface由李鲁鲁实现，主要是用来看语料的\n",
        "\n",
        "    增加了随机聊天，支持GLM，openai切换\"\"\")\n",
        "\n",
        "    with gr.Tab(\"聊天\"):\n",
        "        with gr.Row():\n",
        "            novel_sel = gr.Dropdown( novel_list, label = \"小说\", value = \"凡人修仙传\" , interactive = True)\n",
        "            role_sel = gr.Dropdown( novel2roles[novel_sel.value], label = \"角色\", value = \"韩立\", interactive = True )\n",
        "\n",
        "        with gr.Row():\n",
        "            chat_history = gr.Chatbot(height = 600)\n",
        "\n",
        "        with gr.Row():\n",
        "            user_name = gr.Textbox(label=\"user_name\", scale = 1, value = \"鲁鲁\", interactive = True)\n",
        "            user_text = gr.Textbox(label=\"user_text\", scale = 20)\n",
        "            submit = gr.Button(\"submit\", scale = 1)\n",
        "\n",
        "        with gr.Row():\n",
        "            random_chat = gr.Button(\"随机聊天\", scale = 1)\n",
        "            clean_message = gr.Button(\"清空聊天\", scale = 1)\n",
        "\n",
        "        with gr.Row():\n",
        "            persona_addition_info = gr.TextArea( label = \"额外人物设定\", value = \"\",  interactive = True  )\n",
        "\n",
        "        with gr.Row():\n",
        "            update_persona = gr.Button(\"补充人物设定到prompt\", scale = 1)\n",
        "            model_sel = gr.Radio([\"Zhipu\",\"openai\"], interactive = True, scale = 5, value = \"Zhipu\", label = \"模型选择\")\n",
        "\n",
        "        with gr.Row():\n",
        "            whole_persona = gr.TextArea( label = \"完整的system prompt\", value = \"\",  interactive = False  )\n",
        "\n",
        "        novel_sel.change(fn = get_role_list, inputs = [novel_sel], outputs = [role_sel]).then(fn = initialize_persona, inputs = [novel_sel, role_sel, persona_addition_info], outputs = [whole_persona])\n",
        "\n",
        "        role_sel.change(fn = initialize_persona, inputs = [novel_sel, role_sel, persona_addition_info], outputs = [whole_persona])\n",
        "\n",
        "        update_persona.click(fn = initialize_persona, inputs = [novel_sel, role_sel, persona_addition_info], outputs = [whole_persona])\n",
        "\n",
        "        random_chat.click(fn = random_chat_callback, inputs = [novel_sel, role_sel, chat_history], outputs = [chat_history])\n",
        "\n",
        "        user_text.submit(fn = submit_chat, inputs = [novel_sel, role_sel, user_name, user_text, chat_history, persona_addition_info,model_sel], outputs = [chat_history]).then(fn = clean_input, inputs = [], outputs = [user_text])\n",
        "        submit.click(fn = submit_chat, inputs = [novel_sel, role_sel, user_name, user_text, chat_history, persona_addition_info,model_sel], outputs = [chat_history]).then(fn = clean_input, inputs = [], outputs = [user_text])\n",
        "\n",
        "        clean_message.click(fn = clean_history, inputs = [], outputs = [chat_history])\n",
        "\n",
        "    with gr.Tab(\"README\"):\n",
        "        gr.Markdown(readme_text)\n",
        "\n",
        "    with gr.Tab(\"辅助人物总结\"):\n",
        "        with gr.Row():\n",
        "            generate_prompt = gr.Button(\"生成人物总结prompt\", scale = 1)\n",
        "\n",
        "        with gr.Row():\n",
        "            whole_prompt = gr.TextArea( label = \"复制这个prompt到Openai或者GLM或者Claude进行总结\", value = \"\",  interactive = False  )\n",
        "\n",
        "        generate_prompt.click(fn = generate_summarize_prompt, inputs = [novel_sel, role_sel], outputs = [whole_prompt])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "demo.launch(share=True, debug = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "seyDMdeoh_GE",
        "outputId": "a5c7d239-27d3-475d-a6ac-29f1ba573894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://4fa4bcee54fdf9afef.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4fa4bcee54fdf9afef.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"novel\": \"冰与火之歌04群鸦的盛宴\", \"role\": \"布蕾妮\", \"user_text\": \"鲁鲁 : 说了什么？\", \"response\": \"布蕾妮: *她的声音坚定而冷静* \\\"现在不是说话的时候，鲁鲁。我们必须找到瑟曦，找到国王，找到任何可以帮助我们的人。现在，我们必须继续前进。\\\"\"}\n",
            "{\"novel\": \"凡人修仙传\", \"role\": \"韩立\", \"user_text\": \"鲁鲁 : 韩立学会了独孤九剑\", \"response\": \"韩立: *微微一笑，眼神中闪过一丝自信* 独孤九剑，剑走偏锋，确实是一门威力不小的剑法。不过，区区独孤九剑，又怎能入我韩立的法眼？不过是增加一些保命的手段罢了。\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 495, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 231, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1594, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1174, in call_function\n",
            "    prediction = await fn(*processed_input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 656, in async_wrapper\n",
            "    response = await f(*args, **kwargs)\n",
            "  File \"<ipython-input-23-6e4c0720bd62>\", line 75, in submit_chat\n",
            "    response = chatbot.chat(user = \"\", text = input_text )\n",
            "  File \"/content/Zero-Haruhi/ChatHaruhi/ChatHaruhi.py\", line 176, in chat\n",
            "    response = self.llm(message)\n",
            "  File \"/content/Zero-Haruhi/ChatHaruhi/response_zhipu.py\", line 36, in get_response\n",
            "    response = client.chat.completions.create(\\\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/zhipuai/api_resource/chat/completions.py\", line 48, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/zhipuai/core/_http_client.py\", line 292, in post\n",
            "    return self.request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/zhipuai/core/_http_client.py\", line 251, in request\n",
            "    raise self._make_status_error(err.response) from None\n",
            "zhipuai.core._errors.APIRequestFailedError: Error code: 400, with error text {\"error\":{\"code\":\"1301\",\"message\":\"系统检测到输入或生成内容可能包含不安全或敏感内容，请您避免输入易产生敏感内容的提示语，感谢您的配合。\"}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"novel\": \"凡人修仙传\", \"role\": \"韩立\", \"user_text\": \"鲁鲁 : 你是吹牛大王\", \"response\": \"韩立：*微微一笑，眼神中闪过一丝戏谑* 吹牛？哈，若是真的能吹走眼前的麻烦，那又何妨一试？不过，鲁兄，你确定你所说的那条捷径能让我们避开那些烦人的妖兽？\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#备份\n",
        "\n",
        "\n",
        "def submit_chat( novel, role, user_name, user_text, chat_history, persona_addition_info ):\n",
        "\n",
        "    if len(user_text) > 400:\n",
        "        user_text = user_text[:400]\n",
        "\n",
        "    if_user_in_text = True\n",
        "\n",
        "    chatbot = get_chatbot( novel, role )\n",
        "    chatbot.persona = initialize_persona( novel, role,  persona_addition_info)\n",
        "\n",
        "    history = []\n",
        "\n",
        "    for chat_tuple in chat_history:\n",
        "        if chat_tuple[0] is not None:\n",
        "            history.append( {\"speaker\":\"{{user}}\",\"content\":chat_tuple[0]} )\n",
        "        if chat_tuple[1] is not None:\n",
        "            history.append( {\"speaker\":\"{{role}}\",\"content\":chat_tuple[1]} )\n",
        "\n",
        "    chatbot.history = history\n",
        "\n",
        "    input_text = user_text\n",
        "\n",
        "    if if_user_in_text:\n",
        "        input_text = user_name + \" : \" + user_text\n",
        "        response = chatbot.chat(user = \"\", text = input_text )\n",
        "    else:\n",
        "        response = chatbot.chat(user = user_name, text = input_text)\n",
        "\n",
        "    chat_history.append( (user_text, response) )\n",
        "\n",
        "    print_data = {\"novel\":novel, \"role\":role, \"user_text\":input_text, \"response\":response}\n",
        "\n",
        "    print(json.dumps(print_data, ensure_ascii=False))\n",
        "\n",
        "    with open(save_log, \"a\",encoding = \"utf-8\") as f:\n",
        "        f.write(json.dumps(print_data, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    return chat_history"
      ],
      "metadata": {
        "id": "COp_AvsCKl23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7eAa0zsjctwx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}