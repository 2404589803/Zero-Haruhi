{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMeUVP97Cq+4bn8djIku6PP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2573a37e77a54467a1cc88f765b213c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c4762d1d65d2470cbd6e660d6de369b1",
              "IPY_MODEL_97684bde80c94463ae084d11d2ef9161",
              "IPY_MODEL_d51b7b706e494175b7b1ce7006eef5a5"
            ],
            "layout": "IPY_MODEL_7988d81370b64fa796b4890fa4a44158"
          }
        },
        "c4762d1d65d2470cbd6e660d6de369b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86a0f6c7f49a44f2b513d3955fc97513",
            "placeholder": "​",
            "style": "IPY_MODEL_2e5e1433ab374868ab4ff1d8054aa7d3",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "97684bde80c94463ae084d11d2ef9161": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97c9bea3dfb948dba0cab425c2b794dc",
            "max": 173,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_01cfb0bee81542e1809c9b067aa04e05",
            "value": 173
          }
        },
        "d51b7b706e494175b7b1ce7006eef5a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d74c22e16c949f29b32be7ffb11ca19",
            "placeholder": "​",
            "style": "IPY_MODEL_ab462863809e40c99f5115a616b88736",
            "value": " 173/173 [00:00&lt;00:00, 7.43kB/s]"
          }
        },
        "7988d81370b64fa796b4890fa4a44158": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86a0f6c7f49a44f2b513d3955fc97513": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e5e1433ab374868ab4ff1d8054aa7d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97c9bea3dfb948dba0cab425c2b794dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01cfb0bee81542e1809c9b067aa04e05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3d74c22e16c949f29b32be7ffb11ca19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab462863809e40c99f5115a616b88736": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "390bd7bbb35b4bc2ad7756a7c3b842cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b00e52f2d2044e5b5efb1d5a248e92a",
              "IPY_MODEL_b3eb3149b2a34120a3edb37113b713a7",
              "IPY_MODEL_ea5e0ca1a2b646139002c3b3479dac0c"
            ],
            "layout": "IPY_MODEL_0e5878679ac446c8babef1027e3560ab"
          }
        },
        "8b00e52f2d2044e5b5efb1d5a248e92a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bf984edf0cb4b1f825ee1a03c06b3ff",
            "placeholder": "​",
            "style": "IPY_MODEL_f1a233ced7f849d48b7ce50c6269284f",
            "value": "tokenization_qwen.py: 100%"
          }
        },
        "b3eb3149b2a34120a3edb37113b713a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e43c03cb022840c3a4518a101f467320",
            "max": 9618,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de89b93932c34b3e9d3a2e7ab9e58dd0",
            "value": 9618
          }
        },
        "ea5e0ca1a2b646139002c3b3479dac0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47363f52362149ddb29d81c4d4719005",
            "placeholder": "​",
            "style": "IPY_MODEL_d932d2e6d1dc43969e0c70947c6bcf88",
            "value": " 9.62k/9.62k [00:00&lt;00:00, 438kB/s]"
          }
        },
        "0e5878679ac446c8babef1027e3560ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bf984edf0cb4b1f825ee1a03c06b3ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1a233ced7f849d48b7ce50c6269284f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e43c03cb022840c3a4518a101f467320": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de89b93932c34b3e9d3a2e7ab9e58dd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "47363f52362149ddb29d81c4d4719005": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d932d2e6d1dc43969e0c70947c6bcf88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5fef0dd8bf634142a5b389e56ef76ecd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4136f75a53d14419bc10447c855c07f4",
              "IPY_MODEL_7608d3708c60446ca9182b8920e5cdb1",
              "IPY_MODEL_dd53fb72c4bb419a97a9081014837a09"
            ],
            "layout": "IPY_MODEL_5668b84bdf32460d827091748f004df5"
          }
        },
        "4136f75a53d14419bc10447c855c07f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0c972a90c5d41888248988806779974",
            "placeholder": "​",
            "style": "IPY_MODEL_c70bba6a6bf44c1495565a442f73f88d",
            "value": "qwen.tiktoken: 100%"
          }
        },
        "7608d3708c60446ca9182b8920e5cdb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_908ba57b9c754ff5925a866ac2cefe50",
            "max": 2561218,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_301ecde9e9604239b6226c1a7fada4db",
            "value": 2561218
          }
        },
        "dd53fb72c4bb419a97a9081014837a09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd30c676d6564cda8aa28fbaf34926cb",
            "placeholder": "​",
            "style": "IPY_MODEL_9e01f271732f4ef38e986d4f6fc6278a",
            "value": " 2.56M/2.56M [00:00&lt;00:00, 5.18MB/s]"
          }
        },
        "5668b84bdf32460d827091748f004df5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0c972a90c5d41888248988806779974": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c70bba6a6bf44c1495565a442f73f88d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "908ba57b9c754ff5925a866ac2cefe50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "301ecde9e9604239b6226c1a7fada4db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd30c676d6564cda8aa28fbaf34926cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e01f271732f4ef38e986d4f6fc6278a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/Zero-Haruhi/blob/main/notebook/HaruhiZero%E5%AF%B9%E6%8E%A5Qwen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [ ] cutlen 2048的适配（以qwen的tokenizer为标准）\n",
        "- [ ] 超长message适当切分\n",
        "- [ ] 看一看source字段对llama factory有没有影响"
      ],
      "metadata": {
        "id": "pz2eB96HPQ2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN4GoP3CPnCy",
        "outputId": "d9afa0d0-d35c-4b2e-e55a-bd84adfc668f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/CardBuild/HaruhiZero/merge_all_105k.jsonl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmJsCJBsPryH",
        "outputId": "640a8fdb-42ff-46bd-a92b-09ce8327614d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CardBuild/HaruhiZero/merge_all_105k.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_name = \"/content/drive/MyDrive/CardBuild/HaruhiZero/merge_all_105k.jsonl\"\n",
        "datas = []\n",
        "import json\n",
        "with open(input_name, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        datas.append(data)"
      ],
      "metadata": {
        "id": "zjt2Sq4BP4WA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "检查一下source"
      ],
      "metadata": {
        "id": "39DK8AqKQD5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_source = set()\n",
        "\n",
        "for data in datas:\n",
        "    all_source.add(data[\"source\"])\n",
        "\n",
        "print(all_source)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eutHw4OP-r3",
        "outputId": "838ce218-a445-4ad0-a855-0f45042588c8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'PIPPA', 'Haruhi52K', 'Haruhi_Other_Chinese', 'Claude_Baize', 'Janitor', 'RoleLLM', 'Haruhi_Waifu_extended', 'Direct_Role_Play_Claude'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oOlInW7QkOU",
        "outputId": "1c38ebc1-22c8-46a0-c681-4e77db83eb00"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Note: The default behavior now has injection attack prevention off.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-1_8B-Chat\", trust_remote_code=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167,
          "referenced_widgets": [
            "2573a37e77a54467a1cc88f765b213c9",
            "c4762d1d65d2470cbd6e660d6de369b1",
            "97684bde80c94463ae084d11d2ef9161",
            "d51b7b706e494175b7b1ce7006eef5a5",
            "7988d81370b64fa796b4890fa4a44158",
            "86a0f6c7f49a44f2b513d3955fc97513",
            "2e5e1433ab374868ab4ff1d8054aa7d3",
            "97c9bea3dfb948dba0cab425c2b794dc",
            "01cfb0bee81542e1809c9b067aa04e05",
            "3d74c22e16c949f29b32be7ffb11ca19",
            "ab462863809e40c99f5115a616b88736",
            "390bd7bbb35b4bc2ad7756a7c3b842cb",
            "8b00e52f2d2044e5b5efb1d5a248e92a",
            "b3eb3149b2a34120a3edb37113b713a7",
            "ea5e0ca1a2b646139002c3b3479dac0c",
            "0e5878679ac446c8babef1027e3560ab",
            "3bf984edf0cb4b1f825ee1a03c06b3ff",
            "f1a233ced7f849d48b7ce50c6269284f",
            "e43c03cb022840c3a4518a101f467320",
            "de89b93932c34b3e9d3a2e7ab9e58dd0",
            "47363f52362149ddb29d81c4d4719005",
            "d932d2e6d1dc43969e0c70947c6bcf88",
            "5fef0dd8bf634142a5b389e56ef76ecd",
            "4136f75a53d14419bc10447c855c07f4",
            "7608d3708c60446ca9182b8920e5cdb1",
            "dd53fb72c4bb419a97a9081014837a09",
            "5668b84bdf32460d827091748f004df5",
            "e0c972a90c5d41888248988806779974",
            "c70bba6a6bf44c1495565a442f73f88d",
            "908ba57b9c754ff5925a866ac2cefe50",
            "301ecde9e9604239b6226c1a7fada4db",
            "fd30c676d6564cda8aa28fbaf34926cb",
            "9e01f271732f4ef38e986d4f6fc6278a"
          ]
        },
        "id": "YWqNk3I9Qc-s",
        "outputId": "c77f5c6c-d2fb-40bd-e811-a424d6fa4698"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2573a37e77a54467a1cc88f765b213c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenization_qwen.py:   0%|          | 0.00/9.62k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "390bd7bbb35b4bc2ad7756a7c3b842cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-1_8B-Chat:\n",
            "- tokenization_qwen.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "qwen.tiktoken:   0%|          | 0.00/2.56M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fef0dd8bf634142a5b389e56ef76ecd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"一句句子。\"\n",
        "\n",
        "def tokenizer_len( text ):\n",
        "    return len(tokenizer(text)[\"input_ids\"])\n",
        "print(tokenizer_len(sample_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2os-4MwCR22O",
        "outputId": "194712fd-33e7-48b4-cefb-6e6e7cd3ee7a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sum_token( data ):\n",
        "    if \"system\" in data:\n",
        "        sum_token = tokenizer_len( data[\"system\"] )\n",
        "    else:\n",
        "        sum_token = 0\n",
        "    for conversation in data[\"conversations\"]:\n",
        "        sum_token += tokenizer_len( conversation[\"value\"] )\n",
        "        if sum_token > 4096:\n",
        "            return sum_token\n",
        "    return sum_token\n",
        "\n",
        "print(get_sum_token(datas[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5SlQuC2Vf5b",
        "outputId": "beb7db85-3b21-4cf1-a0a4-03cfdb1a4351"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cutted_source = ['Direct_Role_Play_Claude', 'PIPPA', 'Janitor', 'Claude_Baize']\n",
        "\n",
        "new_datas = []\n",
        "\n",
        "max_len = 1024*3\n",
        "\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "error_count = 0\n",
        "count = 0\n",
        "for id,data in tqdm(enumerate(datas)):\n",
        "    if data[\"source\"] not in cutted_source:\n",
        "        new_datas.append(data)\n",
        "        continue\n",
        "    if get_sum_token(data) < max_len:\n",
        "        new_datas.append(data)\n",
        "        continue\n",
        "\n",
        "    if id % 1000 == 0:\n",
        "        print(\"deal \", id, \" len save = \", len(new_datas))\n",
        "\n",
        "    if data[\"system\"].strip() != \"\":\n",
        "        system_prompt = data[\"system\"]\n",
        "        sys_len = tokenizer_len( data[\"system\"] )\n",
        "        msg_len = sys_len\n",
        "\n",
        "        current_conversations = []\n",
        "\n",
        "        conversations = data[\"conversations\"]\n",
        "\n",
        "        conv_len = len(conversations)\n",
        "\n",
        "        break_id = -1\n",
        "\n",
        "        for i in range(0, conv_len, 2):\n",
        "            current_conversations += conversations[i:i+2]\n",
        "            current_qa_len = tokenizer_len( conversations[i][\"value\"] ) + tokenizer_len( conversations[i+1][\"value\"] )\n",
        "\n",
        "            msg_len += current_qa_len\n",
        "            if msg_len > max_len:\n",
        "                break_id = i\n",
        "                break\n",
        "\n",
        "        save_conversations = copy.deepcopy(current_conversations)\n",
        "        new_data = { \"system\": system_prompt,\n",
        "            \"conversations\": save_conversations,\n",
        "            \"source\": data[\"source\"]\n",
        "        }\n",
        "        new_datas.append(new_data)\n",
        "\n",
        "        if break_id > 0 and break_id < conv_len:\n",
        "            rest_conversation = conversations[break_id+2:]\n",
        "\n",
        "            #这里给一次机会，如果rest_conversation + sys_len < max_len 则把system加上\n",
        "            rest_token = get_sum_token({\"conversations\": rest_conversation})\n",
        "            if rest_token + sys_len < max_len:\n",
        "                new_data = { \"system\": system_prompt,\n",
        "                    \"conversations\": rest_conversation,\n",
        "                    \"source\": data[\"source\"]\n",
        "                }\n",
        "                new_datas.append(new_data)\n",
        "                continue\n",
        "            else:\n",
        "                convs = []\n",
        "                current_len = 0\n",
        "                for i in range(0, len(rest_conversation),2):\n",
        "                    convs += rest_conversation[i:i+2]\n",
        "                    current_qa_len = tokenizer_len( rest_conversation[i][\"value\"] ) + tokenizer_len( rest_conversation[i+1][\"value\"] )\n",
        "                    current_len += current_qa_len\n",
        "                    if current_len > max_len:\n",
        "                        save_conversations = copy.deepcopy(convs)\n",
        "                        new_data = {\n",
        "                            \"system\": \"\",\n",
        "                            \"conversations\": save_conversations,\n",
        "                            \"source\": data[\"source\"]\n",
        "                        }\n",
        "                        new_datas.append(new_data)\n",
        "                        convs = []\n",
        "                        current_len = 0\n",
        "                if len(convs) > 0:\n",
        "                    save_conversations = copy.deepcopy(convs)\n",
        "                    new_data = {\n",
        "                        \"system\": \"\",\n",
        "                        \"conversations\": save_conversations,\n",
        "                        \"source\": data[\"source\"]\n",
        "                    }\n",
        "                    new_datas.append(new_data)\n",
        "                continue\n",
        "\n",
        "    else:\n",
        "        #不然的话system为0\n",
        "        conversations = data[\"conversations\"]\n",
        "        head_len = tokenizer_len( conversations[0][\"value\"] ) + tokenizer_len( conversations[1][\"value\"] )\n",
        "\n",
        "        if head_len > max_len*0.5:\n",
        "            new_datas.append(data)\n",
        "            continue\n",
        "\n",
        "        current_conversations = copy.deepcopy( conversations[0:2] )\n",
        "        msg_len = head_len\n",
        "\n",
        "        break_id = -1\n",
        "\n",
        "        for i in range(2, len(conversations), 2):\n",
        "            current_conversations += conversations[i:i+2]\n",
        "            current_qa_len = tokenizer_len( conversations[i][\"value\"] ) + tokenizer_len( conversations[i+1][\"value\"] )\n",
        "            msg_len += current_qa_len\n",
        "            if msg_len > max_len:\n",
        "                save_conversations = copy.deepcopy(current_conversations)\n",
        "                new_data = {\n",
        "                    \"system\": \"\",\n",
        "                    \"conversations\": save_conversations,\n",
        "                    \"source\": data[\"source\"]\n",
        "                }\n",
        "                new_datas.append(new_data)\n",
        "                break_id = i\n",
        "                break\n",
        "\n",
        "        if break_id > 0 and break_id < len(conversations):\n",
        "            rest_conversation = conversations[break_id+2:]\n",
        "            rest_token = get_sum_token({\"conversations\": rest_conversation})\n",
        "            if rest_token + head_len < max_len:\n",
        "                save_conversations = copy.deepcopy(rest_conversation)\n",
        "                save_conversations = conversations[0:2] + rest_conversation\n",
        "                new_data = {\n",
        "                    \"system\": \"\",\n",
        "                    \"conversations\": save_conversations,\n",
        "                    \"source\": data[\"source\"]\n",
        "                }\n",
        "                new_datas.append(new_data)\n",
        "                continue\n",
        "            else:\n",
        "                convs = []\n",
        "                current_len = 0\n",
        "                for i in range(0, len(rest_conversation),2):\n",
        "                    convs += rest_conversation[i:i+2]\n",
        "                    current_qa_len = tokenizer_len( rest_conversation[i][\"value\"] ) + tokenizer_len( rest_conversation[i+1][\"value\"] )\n",
        "                    current_len += current_qa_len\n",
        "                    if current_len > max_len:\n",
        "                        save_conversations = copy.deepcopy(convs)\n",
        "                        new_data = {\n",
        "                            \"system\": \"\",\n",
        "                            \"conversations\": save_conversations,\n",
        "                            \"source\": data[\"source\"]\n",
        "                        }\n",
        "                        new_datas.append(new_data)\n",
        "                        convs = []\n",
        "                        current_len = 0\n",
        "                if len(convs) > 0:\n",
        "                    save_conversations = copy.deepcopy(convs)\n",
        "                    new_data = {\n",
        "                        \"system\": \"\",\n",
        "                        \"conversations\": save_conversations,\n",
        "                        \"source\": data[\"source\"]\n",
        "                    }\n",
        "                    new_datas.append(new_data)\n",
        "                    continue\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kujiP6xycCbj",
        "outputId": "a05179a1-649b-4a98-ade9-18751d092fae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2014it [00:18, 94.38it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deal  2000  len save =  2746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4004it [01:02, 41.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deal  4000  len save =  6995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5003it [01:27, 31.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deal  5000  len save =  9218\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "6011it [01:51, 53.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deal  6000  len save =  11352\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "73509it [03:47, 157.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deal  74000  len save =  83097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "85009it [06:43, 84.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deal  85000  len save =  101236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "105168it [06:44, 260.24it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "这里我们只有 Direct_Role_Play_Claude、PIPPA、 Janitor、Claude_Baize做超长切分处理"
      ],
      "metadata": {
        "id": "f8P_xNH-QKR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cutted_source = ['Direct_Role_Play_Claude', 'PIPPA', 'Janitor', 'Claude_Baize']\n",
        "\n",
        "new_datas = []\n",
        "\n",
        "max_len = 1024*3\n",
        "\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "error_count = 0\n",
        "for id,data in tqdm(enumerate(datas)):\n",
        "    if data[\"source\"] not in cutted_source:\n",
        "        new_datas.append(data)\n",
        "        continue\n",
        "    if get_sum_token(data) < max_len:\n",
        "        new_datas.append(data)\n",
        "        continue\n",
        "\n",
        "    if id % 1000 == 0:\n",
        "        print(\"deal \", id, \" len save = \", len(new_datas))\n",
        "\n",
        "    if data[\"system\"].strip() != \"\":\n",
        "        system_prompt = data[\"system\"]\n",
        "        sys_len = tokenizer_len( data[\"system\"] )\n",
        "        msg_len = sys_len\n",
        "\n",
        "        if msg_len > max_len*0.5:\n",
        "            # 太长的system prompt\n",
        "            new_datas.append(data)\n",
        "            continue\n",
        "\n",
        "        current_conversations = []\n",
        "\n",
        "        conversations = data[\"conversations\"]\n",
        "        conv_len = len(conversations)\n",
        "\n",
        "        for i in range(0, conv_len, 2):\n",
        "            current_conversations += conversations[i:i+2]\n",
        "            current_qa_len = tokenizer_len( conversations[i][\"value\"] ) + tokenizer_len( conversations[i+1][\"value\"] )\n",
        "\n",
        "            msg_len += current_qa_len\n",
        "\n",
        "            if msg_len > max_len:\n",
        "                # 生成一次数据\n",
        "                save_conversations = copy.deepcopy(current_conversations)\n",
        "                new_data = { \"system\": system_prompt,\n",
        "                    \"conversations\": save_conversations,\n",
        "                    \"source\": data[\"source\"]\n",
        "                }\n",
        "                new_datas.append(new_data)\n",
        "                msg_len = current_qa_len + sys_len\n",
        "                current_conversations = conversations[i:i+2]\n",
        "                if msg_len > max_len:\n",
        "                    error_count += 1\n",
        "\n",
        "        if len(current_conversations) > 2 or conv_len == 2:\n",
        "            save_conversations = copy.deepcopy(current_conversations)\n",
        "            new_data = { \"system\": system_prompt,\n",
        "                \"conversations\": save_conversations,\n",
        "                \"source\": data[\"source\"]\n",
        "            }\n",
        "            new_datas.append(new_data)\n",
        "\n",
        "        continue\n",
        "    else:\n",
        "        #不然的话system为0\n",
        "        conversations = data[\"conversations\"]\n",
        "\n",
        "        head_len = tokenizer_len( conversations[0][\"value\"] ) + tokenizer_len( conversations[1][\"value\"] )\n",
        "\n",
        "        if head_len > max_len*0.5:\n",
        "            new_datas.append(data)\n",
        "            continue\n",
        "\n",
        "        current_conversations = copy.deepcopy( conversations[0:2] )\n",
        "        msg_len = head_len\n",
        "\n",
        "        for i in range(2, len(conversations), 2):\n",
        "            current_conversations += conversations[i:i+2]\n",
        "            current_qa_len = tokenizer_len( conversations[i][\"value\"] ) + tokenizer_len( conversations[i+1][\"value\"] )\n",
        "            msg_len += current_qa_len\n",
        "            if msg_len > max_len:\n",
        "                save_conversations = copy.deepcopy(current_conversations)\n",
        "\n",
        "                new_data = { \"conversations\": save_conversations,\n",
        "                    \"source\": data[\"source\"],\n",
        "                    \"system\": \"\"\n",
        "                }\n",
        "                new_datas.append(data)\n",
        "\n",
        "                msg_len = head_len\n",
        "                current_conversations = copy.deepcopy( conversations[0:2] )\n",
        "\n",
        "        if len(current_conversations) > 2 or len(conversations) == 2:\n",
        "            save_conversations = copy.deepcopy(current_conversations)\n",
        "            new_data = { \"conversations\": save_conversations,\n",
        "                \"source\": data[\"source\"],\n",
        "                \"system\": \"\"\n",
        "            }\n",
        "            new_datas.append(new_data)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jArGn0ItQsA_",
        "outputId": "bce5e5c4-7df1-488f-b865-dfb370a06597"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2008it [00:25, 93.25it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deal  2000  len save =  2818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4005it [01:22, 30.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deal  4000  len save =  7688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5002it [01:58, 24.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deal  5000  len save =  10568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "6017it [02:30, 61.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deal  6000  len save =  13328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "72978it [04:40, 91.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deal  74000  len save =  86074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "85013it [08:37, 83.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deal  85000  len save =  105855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "105168it [08:38, 202.77it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(error_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkife4bCTzvE",
        "outputId": "e2fe028e-9008-4612-eae2-babf5b365e4a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(datas))\n",
        "print(len(new_datas))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZC8pyWhSOn6",
        "outputId": "0353873b-8a66-4905-8d4d-d1e6b3108c75"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105168\n",
            "121431\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for data in new_datas:\n",
        "    if \"system\" not in data:\n",
        "        print(\"system not in data\")\n",
        "    elif \"conversations\" not in data:\n",
        "        print(\"conversations not in data\")\n",
        "    elif len(data[\"conversations\"]) % 2!=0:\n",
        "        print(\"conversations not even\")\n",
        "    elif len(data[\"conversations\"]) == 0:\n",
        "        print(\"conversations is empty\")\n",
        "        continue\n",
        "    else:\n",
        "        text = data[\"conversations\"][0]\n",
        "        continue\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V7IayAVZOdM",
        "outputId": "cdb21ac2-c8d9-40c5-c7b1-f932b6ae9d0c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conversations is empty\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_datas[500].keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLPE2lzjZZgW",
        "outputId": "349c10a2-9668-42a4-8c8f-1883067d2090"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['conversations', 'source', 'system'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_name = \"/content/drive/MyDrive/CardBuild/HaruhiZero/merge_cut3k_121k_no_source.jsonl\"\n",
        "\n",
        "with open(save_name, 'w', encoding='utf-8') as f:\n",
        "    for data in tqdm(new_datas):\n",
        "        if len(data[\"conversations\"]) == 0:\n",
        "            continue\n",
        "        if \"source\" in data:\n",
        "            del data[\"source\"]\n",
        "        f.write(json.dumps(data, ensure_ascii=False) + '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xKKFw4WZGH9",
        "outputId": "c225622e-6ef0-49f0-f84b-e2574f87a690"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 121431/121431 [00:23<00:00, 5101.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "-uBN8P3QaFZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "下面来看训练"
      ],
      "metadata": {
        "id": "h62oNk3qaHX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade huggingface_hub\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zElyM_0tGpZH",
        "outputId": "29646ff7-6446-4d09-c516-5d2cde740a5e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.11.17)\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: read).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjFuGKxUFvbY",
        "outputId": "f53853ac-7930-4417-e813-13c4e4aa0d7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 6754, done.\u001b[K\n",
            "remote: Counting objects: 100% (773/773), done.\u001b[K\n",
            "remote: Compressing objects: 100% (318/318), done.\u001b[K\n",
            "remote: Total 6754 (delta 507), reused 668 (delta 453), pack-reused 5981\u001b[K\n",
            "Receiving objects: 100% (6754/6754), 204.59 MiB | 21.96 MiB/s, done.\n",
            "Resolving deltas: 100% (4853/4853), done.\n",
            "Updating files: 100% (140/140), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd LLaMA-Factory\n",
        "!pip install -q -r requirements.txt\n",
        "!pip install -q einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAcYKc9qGwcY",
        "outputId": "8be2615a-e869-42b5-9887-3286548c844c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.7/79.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/dataset"
      ],
      "metadata": {
        "id": "9OSdjnmbHCSz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/LLaMA-Factory/dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIhsKveEIT7C",
        "outputId": "46dd871c-28bd-434c-8211-372e5952f07f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/content/LLaMA-Factory/dataset': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFANKTp0JWSZ",
        "outputId": "73e55777-8ebd-4e3b-dd0a-afe3a3d818bb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/CardBuild/HaruhiZero/*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfrKNsHhJbKC",
        "outputId": "9db71154-9c01-4ca7-c781-99e9f50787f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CardBuild/HaruhiZero/chinese_dialogue_clean.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/Chinese_erotic.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/Chinese_NovelWriting.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/Claude_10k.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/Claude_7379.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/dialogue_extract.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/dialogue_negative.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/Direct_Role_Play_Claude.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/english_dialogue_clean.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/english_dialogue_extract.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/Haruhi52K.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/Haruhi_Other_Chinese.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/Haruhi_Waifu_extended.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/Janitor.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/merge_all_105k.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/merge_all_126k_no_source.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/merge_cut3k_121k_no_source.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/PIPPA.jsonl\n",
            "/content/drive/MyDrive/CardBuild/HaruhiZero/RoleLLM.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "input_path = \"/content/drive/MyDrive/CardBuild/HaruhiZero/\"\n",
        "file_names = [\"Chinese_erotic.jsonl\",\"Chinese_NovelWriting.jsonl\",\"merge_all_126k_no_source.jsonl\"]\n",
        "\n",
        "target_path = \"/content/LLaMA-Factory/data/\"\n",
        "\n",
        "请为我实现一段python代码，把input_path下的几个file_names对应的jsonl文件\n",
        "拷贝到target_path\n",
        "\n",
        "然后以json格式写入target_file"
      ],
      "metadata": {
        "id": "xDYOUoHDK4x9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "def copy_files(input_path, file_names, target_path):\n",
        "    # 确保目标路径存在\n",
        "    if not os.path.exists(target_path):\n",
        "        os.makedirs(target_path)\n",
        "\n",
        "    # 遍历文件列表并拷贝每个文件\n",
        "    for file_name in file_names:\n",
        "        source_file = os.path.join(input_path, file_name)\n",
        "        destination_file = os.path.join(target_path, file_name)\n",
        "\n",
        "        # 检查源文件是否存在\n",
        "        if os.path.exists(source_file):\n",
        "            # 拷贝文件\n",
        "            shutil.copy(source_file, destination_file)\n",
        "            print(f\"File '{file_name}' copied to {target_path}\")\n",
        "        else:\n",
        "            print(f\"File '{file_name}' not found in {input_path}\")\n",
        "\n",
        "# 要拷贝的文件路径和文件名\n",
        "input_path = \"/content/drive/MyDrive/CardBuild/HaruhiZero/\"\n",
        "file_names = [\"Chinese_erotic.jsonl\", \"Chinese_NovelWriting.jsonl\", \"merge_all_126k_no_source.jsonl\"]\n",
        "target_path = \"/content/LLaMA-Factory/data/\"\n",
        "\n",
        "# 执行拷贝操作\n",
        "copy_files(input_path, file_names, target_path)\n"
      ],
      "metadata": {
        "id": "zYZqDgizJdyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da14b42e-8775-4edb-b9e1-1dca2b87b07c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'Chinese_erotic.jsonl' copied to /content/LLaMA-Factory/data/\n",
            "File 'Chinese_NovelWriting.jsonl' copied to /content/LLaMA-Factory/data/\n",
            "File 'merge_all_126k_no_source.jsonl' copied to /content/LLaMA-Factory/data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "请为我实现一段python代码，对于/content/LLaMA-Factory/data/dataset_info.json 备份到dataset_info_back.json"
      ],
      "metadata": {
        "id": "ZJnK1Za2J-0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "original_file = '/content/LLaMA-Factory/data/dataset_info.json'\n",
        "backup_file = '/content/LLaMA-Factory/data/dataset_info_back.json'\n",
        "\n",
        "# Make a copy of the original file\n",
        "shutil.copy(original_file, backup_file)\n",
        "\n",
        "# Verify copy succeeded\n",
        "if os.path.exists(backup_file):\n",
        "    print(f'Backup of {original_file} to {backup_file} succeeded!')\n",
        "else:\n",
        "    print(f'Backup of {original_file} to {backup_file} failed!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exqKxSYMLKMf",
        "outputId": "df1e812a-8b41-4eef-87b6-09e36b8c48dc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backup of /content/LLaMA-Factory/data/dataset_info.json to /content/LLaMA-Factory/data/dataset_info_back.json succeeded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "new_file = '/content/LLaMA-Factory/data/dataset_info.json'\n",
        "\n",
        "with open(new_file, 'r') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "file_names = [\"Chinese_erotic.jsonl\", \"Chinese_NovelWriting.jsonl\", \"merge_all_126k_no_source.jsonl\"]\n",
        "\n",
        "for i, filename in enumerate(file_names):\n",
        "    data[f'haruhi_zero_{i}'] = {\n",
        "    \"file_name\": filename,\n",
        "    \"formatting\":\"sharegpt\",\n",
        "    \"columns\": {\n",
        "        \"messages\": \"conversations\",\n",
        "        \"system\": \"system\",\n",
        "    },\n",
        "    \"tags\": {\n",
        "        \"role_tag\": \"from\",\n",
        "        \"content_tag\": \"value\",\n",
        "        \"user_tag\": \"human\",\n",
        "        \"assistant_tag\":\"gpt\",\n",
        "    }\n",
        "    }\n",
        "\n",
        "with open(new_file, 'w') as f:\n",
        "  json.dump(data, f, indent=2)\n",
        "\n",
        "print('Data copied and new info appended to', new_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ik4Bvq1LNpE",
        "outputId": "71d2bed4-8bc7-4d8d-a110-5dd2b757ed3d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data copied and new info appended to /content/LLaMA-Factory/data/dataset_info.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tiktoken transformers_stream_generator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaruMxYdL6QU",
        "outputId": "2031e241-1ce1-4b17-bb09-999a9b249aa6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/2.0 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/2.0 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m1.7/2.0 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers_stream_generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "nqZswBFMN6-C"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path  Qwen/Qwen-1_8B-Chat\\\n",
        "    --do_train True\\\n",
        "    --dataset haruhi_zero_0,haruhi_zero_1,haruhi_zero_2 \\\n",
        "    --template qwen \\\n",
        "    --finetuning_type lora \\\n",
        "    --lora_target c_attn \\\n",
        "    --lora_rank 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --output_dir qwen_1_8-finetuned \\\n",
        "    --overwrite_output_dir \\\n",
        "    --overwrite_cache \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --logging_steps 100 \\\n",
        "    --save_steps 1000 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --plot_loss \\\n",
        "    --fp16 True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhZPqKBcGHau",
        "outputId": "3e8299bc-9589-414b-b941-cf54642495f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-25 07:41:50.803083: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-25 07:41:50.803141: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-25 07:41:50.804414: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-25 07:41:51.935026: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "01/25/2024 07:41:53 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
            "[INFO|training_args.py:1828] 2024-01-25 07:41:53,832 >> PyTorch: setting up devices\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1741: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
            "  warnings.warn(\n",
            "01/25/2024 07:41:53 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1\n",
            "  distributed training: True, compute dtype: torch.float16\n",
            "01/25/2024 07:41:53 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0002,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=qwen_1_8-finetuned/runs/Jan25_07-41-53_a7d65db00b9d,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=100,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=cosine,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=qwen_1_8-finetuned,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=qwen_1_8-finetuned,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=1000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-01-25 07:41:54,321 >> loading file qwen.tiktoken from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/qwen.tiktoken\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-01-25 07:41:54,321 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-01-25 07:41:54,321 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-01-25 07:41:54,321 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-01-25 07:41:54,321 >> loading file tokenizer.json from cache at None\n",
            "[INFO|configuration_utils.py:729] 2024-01-25 07:41:55,064 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/config.json\n",
            "[INFO|configuration_utils.py:729] 2024-01-25 07:41:55,273 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/config.json\n",
            "[INFO|configuration_utils.py:792] 2024-01-25 07:41:55,275 >> Model config QWenConfig {\n",
            "  \"_name_or_path\": \"Qwen/Qwen-1_8B-Chat\",\n",
            "  \"architectures\": [\n",
            "    \"QWenLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_dropout_prob\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"Qwen/Qwen-1_8B-Chat--configuration_qwen.QWenConfig\",\n",
            "    \"AutoModelForCausalLM\": \"Qwen/Qwen-1_8B-Chat--modeling_qwen.QWenLMHeadModel\"\n",
            "  },\n",
            "  \"bf16\": false,\n",
            "  \"emb_dropout_prob\": 0.0,\n",
            "  \"fp16\": false,\n",
            "  \"fp32\": false,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"kv_channels\": 128,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"qwen\",\n",
            "  \"no_bias\": true,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"onnx_safe\": null,\n",
            "  \"rotary_emb_base\": 10000,\n",
            "  \"rotary_pct\": 1.0,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"seq_length\": 8192,\n",
            "  \"softmax_in_fp32\": false,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"QWenTokenizer\",\n",
            "  \"transformers_version\": \"4.37.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_cache_kernel\": false,\n",
            "  \"use_cache_quantization\": false,\n",
            "  \"use_dynamic_ntk\": true,\n",
            "  \"use_flash_attn\": \"auto\",\n",
            "  \"use_logn_attn\": true,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3478] 2024-01-25 07:41:55,418 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1428] 2024-01-25 07:41:55,419 >> Instantiating QWenLMHeadModel model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:826] 2024-01-25 07:41:55,420 >> Generate config GenerationConfig {}\n",
            "\n",
            "Try importing flash-attention for faster inference...\n",
            "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
            "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
            "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
            "Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.27it/s]\n",
            "[INFO|modeling_utils.py:4352] 2024-01-25 07:41:57,352 >> All model checkpoint weights were used when initializing QWenLMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:4360] 2024-01-25 07:41:57,352 >> All the weights of QWenLMHeadModel were initialized from the model checkpoint at Qwen/Qwen-1_8B-Chat.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use QWenLMHeadModel for predictions without further training.\n",
            "[INFO|configuration_utils.py:781] 2024-01-25 07:41:57,459 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/generation_config.json\n",
            "[INFO|configuration_utils.py:826] 2024-01-25 07:41:57,460 >> Generate config GenerationConfig {\n",
            "  \"chat_format\": \"chatml\",\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"max_new_tokens\": 512,\n",
            "  \"max_window_size\": 6144,\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"top_k\": 0,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "[WARNING|modeling_utils.py:2134] 2024-01-25 07:41:57,461 >> You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
            "01/25/2024 07:41:57 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.\n",
            "01/25/2024 07:41:57 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
            "01/25/2024 07:41:57 - INFO - llmtuner.model.loader - trainable params: 3145728 || all params: 1839974400 || trainable%: 0.1710\n",
            "01/25/2024 07:41:57 - INFO - llmtuner.data.template - Add eos token: <|endoftext|>\n",
            "01/25/2024 07:41:57 - INFO - llmtuner.data.template - Add pad token: <|endoftext|>\n",
            "01/25/2024 07:41:57 - INFO - llmtuner.data.template - Replace eos token: <|im_end|>\n",
            "01/25/2024 07:41:57 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.\n",
            "Using custom data configuration default-514280a45fbfef29\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/root/.cache/huggingface/datasets/json/default-514280a45fbfef29/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-514280a45fbfef29/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "Generating train split\n",
            "Generating train split: 1621 examples [00:00, 21255.80 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-514280a45fbfef29/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "Converting format of dataset:   0% 0/1621 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-514280a45fbfef29/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a5ed44d9bd909e0e.arrow\n",
            "Converting format of dataset: 100% 1621/1621 [00:00<00:00, 11279.50 examples/s]\n",
            "01/25/2024 07:41:58 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.\n",
            "Using custom data configuration default-88d80eb43c506908\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/root/.cache/huggingface/datasets/json/default-88d80eb43c506908/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-88d80eb43c506908/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "Generating train split\n",
            "Generating train split: 69085 examples [00:01, 48831.41 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-88d80eb43c506908/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "Converting format of dataset:   0% 0/69085 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-88d80eb43c506908/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1265b4fe0b96e9e4.arrow\n",
            "Converting format of dataset: 100% 69085/69085 [00:03<00:00, 17851.89 examples/s]\n",
            "01/25/2024 07:42:03 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.\n",
            "Using custom data configuration default-a3e589427f24d37f\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/root/.cache/huggingface/datasets/json/default-a3e589427f24d37f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-a3e589427f24d37f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "Generating train split\n",
            "Generating train split: 121430 examples [00:02, 56899.41 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-a3e589427f24d37f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "Converting format of dataset:   0% 0/121430 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-a3e589427f24d37f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bca82ccc9425c779.arrow\n",
            "Converting format of dataset: 100% 121430/121430 [00:14<00:00, 8229.16 examples/s] \n",
            "Running tokenizer on dataset:   0% 0/192136 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-514280a45fbfef29/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3499a0242b70c504.arrow\n",
            "Running tokenizer on dataset:   8% 15000/192136 [00:41<08:19, 354.91 examples/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R9OaXCFQLze_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}