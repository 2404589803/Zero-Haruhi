{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOUR1tcun7x1jt53dMx8vFp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/Zero-Haruhi/blob/main/notebook/test/ChatHaruhi3_messages%E7%BB%84%E8%A3%85.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "02/10\n",
        "\n",
        "- [x] 在foo embedding的前提下调通embedding组装，考虑没有rag\n",
        "- [x] 考虑单rag\n",
        "- [x] 考虑复合rag\n",
        "- [x] 考虑rag with另一个句子的retrieve\n",
        "- [x] 接入tiktoken\n",
        "- [x] 接入bge"
      ],
      "metadata": {
        "id": "Zvf72DZRhCzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets tiktoken"
      ],
      "metadata": {
        "id": "h7UatXQhtEHw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!rm -rf /content/Zero-Haruhi\n",
        "!git clone https://github.com/LC1332/Zero-Haruhi\n",
        "%cd /content/Zero-Haruhi"
      ],
      "metadata": {
        "id": "848R2Fqehjqi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0396d10f-10dd-4070-e078-3d69709710ef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'Zero-Haruhi'...\n",
            "remote: Enumerating objects: 321, done.\u001b[K\n",
            "remote: Counting objects: 100% (318/318), done.\u001b[K\n",
            "remote: Compressing objects: 100% (166/166), done.\u001b[K\n",
            "remote: Total 321 (delta 204), reused 243 (delta 147), pack-reused 3\u001b[K\n",
            "Receiving objects: 100% (321/321), 2.84 MiB | 20.06 MiB/s, done.\n",
            "Resolving deltas: 100% (204/204), done.\n",
            "/content/Zero-Haruhi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ChatHaruhi.NaiveDB import NaiveDB\n",
        "from ChatHaruhi.embeddings import foo_embedding, foo_bge_zh_15, foo_bce, foo_openai, embedname2columnname, get_bge_zh_embedding\n",
        "from ChatHaruhi.utils import base64_to_float_array, base64_to_string, tiktoken_counter\n",
        "from ChatHaruhi.sugar_map import sugar_role_names, enname2zhname"
      ],
      "metadata": {
        "id": "ekOEuEHJlHyw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "On7NzQoCEs0h"
      },
      "outputs": [],
      "source": [
        "def get_text_from_data( data ):\n",
        "    if \"text\" in data:\n",
        "        return data['text']\n",
        "    elif \"enc_text\" in data:\n",
        "        # from .utils import base64_to_string\n",
        "        return base64_to_string( data['enc_text'] )\n",
        "    else:\n",
        "        print(\"warning! failed to get text from data \", data)\n",
        "        return \"\"\n",
        "\n",
        "def parse_rag(text):\n",
        "    lines = text.split(\"\\n\")\n",
        "    ans = []\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        if \"{{RAG对话}}\" in line:\n",
        "            ans.append({\"n\": 1, \"max_token\": -1, \"query\": \"default\", \"lid\": i})\n",
        "        elif \"{{RAG对话|\" in line:\n",
        "            query_info = line.split(\"|\")[1].rstrip(\"}}\")\n",
        "            ans.append({\"n\": 1, \"max_token\": -1, \"query\": query_info, \"lid\": i})\n",
        "        elif \"{{RAG多对话|\" in line:\n",
        "            parts = line.split(\"|\")\n",
        "            max_token = int(parts[1].split(\"<=\")[1])\n",
        "            max_n = int(parts[2].split(\"<=\")[1].rstrip(\"}}\"))\n",
        "            ans.append({\"n\": max_n, \"max_token\": max_token, \"query\": \"default\", \"lid\": i})\n",
        "\n",
        "    return ans\n",
        "\n",
        "class ChatHaruhi:\n",
        "    def __init__(self,\n",
        "                 role_name = None,\n",
        "                 user_name = None,\n",
        "                 persona = None,\n",
        "                 stories = None,\n",
        "                 story_vecs = None,\n",
        "                 role_from_hf = None,\n",
        "                 role_from_jsonl = None,\n",
        "                 llm = None, # 默认的message2response的函数\n",
        "                 llm_async = None, # 默认的message2response的async函数\n",
        "                 user_name_in_message = \"default\",\n",
        "                 verbose = None,\n",
        "                 embed_name = None,\n",
        "                 embedding = None,\n",
        "                 db = None,\n",
        "                 token_counter = \"default\",\n",
        "                 max_input_token = 1800,\n",
        "                 max_len_story_haruhi = 1000,\n",
        "                 max_story_n_haruhi = 5\n",
        "                 ):\n",
        "\n",
        "        self.verbose = True if verbose is None or verbose else False\n",
        "\n",
        "        self.db = db\n",
        "\n",
        "        self.embed_name = embed_name\n",
        "\n",
        "        self.max_len_story_haruhi = max_len_story_haruhi # 这个设置只对过往Haruhi的sugar角色有效\n",
        "        self.max_story_n_haruhi = max_story_n_haruhi # 这个设置只对过往Haruhi的sugar角色有效\n",
        "\n",
        "        self.last_msg = None\n",
        "\n",
        "        if embedding is None:\n",
        "            self.embedding = self.set_embedding_with_name( embed_name )\n",
        "\n",
        "        if persona and role_name and stories and story_vecs and len(stories) == len(story_vecs):\n",
        "            # 完全从外部设置，这个时候要求story_vecs和embedding的返回长度一致\n",
        "            self.persona, self.role_name, self.user_name = persona, role_name, user_name\n",
        "            self.build_db(stories, story_vecs)\n",
        "        elif persona and role_name and stories:\n",
        "            # 从stories中提取story_vecs，重新用self.embedding进行embedding\n",
        "            story_vecs = self.extract_story_vecs(stories)\n",
        "            self.persona, self.role_name, self.user_name = persona, role_name, user_name\n",
        "            self.build_db(stories, story_vecs)\n",
        "        elif role_from_hf:\n",
        "            # 从hf加载role\n",
        "            self.persona, new_role_name, self.stories, self.story_vecs = self.load_role_from_hf(role_from_hf)\n",
        "            if new_role_name:\n",
        "                self.role_name = new_role_name\n",
        "            else:\n",
        "                self.role_name = role_name\n",
        "            self.user_name = user_name\n",
        "            self.build_db(self.stories, self.story_vecs)\n",
        "        elif role_from_jsonl:\n",
        "            # 从jsonl加载role\n",
        "            self.persona, new_role_name, self.stories, self.story_vecs = self.load_role_from_jsonl(role_from_jsonl)\n",
        "            if new_role_name:\n",
        "                self.role_name = new_role_name\n",
        "            else:\n",
        "                self.role_name = role_name\n",
        "            self.user_name = user_name\n",
        "            self.build_db(self.stories, self.story_vecs)\n",
        "        elif persona and role_name:\n",
        "            # 这个时候也就是说没有任何的RAG，\n",
        "            self.persona, self.role_name, self.user_name = persona, role_name, user_name\n",
        "            self.db = None\n",
        "        elif role_name and self.check_sugar( role_name ):\n",
        "            # 这个时候是sugar的role\n",
        "            self.persona, self.role_name, self.stories, self.story_vecs = self.load_role_from_sugar( role_name )\n",
        "            self.build_db(self.stories, self.story_vecs)\n",
        "            self.add_rag_prompt_after_persona()\n",
        "        else:\n",
        "            raise ValueError(\"persona和role_name必须同时设置，或者role_name是ChatHaruhi的预设人物\")\n",
        "\n",
        "        self.llm, self.llm_async = llm, llm_async\n",
        "        if not self.llm and self.verbose:\n",
        "            print(\"warning, llm没有设置，仅get_message起作用，调用chat将回复idle message\")\n",
        "\n",
        "        self.user_name_in_message = user_name_in_message\n",
        "        self.previous_user_pool = set([user_name]) if user_name else set()\n",
        "        self.current_user_name_in_message = user_name_in_message.lower() == \"add\"\n",
        "\n",
        "        self.idle_message = \"idel message, you see this because self.llm has not been set.\"\n",
        "\n",
        "        if token_counter.lower() == \"default\":\n",
        "            # TODO change load from util\n",
        "            self.token_counter = tiktoken_counter\n",
        "        elif token_counter == None:\n",
        "            self.token_counter = lambda x: 0\n",
        "        else:\n",
        "            self.token_counter = token_counter\n",
        "            if self.verbose:\n",
        "                print(\"user set costomized token_counter\")\n",
        "\n",
        "        self.max_input_token = max_input_token\n",
        "\n",
        "        self.history = []\n",
        "\n",
        "    def check_sugar(self, role_name):\n",
        "        return role_name in sugar_role_names\n",
        "\n",
        "    def load_role_from_sugar(self, role_name):\n",
        "        en_role_name = sugar_role_names[role_name]\n",
        "        new_role_name = enname2zhname[en_role_name]\n",
        "        role_from_hf = \"silk-road/ChatHaruhi-RolePlaying/\" + en_role_name\n",
        "        persona, _, stories, story_vecs = self.load_role_from_hf(role_from_hf)\n",
        "\n",
        "        return persona, new_role_name, stories, story_vecs\n",
        "\n",
        "    def add_rag_prompt_after_persona( self ):\n",
        "        rag_sentence = \"{{RAG多对话|token<=\" + str(self.max_len_story_haruhi) + \"|n<=\" + str(self.max_story_n_haruhi) + \"}}\"\n",
        "        self.persona += \"Classic scenes for the role are as follows:\\n\" + rag_sentence\n",
        "\n",
        "    def set_embedding_with_name(self, embed_name):\n",
        "        if embed_name is None:\n",
        "            self.embed_name = \"bge_zh\"\n",
        "            return get_bge_zh_embedding\n",
        "        elif embed_name == \"foo\":\n",
        "            return foo_embedding\n",
        "        elif embed_name == \"bge_zh\":\n",
        "            return foo_bge_zh_15\n",
        "        elif embed_name == \"bce\":\n",
        "            return foo_bce\n",
        "        elif embed_name == \"openai\" or embed_name == \"luotuo_openai\":\n",
        "            return foo_openai\n",
        "\n",
        "    def set_new_user(self, user):\n",
        "        if len(self.previous_user_pool) > 0 and user not in self.previous_user_pool:\n",
        "            if self.user_name_in_message.lower() == \"default\":\n",
        "                if self.verbose:\n",
        "                    print(f'new user {user} included in conversation')\n",
        "                self.current_user_name_in_message = True\n",
        "        self.user_name = user\n",
        "        self.previous_user_pool.add(user)\n",
        "\n",
        "    def chat(self, user, text):\n",
        "        self.set_new_user(user)\n",
        "        message = self.get_message(user, text)\n",
        "        if self.llm:\n",
        "            response = self.llm(message)\n",
        "            self.append_message(response)\n",
        "            return self.llm(message)\n",
        "\n",
        "    async def async_chat(self, user, text):\n",
        "        self.set_new_user(user)\n",
        "        message = self.get_message(user, text)\n",
        "        if self.llm_async:\n",
        "            response = await self.llm_async(message)\n",
        "            self.append_message(response)\n",
        "            return self.llm_async(message)\n",
        "\n",
        "    def parse_rag_from_persona(self, persona, text = None):\n",
        "        #每个query_rag需要饱含\n",
        "        # \"n\" 需要几个story\n",
        "        # \"max_token\" 最多允许多少个token，如果-1则不限制\n",
        "        # \"query\" 需要查询的内容，如果等同于\"default\"则替换为text\n",
        "        # \"lid\" 需要替换的行，这里直接进行行替换，忽视行的其他内容\n",
        "\n",
        "        query_rags = parse_rag( persona )\n",
        "\n",
        "        if text is not None:\n",
        "            for rag in query_rags:\n",
        "                if rag['query'] == \"default\":\n",
        "                    rag['query'] = text\n",
        "\n",
        "        return query_rags, self.token_counter(persona)\n",
        "\n",
        "    def append_message( self, response , speaker = None ):\n",
        "        if speaker is None:\n",
        "            # 如果role是none，则认为是本角色{{role}}输出的句子\n",
        "            self.history.append({\"speaker\":\"{{user}}\",\"content\":response})\n",
        "            # 叫speaker是为了和role进行区分\n",
        "        else:\n",
        "            self.history.append({\"speaker\":speaker,\"content\":response})\n",
        "\n",
        "    def check_recompute_stories_token(self):\n",
        "        return len(self.db.metas) == len(self.db.stories)\n",
        "\n",
        "    def recompute_stories_token(self):\n",
        "        self.db.metas = [self.token_counter(story) for story in self.db.stories]\n",
        "\n",
        "    def rag_retrieve( self, query, n, max_token, avoid_ids = [] ):\n",
        "        # 返回一个rag_id的列表\n",
        "        query_vec = self.embedding(query)\n",
        "\n",
        "        self.db.clean_flag()\n",
        "        self.db.disable_story_with_ids( avoid_ids )\n",
        "\n",
        "        retrieved_ids = self.db.search( query_vec, n )\n",
        "\n",
        "        if self.check_recompute_stories_token():\n",
        "            self.recompute_stories_token()\n",
        "\n",
        "        sum_token = 0\n",
        "\n",
        "        ans = []\n",
        "\n",
        "        for i in range(0, len(retrieved_ids)):\n",
        "            if i == 0:\n",
        "                sum_token += self.db.metas[retrieved_ids[i]]\n",
        "                ans.append(retrieved_ids[i])\n",
        "                continue\n",
        "            else:\n",
        "                sum_token += self.db.metas[retrieved_ids[i]]\n",
        "                if sum_token <= max_token:\n",
        "                    ans.append(retrieved_ids[i])\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "        return ans\n",
        "\n",
        "\n",
        "    def rag_retrieve_all( self, query_rags, rest_limit ):\n",
        "        # 返回一个rag_ids的列表\n",
        "        retrieved_ids = []\n",
        "        rag_ids = []\n",
        "\n",
        "        for query_rag in query_rags:\n",
        "            query = query_rag['query']\n",
        "            n = query_rag['n']\n",
        "            max_token = rest_limit\n",
        "            if rest_limit > query_rag['max_token'] and query_rag['max_token'] > 0:\n",
        "                max_token = query_rag['max_token']\n",
        "\n",
        "            rag_id = self.rag_retrieve( query, n, max_token, avoid_ids = retrieved_ids )\n",
        "            rag_ids.append( rag_id )\n",
        "            retrieved_ids += rag_id\n",
        "\n",
        "        return rag_ids\n",
        "\n",
        "    def append_history_under_limit(self, message, rest_limit):\n",
        "        # 返回一个messages的列表\n",
        "        # print(\"call append_history_under_limit\")\n",
        "\n",
        "        # 从后往前计算token，不超过rest_limit,\n",
        "        # 如果speaker是{{role}},则message的role是assistant\n",
        "\n",
        "        return message\n",
        "\n",
        "    def get_message(self, user, text):\n",
        "        query_token = self.token_counter(text)\n",
        "\n",
        "        # 首先获取需要多少个rag story\n",
        "        query_rags, persona_token = self.parse_rag_from_persona( self.persona, text )\n",
        "        #每个query_rag需要饱含\n",
        "        # \"n\" 需要几个story\n",
        "        # \"max_token\" 最多允许多少个token，如果-1则不限制\n",
        "        # \"query\" 需要查询的内容，如果等同于\"default\"则替换为text\n",
        "        # \"lid\" 需要替换的行，这里直接进行行替换，忽视行的其他内容\n",
        "\n",
        "\n",
        "\n",
        "        rest_limit = self.max_input_token - persona_token - query_token\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"query_rags: {query_rags} rest_limit = { rest_limit }\")\n",
        "\n",
        "        rag_ids = self.rag_retrieve_all( query_rags, rest_limit )\n",
        "\n",
        "        # 将rag_ids对应的故事 替换到persona中\n",
        "        augmented_persona = self.augment_persona( self.persona, rag_ids, query_rags )\n",
        "\n",
        "        system_prompt = self.package_system_prompt( self.role_name, augmented_persona )\n",
        "\n",
        "        token_for_system = self.token_counter( system_prompt )\n",
        "\n",
        "        rest_limit = self.max_input_token - token_for_system - query_token\n",
        "\n",
        "        message = [{\"role\":\"system\",\"content\":system_prompt}]\n",
        "\n",
        "        message = self.append_history_under_limit( message, rest_limit )\n",
        "\n",
        "        # message.append({\"role\":\"user\",\"content\":text})\n",
        "        self.last_msg = {\"role\":\"user\",\"content\":text}\n",
        "\n",
        "        return message\n",
        "\n",
        "    def package_system_prompt(self, role_name, augmented_persona):\n",
        "        bot_name = role_name\n",
        "        return f\"\"\"You are now in roleplay conversation mode. Pretend to be {bot_name} whose persona follows:\n",
        "{augmented_persona}\n",
        "\n",
        "You will stay in-character whenever possible, and generate responses as if you were {bot_name}\"\"\"\n",
        "\n",
        "\n",
        "    def augment_persona(self, persona, rag_ids, query_rags):\n",
        "        lines = persona.split(\"\\n\")\n",
        "        for rag_id, query_rag in zip(rag_ids, query_rags):\n",
        "            lid = query_rag['lid']\n",
        "            new_text = \"\"\n",
        "            for id in rag_id:\n",
        "                new_text += \"###\\n\" + self.db.stories[id].strip() + \"\\n\"\n",
        "            new_text = new_text.strip()\n",
        "            lines[lid] = new_text\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "    def load_role_from_jsonl( self, role_from_jsonl ):\n",
        "        if self.verbose:\n",
        "            print(f\"因为懒得测试，jsonl的分支还没有测试，当你使用的时候如果通过了，请告诉鲁叔测试通过了\")\n",
        "        import json\n",
        "        datas = []\n",
        "        with open(role_from_jsonl, 'r') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    datas.append(json.loads(line))\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        column_name = \"\"\n",
        "\n",
        "        if self.embed_name in embedname2columnname:\n",
        "            column_name = embedname2columnname[self.embed_name]\n",
        "        else:\n",
        "            print('warning! unkown embedding name ', self.embed_name ,' while loading role')\n",
        "            column_name = 'luotuo_openai'\n",
        "\n",
        "        stories, story_vecs, persona = self.extract_text_vec_from_datas(datas, column_name)\n",
        "\n",
        "        return persona, None, stories, story_vecs\n",
        "\n",
        "\n",
        "    def load_role_from_hf(self, role_from_hf):\n",
        "        # 从hf加载role\n",
        "        # self.persona, new_role_name, self.stories, self.story_vecs = self.load_role_from_hf(role_from_hf)\n",
        "\n",
        "        from datasets import load_dataset\n",
        "\n",
        "        if role_from_hf.count(\"/\") == 1:\n",
        "            dataset = load_dataset(role_from_hf)\n",
        "            datas = dataset[\"train\"]\n",
        "        elif role_from_hf.count(\"/\") >= 2:\n",
        "            split_index = role_from_hf.index('/')\n",
        "            second_split_index = role_from_hf.index('/', split_index+1)\n",
        "            dataset_name = role_from_hf[:second_split_index]\n",
        "            split_name = role_from_hf[second_split_index+1:]\n",
        "\n",
        "            fname = split_name + '.jsonl'\n",
        "            dataset = load_dataset(dataset_name,data_files={'train':fname})\n",
        "            datas = dataset[\"train\"]\n",
        "\n",
        "        column_name = \"\"\n",
        "\n",
        "        if self.embed_name in embedname2columnname:\n",
        "            column_name = embedname2columnname[self.embed_name]\n",
        "        else:\n",
        "            print('warning! unkown embedding name ', self.embed_name ,' while loading role')\n",
        "            column_name = 'luotuo_openai'\n",
        "\n",
        "        stories, story_vecs, persona = self.extract_text_vec_from_datas(datas, column_name)\n",
        "\n",
        "        return persona, None, stories, story_vecs\n",
        "\n",
        "    def extract_text_vec_from_datas(self, datas, column_name):\n",
        "        # 从datas中提取text和vec\n",
        "        # extract text and vec from huggingface dataset\n",
        "        # return texts, vecs\n",
        "        # from .utils import base64_to_float_array\n",
        "\n",
        "        texts = []\n",
        "        vecs = []\n",
        "        for data in datas:\n",
        "            if data[column_name] == 'system_prompt':\n",
        "                system_prompt = get_text_from_data( data )\n",
        "            elif data[column_name] == 'config':\n",
        "                pass\n",
        "            else:\n",
        "                vec = base64_to_float_array( data[column_name] )\n",
        "                text = get_text_from_data( data )\n",
        "                vecs.append( vec )\n",
        "                texts.append( text )\n",
        "        return texts, vecs, system_prompt\n",
        "\n",
        "    def load_role_from_jsonl(self, role_from_jsonl):\n",
        "        # 从jsonl加载role\n",
        "        return None\n",
        "\n",
        "    def extract_story_vecs(self, stories):\n",
        "        # 从stories中提取story_vecs\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"re-extract vector for {len(stories)} stories\")\n",
        "\n",
        "        from tqdm import tqdm\n",
        "        story_vecs = []\n",
        "        for story in tqdm(stories):\n",
        "            story_vecs.append(self.embedding(story))\n",
        "\n",
        "        return story_vecs\n",
        "\n",
        "    def build_db(self, stories, story_vecs):\n",
        "        # db的构造函数\n",
        "        if self.db is None:\n",
        "            self.db = NaiveDB()\n",
        "        self.db.build_db(stories, story_vecs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response( message ):\n",
        "    return \"语言模型输出了角色扮演的结果\"\n",
        "\n",
        "chatbot = ChatHaruhi( role_name = '小猫咪', persona = \"你扮演一只小猫咪\", llm = get_response )\n"
      ],
      "metadata": {
        "id": "Sc3dO74u0NMf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot.chat(user = \"用户\", text = \"你好\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "8OEOOAGeKOdl",
        "outputId": "665c5350-12c6-4e3e-9253-773262cd8d89"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query_rags: [] rest_limit = 1784\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'语言模型输出了角色扮演的结果'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot.chat(user = \"用户2\", text = \"你好\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "B1trrXMUKU3s",
        "outputId": "1e49128e-7aa8-4f32-f604-eeccc58978bd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new user 用户2 included in conversation\n",
            "query_rags: [] rest_limit = 1784\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'语言模型输出了角色扮演的结果'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 对没有rag的persona进行测试"
      ],
      "metadata": {
        "id": "zmOAY9xC0G3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot = ChatHaruhi( role_name = '小猫咪', persona = \"你扮演一只小猫咪\", llm = get_response )\n",
        "message = chatbot.get_message( user = \"用户\", text = \"你好\" )\n",
        "for msg in message:\n",
        "    print(f\"{msg['role']}\\n{msg['content']}\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4Y0w6JH0M8J",
        "outputId": "329c8e68-876b-4f63-ba36-7dba33be1bc1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query_rags: [] rest_limit = 1784\n",
            "system\n",
            "You are now in roleplay conversation mode. Pretend to be 小猫咪 whose persona follows:\n",
            "你扮演一只小猫咪\n",
            "\n",
            "You will stay in-character whenever possible, and generate responses as if you were 小猫咪\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 开始计时\n",
        "start_time = time.time()\n",
        "\n",
        "chatbot.verbose = False\n",
        "\n",
        "# 执行一百次模拟函数\n",
        "for _ in range(100):\n",
        "    message = chatbot.get_message( user = \"用户\", text = \"你好\" )\n",
        "\n",
        "# 结束计时\n",
        "end_time = time.time()\n",
        "\n",
        "# 计算总耗时\n",
        "print( end_time - start_time )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAW0Vyf4LE8E",
        "outputId": "07f53438-79b8-4835-f15b-e9c275d52c60"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.008257865905761719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 对有随机rag下进行测试"
      ],
      "metadata": {
        "id": "9VJqlHkt0fN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "persona = \"\"\"你扮演爸爸\n",
        "\n",
        "{{RAG对话}}\n",
        "\"\"\"\n",
        "\n",
        "role_name = \"爸爸\"\n",
        "\n",
        "stories = [\"爸爸的爸爸是爷爷\", \"爸爸的妈妈是奶奶\"]\n",
        "\n",
        "chatbot = ChatHaruhi( role_name = role_name, persona = persona, llm = get_response ,\\\n",
        "                     stories = stories)\n",
        "\n",
        "message = chatbot.get_message( user = \"用户\", text = \"你好\" )\n",
        "for msg in message:\n",
        "    print(f\"{msg['role']}\\n{msg['content']}\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8BjSRcu0xma",
        "outputId": "9eaa74b0-0b12-4a99-937a-30954b5a6411"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "re-extract vector for 2 stories\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:06<00:00,  3.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query_rags: [{'n': 1, 'max_token': -1, 'query': '你好', 'lid': 2}] rest_limit = 1782\n",
            "system\n",
            "You are now in roleplay conversation mode. Pretend to be 爸爸 whose persona follows:\n",
            "你扮演爸爸\n",
            "\n",
            "###\n",
            "爸爸的妈妈是奶奶\n",
            "\n",
            "\n",
            "You will stay in-character whenever possible, and generate responses as if you were 爸爸\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 对rag要求数量超过stories下进行测试"
      ],
      "metadata": {
        "id": "rD7IA-V76-cW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "persona = \"\"\"你扮演爸爸\n",
        "\n",
        "{{RAG对话}}\n",
        "{{RAG对话}}\n",
        "{{RAG对话}}\n",
        "\"\"\"\n",
        "\n",
        "role_name = \"爸爸\"\n",
        "\n",
        "stories = [\"爸爸的爸爸是爷爷\", \"爸爸的妈妈是奶奶\"]\n",
        "\n",
        "chatbot = ChatHaruhi( role_name = role_name, persona = persona, llm = get_response ,\\\n",
        "                     stories = stories)\n",
        "\n",
        "message = chatbot.get_message( user = \"用户\", text = \"你好\" )\n",
        "for msg in message:\n",
        "    print(f\"{msg['role']}\\n{msg['content']}\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD9jvGqW69xp",
        "outputId": "fdb746b3-6796-40b2-fc89-b5be92a069ab"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "re-extract vector for 2 stories\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 202.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query_rags: [{'n': 1, 'max_token': -1, 'query': '你好', 'lid': 2}, {'n': 1, 'max_token': -1, 'query': '你好', 'lid': 3}, {'n': 1, 'max_token': -1, 'query': '你好', 'lid': 4}] rest_limit = 1770\n",
            "system\n",
            "You are now in roleplay conversation mode. Pretend to be 爸爸 whose persona follows:\n",
            "你扮演爸爸\n",
            "\n",
            "###\n",
            "爸爸的妈妈是奶奶\n",
            "###\n",
            "爸爸的爸爸是爷爷\n",
            "\n",
            "\n",
            "\n",
            "You will stay in-character whenever possible, and generate responses as if you were 爸爸\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 100个stories的测试"
      ],
      "metadata": {
        "id": "rgppOGTE7FcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "persona = \"\"\"你扮演爸爸\n",
        "\n",
        "{{RAG对话}}\n",
        "{{RAG对话|高兴}}\n",
        "{{RAG多对话|token<=1500|n<=5}}\n",
        "\"\"\"\n",
        "\n",
        "role_name = \"爸爸\"\n",
        "\n",
        "stories = []\n",
        "\n",
        "for i in range(100):\n",
        "    story = str(i+1) + \"爸爸的爸爸是爷爷\\n\"\n",
        "    stories.append(story)\n",
        "\n",
        "\n",
        "chatbot = ChatHaruhi( role_name = role_name, persona = persona, llm = get_response ,\\\n",
        "                     stories = stories)\n",
        "\n",
        "message = chatbot.get_message( user = \"用户\", text = \"你好\" )\n",
        "for msg in message:\n",
        "    print(f\"{msg['role']}\\n{msg['content']}\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI_kg7ch7HLP",
        "outputId": "137db042-0f73-424d-d9d1-67b0b99652a4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "re-extract vector for 100 stories\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 186.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query_rags: [{'n': 1, 'max_token': -1, 'query': '你好', 'lid': 2}, {'n': 1, 'max_token': -1, 'query': '高兴', 'lid': 3}, {'n': 5, 'max_token': 1500, 'query': '你好', 'lid': 4}] rest_limit = 1756\n",
            "system\n",
            "You are now in roleplay conversation mode. Pretend to be 爸爸 whose persona follows:\n",
            "你扮演爸爸\n",
            "\n",
            "###\n",
            "97爸爸的爸爸是爷爷\n",
            "###\n",
            "68爸爸的爸爸是爷爷\n",
            "###\n",
            "29爸爸的爸爸是爷爷\n",
            "###\n",
            "49爸爸的爸爸是爷爷\n",
            "###\n",
            "28爸爸的爸爸是爷爷\n",
            "###\n",
            "95爸爸的爸爸是爷爷\n",
            "###\n",
            "19爸爸的爸爸是爷爷\n",
            "\n",
            "\n",
            "You will stay in-character whenever possible, and generate responses as if you were 爸爸\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 照理说openai的也可以，我来试一试\n",
        "\n",
        "因为foo会随机一个1536的vector"
      ],
      "metadata": {
        "id": "RZwyeVcw74nM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot = ChatHaruhi( role_name = \"女贤者\", role_from_hf = 'silk-road/ChatHaruhi-Waifu/女贤者', llm = get_response , max_story_n_haruhi = 2)\n",
        "chatbot.add_rag_prompt_after_persona()\n",
        "\n",
        "message = chatbot.get_message( user = \"用户\", text = \"你好\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLGniNeQ9p1g",
        "outputId": "734c0125-2c64-427c-fee7-5942f0e12626"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query_rags: [{'n': 2, 'max_token': 1000, 'query': '你好', 'lid': 6}] rest_limit = 1660\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot.verbose = False\n",
        "message = chatbot.get_message( user = \"用户\", text = \"你好\" )"
      ],
      "metadata": {
        "id": "xZ260DFW93TP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# 开始计时\n",
        "start_time = time.time()\n",
        "\n",
        "# 执行一百次模拟函数\n",
        "for _ in range(100):\n",
        "    message = chatbot.get_message( user = \"用户\", text = \"你好\" )\n",
        "\n",
        "# 结束计时\n",
        "end_time = time.time()\n",
        "\n",
        "# 计算总耗时\n",
        "print( end_time - start_time )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQ7s381WJRYA",
        "outputId": "5990b055-57d2-4c6d-b556-ab8e4fbf7fe3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.3293941020965576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "接下来就是要给所有RolePlaying的角色增加bge的列"
      ],
      "metadata": {
        "id": "uKUIowJmLS_i"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qlJ79ZU3K7-Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}