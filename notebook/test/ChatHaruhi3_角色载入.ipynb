{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOt9tWeJvj6dDm1d3ynRRgB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/Zero-Haruhi/blob/main/notebook/test/ChatHaruhi3_%E8%A7%92%E8%89%B2%E8%BD%BD%E5%85%A5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "02/10\n",
        "\n",
        "- [x] 做一下init\n",
        "- [x] 跑一下各种测试\n",
        "- [x] 把db类放到单一文件\n",
        "\n",
        "02/11\n",
        "\n",
        "- [ ] 直接替换为一次性载入ChatHaruhi"
      ],
      "metadata": {
        "id": "Zvf72DZRhCzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets tiktoken"
      ],
      "metadata": {
        "id": "h7UatXQhtEHw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd899429-e658-4b1a-938d-a2077b391fd5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 15.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!rm -rf /content/Zero-Haruhi\n",
        "!git clone https://github.com/LC1332/Zero-Haruhi\n",
        "%cd /content/Zero-Haruhi"
      ],
      "metadata": {
        "id": "848R2Fqehjqi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72f3e022-a143-49df-acfd-3f9a49d13888"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'Zero-Haruhi'...\n",
            "remote: Enumerating objects: 386, done.\u001b[K\n",
            "remote: Counting objects: 100% (383/383), done.\u001b[K\n",
            "remote: Compressing objects: 100% (213/213), done.\u001b[K\n",
            "remote: Total 386 (delta 251), reused 271 (delta 164), pack-reused 3\u001b[K\n",
            "Receiving objects: 100% (386/386), 2.91 MiB | 16.02 MiB/s, done.\n",
            "Resolving deltas: 100% (251/251), done.\n",
            "/content/Zero-Haruhi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ChatHaruhi import ChatHaruhi"
      ],
      "metadata": {
        "id": "JOOtcb2yt-AL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response( message ):\n",
        "    return \"语言模型输出了角色扮演的结果\"\n",
        "\n",
        "chatbot = ChatHaruhi( role_name = '小猫咪', persona = \"你扮演一只小猫咪\", llm = get_response )"
      ],
      "metadata": {
        "id": "jdblsE-EuLV8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot.chat(user = \"用户\", text = \"你好1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "8OEOOAGeKOdl",
        "outputId": "2c6a9ec4-0db9-4950-abe7-730667acaceb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query_rags: [] rest_limit = 1783\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'语言模型输出了角色扮演的结果'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot.chat(user = \"用户2\", text = \"你好2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "B1trrXMUKU3s",
        "outputId": "b2d827a1-2beb-4ea0-9b8d-78e736d05af8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new user 用户2 included in conversation\n",
            "query_rags: [] rest_limit = 1783\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'语言模型输出了角色扮演的结果'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = chatbot.get_message(user = \"用户\", text = \"你好3\")\n",
        "\n",
        "for msg in message:\n",
        "    print(msg['role'])\n",
        "    print(msg['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ahypLhFxQ75",
        "outputId": "cf0d1707-7f2c-4e9c-fdd4-a9787d368719"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query_rags: [] rest_limit = 1783\n",
            "system\n",
            "You are now in roleplay conversation mode. Pretend to be 小猫咪 whose persona follows:\n",
            "你扮演一只小猫咪\n",
            "\n",
            "You will stay in-character whenever possible, and generate responses as if you were 小猫咪\n",
            "user\n",
            "你好1\n",
            "assistant\n",
            "语言模型输出了角色扮演的结果\n",
            "user\n",
            "你好2\n",
            "assistant\n",
            "语言模型输出了角色扮演的结果\n",
            "user\n",
            "你好3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 载入测试"
      ],
      "metadata": {
        "id": "Cw_1h9vilxav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "测试第一个分支"
      ],
      "metadata": {
        "id": "KkOAPN1dlzc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if persona and role_name and stories and story_vecs and len(stories) == len(story_vecs):\n",
        "#             # 完全从外部设置，这个时候要求story_vecs和embedding的返回长度一致\n",
        "#             self.persona, self.role_name, self.user_name = persona, role_name, user_name\n",
        "#             self.db = self.build_db(stories, story_vecs)\n",
        "\n",
        "persona = \"\"\"你扮演爸爸\"\"\"\n",
        "\n",
        "role_name = \"爸爸\"\n",
        "\n",
        "stories = [\"爸爸的爸爸是爷爷\", \"爸爸的妈妈是奶奶\"]\n",
        "\n",
        "vecs = [[0.0,1.0],[1.0,0.0]]\n",
        "\n",
        "chatbot = ChatHaruhi( role_name = role_name, persona = persona, llm = get_response ,\\\n",
        "                     stories = stories, story_vecs = vecs)\n",
        "\n",
        "print(chatbot.db)"
      ],
      "metadata": {
        "id": "mVzJphirLWdL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f009ba7-53df-4bf3-a7e1-d5d104d86dbf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<ChatHaruhi.NaiveDB.NaiveDB object at 0x799123fba3b0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "测试第二个分支 （没有vecs）"
      ],
      "metadata": {
        "id": "ywuECNVQoEdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if persona and role_name and stories and story_vecs and len(stories) == len(story_vecs):\n",
        "#             # 完全从外部设置，这个时候要求story_vecs和embedding的返回长度一致\n",
        "#             self.persona, self.role_name, self.user_name = persona, role_name, user_name\n",
        "#             self.db = self.build_db(stories, story_vecs)\n",
        "\n",
        "persona = \"\"\"你扮演爸爸\"\"\"\n",
        "\n",
        "role_name = \"爸爸\"\n",
        "\n",
        "stories = [\"爸爸的爸爸是爷爷\", \"爸爸的妈妈是奶奶\"]\n",
        "\n",
        "chatbot = ChatHaruhi( role_name = role_name, persona = persona, llm = get_response ,\\\n",
        "                     stories = stories, embed_name = \"foo\")\n",
        "\n",
        "print(chatbot.db)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpuNuKpfmEfB",
        "outputId": "8ca8b03b-1cdd-478b-f2bf-7280afab9db9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "re-extract vector for 2 stories\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 16878.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<ChatHaruhi.NaiveDB.NaiveDB object at 0x7991229f04f0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "测试第三个分支"
      ],
      "metadata": {
        "id": "s9Yme4p8wP0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_role_name = \"silk-road/ChatHaruhi-RolePlaying/Malfoy\"\n",
        "\n",
        "chatbot = ChatHaruhi( role_name = \"Malfoy\", \\\n",
        "                      role_from_hf = hf_role_name,\n",
        "                      llm = get_response, embed_name = \"openai\")"
      ],
      "metadata": {
        "id": "49b2wQE_sl9j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "ffde7baa-d0e6-4126-81ae-59e6a227b058"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HfHubHTTPError",
          "evalue": "500 Server Error: Internal Server Error for url: https://huggingface.co/api/datasets/silk-road/ChatHaruhi-RolePlaying (Request ID: Root=1-65c82531-1f8d222b112f86971b9f9910;a16357bf-3182-4cc4-8fd9-385bc9943042)\n\nInternal Error - We're working hard to fix this as soon as possible!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-312adae90168>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhf_role_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"silk-road/ChatHaruhi-RolePlaying/Malfoy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m chatbot = ChatHaruhi( role_name = \"Malfoy\", \\\n\u001b[0m\u001b[1;32m      4\u001b[0m                       \u001b[0mrole_from_hf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhf_role_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                       llm = get_response, embed_name = \"openai\")\n",
            "\u001b[0;32m/content/Zero-Haruhi/ChatHaruhi/ChatHaruhi.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, role_name, user_name, persona, stories, story_vecs, role_from_hf, role_from_jsonl, llm, llm_async, user_name_in_message, verbose, embed_name, embedding, db, token_counter, max_input_token, max_len_story_haruhi, max_story_n_haruhi)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mrole_from_hf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m# 从hf加载role\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersona\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_role_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstory_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_role_from_hf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrole_from_hf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnew_role_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrole_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_role_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Zero-Haruhi/ChatHaruhi/ChatHaruhi.py\u001b[0m in \u001b[0;36mload_role_from_hf\u001b[0;34m(self, role_from_hf)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.jsonl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mdatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2547\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2548\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   2549\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   2218\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2219\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2220\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   2221\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2222\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1869\u001b[0m                         \u001b[0;34mf\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1870\u001b[0m                     ) from None\n\u001b[0;32m-> 1871\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1872\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1873\u001b[0m         raise FileNotFoundError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1814\u001b[0m                     )\n\u001b[1;32m   1815\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1816\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1817\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msibling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfilename\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msibling\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msiblings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# contains a dataset script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1818\u001b[0m                 \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHfFileSystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHF_ENDPOINT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1788\u001b[0m             \u001b[0mhf_api\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHfApi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHF_ENDPOINT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m                 dataset_info = hf_api.dataset_info(\n\u001b[0m\u001b[1;32m   1791\u001b[0m                     \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m                     \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mdataset_info\u001b[0;34m(self, repo_id, revision, timeout, files_metadata, token)\u001b[0m\n\u001b[1;32m   2146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2147\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2148\u001b[0;31m         \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2149\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2150\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDatasetInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;31m# as well (request id and/or server error message)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://huggingface.co/api/datasets/silk-road/ChatHaruhi-RolePlaying (Request ID: Root=1-65c82531-1f8d222b112f86971b9f9910;a16357bf-3182-4cc4-8fd9-385bc9943042)\n\nInternal Error - We're working hard to fix this as soon as possible!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "测试第四个分支"
      ],
      "metadata": {
        "id": "ZRg4drmJxOiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# jsonl懒得测了，回头和save一起测"
      ],
      "metadata": {
        "id": "ZK5_1-_0ws74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "测试第五个分支"
      ],
      "metadata": {
        "id": "ToKgVOLyyTXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot = ChatHaruhi( role_name = '小猫咪', persona = \"你扮演一只小猫咪\", llm = get_response )\n"
      ],
      "metadata": {
        "id": "7QnlGWldyOEK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "测试第六个分支"
      ],
      "metadata": {
        "id": "D8DFGTQn0uj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot = ChatHaruhi( role_name = '于谦', llm = get_response, embed_name = \"openai\" )\n"
      ],
      "metadata": {
        "id": "G8K4-sZpyUpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chatbot.persona)"
      ],
      "metadata": {
        "id": "GrZ6EHKB1AHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oXS4znrH1BV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ChatHaruhi.NaiveDB import NaiveDB\n",
        "from ChatHaruhi.embeddings import foo_embedding, foo_bge_zh_15, foo_bce, foo_openai, embedname2columnname\n",
        "from ChatHaruhi.utils import base64_to_float_array, base64_to_string\n",
        "# from ChatHaruhi.sugar_map import sugar_role_names, enname2zhname"
      ],
      "metadata": {
        "id": "ETv2QXHvuAbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sugar_role_names = {'汤师爷': 'tangshiye', 'tangshiye': 'tangshiye', 'Tangshiye': 'tangshiye',\n",
        "                     '慕容复': 'murongfu', 'murongfu': 'murongfu', 'Murongfu': 'murongfu',\n",
        "                     '李云龙': 'liyunlong', 'liyunlong': 'liyunlong', 'Liyunlong': 'liyunlong',\n",
        "                     'Luna': 'Luna', '王多鱼': 'wangduoyu', 'wangduoyu': 'wangduoyu',\n",
        "                     'Wangduoyu': 'wangduoyu', 'Ron': 'Ron', '鸠摩智': 'jiumozhi',\n",
        "                     'jiumozhi': 'jiumozhi', 'Jiumozhi': 'jiumozhi', 'Snape': 'Snape',\n",
        "                     '凉宫春日': 'haruhi', 'haruhi': 'haruhi', 'Haruhi': 'haruhi',\n",
        "                     'Malfoy': 'Malfoy', '虚竹': 'xuzhu', 'xuzhu': 'xuzhu',\n",
        "                     'Xuzhu': 'xuzhu', '萧峰': 'xiaofeng',\n",
        "                     'xiaofeng': 'xiaofeng', 'Xiaofeng': 'xiaofeng', '段誉': 'duanyu',\n",
        "                     'duanyu': 'duanyu', 'Duanyu': 'duanyu', 'Hermione': 'Hermione',\n",
        "                     'Dumbledore': 'Dumbledore', '王语嫣': 'wangyuyan', 'wangyuyan':\n",
        "                     'wangyuyan', 'Wangyuyan': 'wangyuyan', 'Harry': 'Harry',\n",
        "                     'McGonagall': 'McGonagall', '白展堂': 'baizhantang',\n",
        "                     'baizhantang': 'baizhantang', 'Baizhantang': 'baizhantang',\n",
        "                     '佟湘玉': 'tongxiangyu', 'tongxiangyu': 'tongxiangyu',\n",
        "                     'Tongxiangyu': 'tongxiangyu', '郭芙蓉': 'guofurong',\n",
        "                     'guofurong': 'guofurong', 'Guofurong': 'guofurong', '流浪者': 'wanderer',\n",
        "                     'wanderer': 'wanderer', 'Wanderer': 'wanderer', '钟离': 'zhongli',\n",
        "                     'zhongli': 'zhongli', 'Zhongli': 'zhongli', '胡桃': 'hutao', 'hutao': 'hutao',\n",
        "                     'Hutao': 'hutao', 'Sheldon': 'Sheldon', 'Raj': 'Raj',\n",
        "                     'Penny': 'Penny', '韦小宝': 'weixiaobao', 'weixiaobao': 'weixiaobao',\n",
        "                     'Weixiaobao': 'weixiaobao', '乔峰': 'qiaofeng', 'qiaofeng': 'qiaofeng',\n",
        "                     'Qiaofeng': 'qiaofeng', '神里绫华': 'ayaka', 'ayaka': 'ayaka',\n",
        "                     'Ayaka': 'ayaka', '雷电将军': 'raidenShogun', 'raidenShogun': 'raidenShogun',\n",
        "                     'RaidenShogun': 'raidenShogun', '于谦': 'yuqian', 'yuqian': 'yuqian',\n",
        "                     'Yuqian': 'yuqian', 'Professor McGonagall': 'McGonagall',\n",
        "                     'Professor Dumbledore': 'Dumbledore'}\n",
        "\n",
        "enname2zhname = {'tangshiye': '汤师爷', 'murongfu': '慕容复', 'liyunlong': '李云龙', 'Luna': 'Luna', 'wangduoyu': '王多鱼', 'Ron': 'Ron', 'jiumozhi': '鸠摩智', 'Snape': 'Snape', 'haruhi': '凉宫春日', 'Malfoy': 'Malfoy', 'xuzhu': '虚竹', 'xiaofeng': '萧峰', 'duanyu': '段誉', 'Hermione': 'Hermione', 'Dumbledore': 'Dumbledore', 'wangyuyan': '王语嫣', 'Harry': 'Harry', 'McGonagall': 'McGonagall', 'baizhantang': '白展堂', 'tongxiangyu': '佟湘玉', 'guofurong': '郭芙蓉', 'wanderer': '流浪者', 'zhongli': '钟离', 'hutao': '胡桃', 'Sheldon': 'Sheldon', 'Raj': 'Raj', 'Penny': 'Penny', 'weixiaobao': '韦小宝', 'qiaofeng': '乔峰', 'ayaka': '神里绫华', 'raidenShogun': '雷电将军', 'yuqian': '于谦'}\n"
      ],
      "metadata": {
        "id": "wjg6EbVguDIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tiktoken_counter( text ):\n",
        "    # TODO 把这个实现为tiktoken 然后放到util\n",
        "    return len(text)\n",
        "\n",
        "def get_text_from_data( data ):\n",
        "    if \"text\" in data:\n",
        "        return data['text']\n",
        "    elif \"enc_text\" in data:\n",
        "        # from .utils import base64_to_string\n",
        "        return base64_to_string( data['enc_text'] )\n",
        "    else:\n",
        "        print(\"warning! failed to get text from data \", data)\n",
        "        return \"\"\n",
        "\n",
        "class ChatHaruhi:\n",
        "    def __init__(self,\n",
        "                 role_name = None,\n",
        "                 user_name = None,\n",
        "                 persona = None,\n",
        "                 stories = None,\n",
        "                 story_vecs = None,\n",
        "                 role_from_hf = None,\n",
        "                 role_from_jsonl = None,\n",
        "                 llm = None, # 默认的message2response的函数\n",
        "                 llm_async = None, # 默认的message2response的async函数\n",
        "                 user_name_in_message = \"default\",\n",
        "                 verbose = None,\n",
        "                 embed_name = None,\n",
        "                 embedding = None,\n",
        "                 db = None,\n",
        "                 token_counter = \"default\",\n",
        "                 max_input_token = 1800\n",
        "                 ):\n",
        "\n",
        "        self.verbose = True if verbose is None or verbose else False\n",
        "\n",
        "        self.db = db\n",
        "\n",
        "        self.embed_name = embed_name\n",
        "\n",
        "        if embedding is None:\n",
        "            self.embedding = self.set_embedding_with_name( embed_name )\n",
        "\n",
        "        if persona and role_name and stories and story_vecs and len(stories) == len(story_vecs):\n",
        "            # 完全从外部设置，这个时候要求story_vecs和embedding的返回长度一致\n",
        "            self.persona, self.role_name, self.user_name = persona, role_name, user_name\n",
        "            self.build_db(stories, story_vecs)\n",
        "        elif persona and role_name and stories:\n",
        "            # 从stories中提取story_vecs，重新用self.embedding进行embedding\n",
        "            story_vecs = self.extract_story_vecs(stories)\n",
        "            self.persona, self.role_name, self.user_name = persona, role_name, user_name\n",
        "            self.build_db(stories, story_vecs)\n",
        "        elif role_from_hf:\n",
        "            # 从hf加载role\n",
        "            self.persona, new_role_name, self.stories, self.story_vecs = self.load_role_from_hf(role_from_hf)\n",
        "            if new_role_name:\n",
        "                self.role_name = new_role_name\n",
        "            else:\n",
        "                self.role_name = role_name\n",
        "            self.user_name = user_name\n",
        "            self.build_db(self.stories, self.story_vecs)\n",
        "        elif role_from_jsonl:\n",
        "            # 从jsonl加载role\n",
        "            self.persona, new_role_name, self.stories, self.story_vecs = self.load_role_from_jsonl(role_from_jsonl)\n",
        "            if new_role_name:\n",
        "                self.role_name = new_role_name\n",
        "            else:\n",
        "                self.role_name = role_name\n",
        "            self.user_name = user_name\n",
        "            self.build_db(self.stories, self.story_vecs)\n",
        "        elif persona and role_name:\n",
        "            # 这个时候也就是说没有任何的RAG，\n",
        "            self.persona, self.role_name, self.user_name = persona, role_name, user_name\n",
        "            self.db = None\n",
        "        elif role_name and self.check_sugar( role_name ):\n",
        "            # 这个时候是sugar的role\n",
        "            self.persona, self.role_name, self.user_name, self.db = self.load_role_from_sugar( role_name )\n",
        "        else:\n",
        "            raise ValueError(\"persona和role_name必须同时设置，或者role_name是ChatHaruhi的预设人物\")\n",
        "\n",
        "        self.llm, self.llm_async = llm, llm_async\n",
        "        if not self.llm and self.verbose:\n",
        "            print(\"warning, llm没有设置，仅get_message起作用，调用chat将回复idle message\")\n",
        "\n",
        "        self.user_name_in_message = user_name_in_message\n",
        "        self.previous_user_pool = set([user_name]) if user_name else set()\n",
        "        self.current_user_name_in_message = user_name_in_message.lower() == \"add\"\n",
        "\n",
        "        self.idle_message = \"idel message, you see this because self.llm has not been set.\"\n",
        "\n",
        "        if token_counter.lower() == \"default\":\n",
        "            # TODO change load from util\n",
        "            self.token_counter = tiktoken_counter\n",
        "        elif token_counter == None:\n",
        "            self.token_counter = lambda x: 0\n",
        "        else:\n",
        "            self.token_counter = token_counter\n",
        "            if self.verbose:\n",
        "                print(\"user set costomized token_counter\")\n",
        "\n",
        "        self.max_input_token = max_input_token\n",
        "\n",
        "        self.history = []\n",
        "\n",
        "    def check_sugar(self, role_name):\n",
        "        return role_name in sugar_role_names\n",
        "\n",
        "    def load_role_from_sugar(self, role_name):\n",
        "        en_role_name = sugar_role_names[role_name]\n",
        "        new_role_name = enname2zhname[en_role_name]\n",
        "        role_from_hf = \"silk-road/ChatHaruhi-RolePlaying/\" + en_role_name\n",
        "        persona, _, stories, story_vecs = self.load_role_from_hf(role_from_hf)\n",
        "        return persona, new_role_name, stories, story_vecs\n",
        "\n",
        "    def set_embedding_with_name(self, embed_name):\n",
        "        if embed_name is None or embed_name == \"foo\":\n",
        "            return foo_embedding\n",
        "        elif embed_name == \"bge-zh\":\n",
        "            return foo_bge_zh_15\n",
        "        elif embed_name == \"bce\":\n",
        "            return foo_bce\n",
        "        elif embed_name == \"openai\" or embed_name == \"luotuo_openai\":\n",
        "            return foo_openai\n",
        "\n",
        "    def set_new_user(self, user):\n",
        "        if len(self.previous_user_pool) > 0 and user not in self.previous_user_pool:\n",
        "            if self.user_name_in_message.lower() == \"default\":\n",
        "                if self.verbose:\n",
        "                    print(f'new user {user} included in conversation')\n",
        "                self.current_user_name_in_message = True\n",
        "        self.user_name = user\n",
        "        self.previous_user_pool.add(user)\n",
        "\n",
        "    def chat(self, user, text):\n",
        "        self.set_new_user(user)\n",
        "        message = self.get_message(user, text)\n",
        "        if self.llm:\n",
        "            response = self.llm(message)\n",
        "            self.append_message(response)\n",
        "            return self.llm(message)\n",
        "\n",
        "    async def async_chat(self, user, text):\n",
        "        self.set_new_user(user)\n",
        "        message = self.get_message(user, text)\n",
        "        if self.llm_async:\n",
        "            response = await self.llm_async(message)\n",
        "            self.append_message(response)\n",
        "            return self.llm_async(message)\n",
        "\n",
        "    def parse_rag_from_persona(self, persona):\n",
        "        #每个query_rag需要饱含\n",
        "        # \"n\" 需要几个story\n",
        "        # \"max_token\" 最多允许多少个token，如果-1则不限制\n",
        "        # \"query\" 需要查询的内容，如果等同于\"default\"则替换为text\n",
        "        # \"lid\" 需要替换的行，这里直接进行行替换，忽视行的其他内容\n",
        "\n",
        "        print(\"parse_rag_from_persona\")\n",
        "        return [], self.token_counter(persona)\n",
        "\n",
        "    def append_message( self, response , speaker = None ):\n",
        "        if speaker is None:\n",
        "            # 如果role是none，则认为是本角色{{role}}输出的句子\n",
        "            self.history.append({\"speaker\":\"{{user}}\",\"content\":response})\n",
        "            # 叫speaker是为了和role进行区分\n",
        "        else:\n",
        "            self.history.append({\"speaker\":speaker,\"content\":response})\n",
        "\n",
        "    def rag_retrieve( self, query, n, max_token, avoid_ids = [] ):\n",
        "        # 返回一个rag_id的列表\n",
        "        print(\"call rag_retrieve\")\n",
        "        return []\n",
        "\n",
        "    def rag_retrieve_all( self, query_rags, rest_limit ):\n",
        "        # 返回一个rag_ids的列表\n",
        "        retrieved_ids = []\n",
        "        rag_ids = []\n",
        "\n",
        "        for query_rag in query_rags:\n",
        "            query = query_rag['query']\n",
        "            n = query_rag['n']\n",
        "            max_token = rest_limit\n",
        "            if rest_limit > query_rag['max_token'] and query_rag['max_token'] > 0:\n",
        "                max_token = query_rag['max_token']\n",
        "\n",
        "            rag_id = self.rag_retrieve( query, n, max_token, avoid_ids = retrieved_ids )\n",
        "            rag_ids.append( rag_id )\n",
        "            retrieved_ids += rag_id\n",
        "\n",
        "        return rag_ids\n",
        "\n",
        "    def append_history_under_limit(self, message, rest_limit):\n",
        "        # 返回一个messages的列表\n",
        "        print(\"call append_history_under_limit\")\n",
        "\n",
        "        # 从后往前计算token，不超过rest_limit,\n",
        "        # 如果speaker是{{role}},则message的role是assistant\n",
        "\n",
        "        return message\n",
        "\n",
        "    def get_message(self, user, text):\n",
        "        query_token = self.token_counter(text)\n",
        "\n",
        "        # 首先获取需要多少个rag story\n",
        "        query_rags, persona_token = self.parse_rag_from_persona( self.persona )\n",
        "        #每个query_rag需要饱含\n",
        "        # \"n\" 需要几个story\n",
        "        # \"max_token\" 最多允许多少个token，如果-1则不限制\n",
        "        # \"query\" 需要查询的内容，如果等同于\"default\"则替换为text\n",
        "        # \"lid\" 需要替换的行，这里直接进行行替换，忽视行的其他内容\n",
        "\n",
        "        rest_limit = self.max_input_token - persona_token - query_token\n",
        "\n",
        "        rag_ids = self.rag_retrieve_all( query_rags, rest_limit )\n",
        "\n",
        "        # 将rag_ids对应的故事 替换到persona中\n",
        "        augmented_persona = self.augment_persona( self.persona, rag_ids, query_rags )\n",
        "\n",
        "        system_prompt = self.package_system_prompt( self.role_name, augmented_persona )\n",
        "\n",
        "        token_for_system = self.token_counter( system_prompt )\n",
        "\n",
        "        rest_limit = self.max_input_token - token_for_system - query_token\n",
        "\n",
        "        message = [{\"role\":\"system\",\"content\":system_prompt}]\n",
        "\n",
        "        message = self.append_history_under_limit( message, rest_limit )\n",
        "\n",
        "        message.append({\"role\":\"user\",\"content\":text})\n",
        "\n",
        "        return message\n",
        "\n",
        "    def package_system_prompt(self, role_name, augmented_persona):\n",
        "        bot_name = role_name\n",
        "        return f\"\"\"You are now in roleplay conversation mode. Pretend to be {bot_name} whose persona follows:\n",
        "{augmented_persona}\n",
        "\n",
        "You will stay in-character whenever possible, and generate responses as if you were {bot_name}\"\"\"\n",
        "\n",
        "\n",
        "    def augment_persona(self, persona, rag_ids, query_rags):\n",
        "        lines = persona.split(\"\\n\")\n",
        "        for rag_id, query_rag in zip(rag_ids, query_rags):\n",
        "            lid = query_rag['lid']\n",
        "            new_text = \"\"\n",
        "            for id in rag_id:\n",
        "                new_text += \"###\\n\" + self.db.get_text(id) + \"\\n\"\n",
        "            new_text = new_text.strip()\n",
        "            lines[lid] = new_text\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "    def load_role_from_jsonl( self, role_from_jsonl ):\n",
        "        if self.verbose:\n",
        "            print(f\"因为懒得测试，jsonl的分支还没有测试，当你使用的时候如果通过了，请告诉鲁叔测试通过了\")\n",
        "        import json\n",
        "        datas = []\n",
        "        with open(role_from_jsonl, 'r') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    datas.append(json.loads(line))\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        column_name = \"\"\n",
        "\n",
        "        if self.embed_name in embedname2columnname:\n",
        "            column_name = embedname2columnname[self.embed_name]\n",
        "        else:\n",
        "            print('warning! unkown embedding name ', self.embed_name ,' while loading role')\n",
        "            column_name = 'luotuo_openai'\n",
        "\n",
        "        stories, story_vecs, persona = self.extract_text_vec_from_datas(datas, column_name)\n",
        "\n",
        "        return persona, None, stories, story_vecs\n",
        "\n",
        "\n",
        "    def load_role_from_hf(self, role_from_hf):\n",
        "        # 从hf加载role\n",
        "        # self.persona, new_role_name, self.stories, self.story_vecs = self.load_role_from_hf(role_from_hf)\n",
        "\n",
        "        from datasets import load_dataset\n",
        "\n",
        "        if role_from_hf.count(\"/\") == 1:\n",
        "            dataset = load_dataset(role_from_hf)\n",
        "            datas = dataset[\"train\"]\n",
        "        elif role_from_hf.count(\"/\") >= 2:\n",
        "            split_index = role_from_hf.index('/')\n",
        "            second_split_index = role_from_hf.index('/', split_index+1)\n",
        "            dataset_name = role_from_hf[:second_split_index]\n",
        "            split_name = role_from_hf[second_split_index+1:]\n",
        "\n",
        "            fname = split_name + '.jsonl'\n",
        "            dataset = load_dataset(dataset_name,data_files={'train':fname})\n",
        "            datas = dataset[\"train\"]\n",
        "\n",
        "        column_name = \"\"\n",
        "\n",
        "        if self.embed_name in embedname2columnname:\n",
        "            column_name = embedname2columnname[self.embed_name]\n",
        "        else:\n",
        "            print('warning! unkown embedding name ', self.embed_name ,' while loading role')\n",
        "            column_name = 'luotuo_openai'\n",
        "\n",
        "        stories, story_vecs, persona = self.extract_text_vec_from_datas(datas, column_name)\n",
        "\n",
        "        return persona, None, stories, story_vecs\n",
        "\n",
        "    def extract_text_vec_from_datas(self, datas, column_name):\n",
        "        # 从datas中提取text和vec\n",
        "        # extract text and vec from huggingface dataset\n",
        "        # return texts, vecs\n",
        "        # from .utils import base64_to_float_array\n",
        "\n",
        "        texts = []\n",
        "        vecs = []\n",
        "        for data in datas:\n",
        "            if data[column_name] == 'system_prompt':\n",
        "                system_prompt = get_text_from_data( data )\n",
        "            elif data[column_name] == 'config':\n",
        "                pass\n",
        "            else:\n",
        "                vec = base64_to_float_array( data[column_name] )\n",
        "                text = get_text_from_data( data )\n",
        "                vecs.append( vec )\n",
        "                texts.append( text )\n",
        "        return texts, vecs, system_prompt\n",
        "\n",
        "    def load_role_from_jsonl(self, role_from_jsonl):\n",
        "        # 从jsonl加载role\n",
        "        return None\n",
        "\n",
        "    def extract_story_vecs(self, stories):\n",
        "        # 从stories中提取story_vecs\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"re-extract vector for {len(stories)} stories\")\n",
        "\n",
        "        from tqdm import tqdm\n",
        "        story_vecs = []\n",
        "        for story in tqdm(stories):\n",
        "            story_vecs.append(self.embedding(story))\n",
        "\n",
        "        return story_vecs\n",
        "\n",
        "    def build_db(self, stories, story_vecs):\n",
        "        # db的构造函数\n",
        "        if self.db is None:\n",
        "            self.db = NaiveDB()\n",
        "        self.db.build_db(stories, story_vecs)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hw3zsIXbuGGy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}